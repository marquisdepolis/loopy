{"https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/?ref=thebrowser.com": {"embedding": [-0.6785940527915955, 1.7291110754013062, -1.7901932001113892, -0.9332209229469299, 2.529909372329712, 1.091896414756775, 1.3075743913650513, 3.398064136505127, -2.226255178451538, -1.3929752111434937, 7.594284534454346, 1.5880872011184692, -3.0987977981567383, 1.0686372518539429, 1.3925031423568726, 0.5978733897209167, 1.8922431468963623, -0.3909510672092438, -1.1778031587600708, -1.6441962718963623, 1.0654513835906982, -0.4304971396923065, -0.790671169757843, -1.5191643238067627, -1.3586807250976562, -1.8631787300109863, -1.5450180768966675, -1.1340512037277222, -1.5288091897964478, 2.2619619369506836, 0.8302648067474365, -1.3727785348892212, -1.1530787944793701, -1.8215500116348267, -1.3314299583435059, -0.800247311592102, -0.24933162331581116, 0.26307111978530884, 3.101806879043579, 1.6777385473251343, -0.0919230654835701, 0.17133879661560059, -0.12809374928474426, -0.5484943389892578, -2.0188052654266357, 2.533515214920044, 1.5934491157531738, -3.295811176300049, -1.7242404222488403, 0.9818775653839111, -0.9398075342178345, 0.8118459582328796, 0.8560022711753845, -4.68154764175415, -1.7838269472122192, 0.2836751937866211, 0.5135048031806946, 2.1883065700531006, 1.1706405878067017, 0.3430624306201935, 2.6357645988464355, 0.11307106912136078, 0.2367185652256012, -1.5535838603973389, 0.34663867950439453, 2.4652316570281982, -3.2756495475769043, -5.077859878540039, -0.08542883396148682, 2.176283121109009, -0.10517699271440506, -0.0921587198972702, -1.723023772239685, -0.657006561756134, -0.5756516456604004, 1.8416117429733276, -3.14858078956604, 1.8707727193832397, -3.1314713954925537, 0.21521340310573578, -4.223514080047607, -0.5187568068504333, 2.509450912475586, 1.2100392580032349, 1.975691795349121, 0.08603429794311523, -1.1950277090072632, -2.336575508117676, 1.5220410823822021, -0.8981115221977234, -0.7011659145355225, 0.12896369397640228, 2.0648181438446045, -4.839412212371826, -0.0511339008808136, -2.3586301803588867, 1.1886754035949707, -1.3804901838302612, -0.5376588106155396, 0.004955358803272247, 1.975113034248352, 1.8384215831756592, 1.5799176692962646, 2.5680346488952637, -1.0781238079071045, 4.349825382232666, 0.18440420925617218, -2.3017098903656006, -0.9629716277122498, -1.8285776376724243, 1.5667732954025269, 0.18079961836338043, -0.8892907500267029, 1.4121723175048828, 0.398520827293396, 1.3658782243728638, -1.6186920404434204, -1.4517087936401367, -0.635441243648529, -1.5274906158447266, -2.2035605907440186, -2.507387638092041, -0.1559298038482666, 1.3033080101013184, -0.4545643627643585, -2.230010986328125, 1.1618094444274902, -2.7657573223114014, 2.398242235183716, -1.4097775220870972, -2.8440537452697754, 0.03082170896232128, 2.5106160640716553, -0.9439104199409485, -0.2618231475353241, 1.0502251386642456, -1.7144813537597656, -1.6103965044021606, 3.3821144104003906, -3.2323944568634033, -1.459263563156128, -1.292586326599121, 1.1945363283157349, 1.2596882581710815, -0.8013101816177368, -0.11949769407510757, -3.3763084411621094, 0.5097159147262573, -0.8177471160888672, 0.534083902835846, -1.5916564464569092, 3.904086112976074, -0.3427584767341614, 0.7250934839248657, -1.0230164527893066, 1.7283474206924438, 3.6140849590301514, 0.4146382510662079, -1.5165225267410278, -0.6879377365112305, 0.3845807611942291, -1.2509500980377197, -1.1848092079162598, 1.501527190208435, -2.2372541427612305, -2.3021843433380127, -3.1631524562835693, 1.32980215549469, -0.5672159790992737, -0.1828182488679886, 1.6168543100357056, -0.3391645550727844, 2.6120223999023438, 0.486378014087677, 1.551414966583252, -0.5192087888717651, -0.42610475420951843, 0.8905063271522522, -1.7664738893508911, -1.851396918296814, -0.08031590282917023, 0.5072335600852966, 2.968625545501709, -1.4228328466415405, -1.1259510517120361, 0.710886538028717, -1.2989585399627686, -2.021810293197632, 1.8976613283157349, 2.233503580093384, -1.1905428171157837, -0.7605175971984863, -0.27253690361976624, -1.3175427913665771, -0.42528975009918213, 0.19622701406478882, -2.955749988555908, -0.0722019150853157, 0.09424912184476852, 1.0883897542953491, -1.5539164543151855, -0.5139620900154114, -0.9105918407440186, -2.6447606086730957, 1.8666585683822632, 1.5399197340011597, -3.7977242469787598, 0.28188925981521606, 0.26134777069091797, -0.2173713594675064, 2.361018657684326, 0.19754493236541748, -0.3739252984523773, 2.0331761837005615, 0.4610597789287567, 2.162691116333008, -0.054116539657115936, -2.367219924926758, -1.2159959077835083, 0.4626428782939911, -2.85373854637146, 0.5726457834243774, 0.33874398469924927, 0.7162370681762695, -0.9028265476226807, -0.7873798608779907, 0.4357265830039978, 2.074880361557007, 1.949233055114746, -0.5791334509849548, 0.07261084020137787, -2.754340410232544, 0.2698949873447418, 0.7817748188972473, 0.5019739866256714, 1.1000046730041504, -0.6348000168800354, 2.134936571121216, 1.2471979856491089, 0.41805407404899597, -1.3630257844924927, -0.13566480576992035, 1.6723068952560425, 0.7743569612503052, 0.2703959047794342, 1.021061658859253, -2.0624239444732666, 0.36698493361473083, 1.080094575881958, 1.9522191286087036, 0.3140290379524231, -0.9334655404090881, -4.544552326202393, -0.013079269789159298, 0.2404964566230774, -2.615551471710205, 1.2284079790115356, 1.012358546257019, -0.48349952697753906, 1.9743411540985107, -0.3191983103752136, 4.789981842041016, 2.911463975906372, 2.4042155742645264, 1.9842830896377563, 0.4978928565979004, -0.0978890135884285, 2.8404223918914795, -3.8477206230163574, -0.4260811507701874, 1.0207786560058594, -0.7541360259056091, 0.10647086799144745, -1.2547526359558105, 0.9842489957809448, -1.163507342338562, 1.424094557762146, -1.8588769435882568, -1.3940273523330688, 1.9766407012939453, -0.04367789998650551, -0.9563997387886047, 0.5317322015762329, 1.32033109664917, 3.9415018558502197, -0.5470678210258484, 1.21639883518219, 0.8378322124481201, -1.0918561220169067, 0.7159098386764526, 1.2035263776779175, -0.5888654589653015, 0.9496849179267883, -1.5859805345535278, -0.7392967343330383, 0.3660363256931305, 0.7312659621238708, 0.05908823385834694, -3.1547882556915283, 0.9319337606430054], "text_chunks": ["That ChatGPT can automatically generate something that reads even superficially like human-written text is remarkable, and unexpected. But how does it do it? And why does it work? My purpose here is to give a rough outline of what\u2019s going on inside ChatGPT\u2014and then to explore why it is that it can do so well in producing what we might consider to be meaningful text. I should say at the outset that I\u2019m going to focus on the big picture of what\u2019s going on\u2014and while I\u2019ll mention some engineering details, I won\u2019t get deeply into them. (And the essence of what I\u2019ll say applies just as well to other current \u201clarge language models\u201d [LLMs] as to ChatGPT.) The first thing to explain is that what ChatGPT is always fundamentally trying to do is to produce a \u201creasonable continuation\u201d of whatever text it\u2019s got so far, where by \u201creasonable\u201d we mean \u201cwhat one might expect someone to write after seeing what people have written on billions of webpages, etc.\u201d So let\u2019s say we\u2019ve got the text \u201cThe best thing about AI is its ability to\u201d. Imagine scanning billions of pages of human-written text (say on the web and in digitized books) and finding all instances of this text\u2014then seeing what word comes next what fraction of the time. ChatGPT effectively does something like this, except that (as I\u2019ll explain) it doesn\u2019t look at literal text; it looks for things that in a certain sense \u201cmatch in meaning\u201d. But the end result is that it produces a ranked list of words that might follow, together with \u201cprobabilities\u201d: And the remarkable thing is that when ChatGPT does something like write an essay what it\u2019s essentially doing is just asking over and over again \u201cgiven the text so far, what should the next word be?\u201d\u2014and each time adding a word. (More precisely, as I\u2019ll explain, it\u2019s adding a \u201ctoken\u201d, which could be just a part of a word, which is why it can sometimes \u201cmake up new words\u201d.) But, OK, at each step it gets a list of words with probabilities. But which one should it actually pick to add to the essay (or whatever) that it\u2019s writing?", "One might think it should be the \u201chighest-ranked\u201d word (i.e. the one to which the highest \u201cprobability\u201d was assigned). But this is where a bit of voodoo begins to creep in. Because for some reason\u2014that maybe one day we\u2019ll have a scientific-style understanding of\u2014if we always pick the highest-ranked word, we\u2019ll typically get a very \u201cflat\u201d essay, that never seems to \u201cshow any creativity\u201d (and even sometimes repeats word for word). But if sometimes (at random) we pick lower-ranked words, we get a \u201cmore interesting\u201d essay. The fact that there\u2019s randomness here means that if we use the same prompt multiple times, we\u2019re likely to get different essays each time. And, in keeping with the idea of voodoo, there\u2019s a particular so-called \u201ctemperature\u201d parameter that determines how often lower-ranked words will be used, and for essay generation, it turns out that a \u201ctemperature\u201d of 0.8 seems best. (It\u2019s worth emphasizing that there\u2019s no \u201ctheory\u201d being used here; it\u2019s just a matter of what\u2019s been found to work in practice. And for example the concept of \u201ctemperature\u201d is there because exponential distributions familiar from statistical physics happen to be being used, but there\u2019s no \u201cphysical\u201d connection\u2014at least so far as we know.) Before we go on I should explain that for purposes of exposition I\u2019m mostly not going to use the full system that\u2019s in ChatGPT; instead I\u2019ll usually work with a simpler GPT-2 system, which has the nice feature that it\u2019s small enough to be able to run on a standard desktop computer. And so for essentially everything I show I\u2019ll be able to include explicit Wolfram Language code that you can immediately run on your computer. (Click any picture here to copy the code behind it.) For example, here\u2019s how to get the table of probabilities above. First, we have to retrieve the underlying \u201clanguage model\u201d neural net: Later on, we\u2019ll look inside this neural net, and talk about how it works.", "But for now we can just apply this \u201cnet model\u201d as a black box to our text so far, and ask for the top 5 words by probability that the model says should follow:   This takes that result and makes it into an explicit formatted \u201cdataset\u201d: Here\u2019s what happens if one repeatedly \u201capplies the model\u201d\u2014at each step adding the word that has the top probability (specified in this code as the \u201cdecision\u201d from the model): What happens if one goes on longer? In this (\u201czero temperature\u201d) case what comes out soon gets rather confused and repetitive: But what if instead of always picking the \u201ctop\u201d word one sometimes randomly picks \u201cnon-top\u201d words (with the \u201crandomness\u201d corresponding to \u201ctemperature\u201d 0.8)? Again one can build up text:   And every time one does this, different random choices will be made, and the text will be different\u2014as in these 5 examples: It\u2019s worth pointing out that even at the first step there are a lot of possible \u201cnext words\u201d to choose from (at temperature 0.8), though their probabilities fall off quite quickly (and, yes, the straight line on this log-log plot corresponds to an n\u20131 \u201cpower-law\u201d decay that\u2019s very characteristic of the general statistics of language):   So what happens if one goes on longer? Here\u2019s a random example. It\u2019s better than the top-word (zero temperature) case, but still at best a bit weird: This was done with the simplest GPT-2 model (from 2019). With the newer and bigger GPT-3 models the results are better. Here\u2019s the top-word (zero temperature) text produced with the same \u201cprompt\u201d, but with the biggest GPT-3 model: And here\u2019s a random example at \u201ctemperature 0.8\u201d: OK, so ChatGPT always picks its next word based on probabilities. But where do those probabilities come from? Let\u2019s start with a simpler problem. Let\u2019s consider generating English text one letter (rather than word) at a time. How can we work out what the probability for each letter should be? A very minimal thing we could do is just take a sample of English text, and calculate how often different letters occur in it.", "So, for example, this counts letters in the Wikipedia article on \u201ccats\u201d:   And this does the same thing for \u201cdogs\u201d: The results are similar, but not the same (\u201co\u201d is no doubt more common in the \u201cdogs\u201d article because, after all, it occurs in the word \u201cdog\u201d itself). Still, if we take a large enough sample of English text we can expect to eventually get at least fairly consistent results:  Here\u2019s a sample of what we get if we just generate a sequence of letters with these probabilities: We can break this into \u201cwords\u201d by adding in spaces as if they were letters with a certain probability: We can do a slightly better job of making \u201cwords\u201d by forcing the distribution of \u201cword lengths\u201d to agree with what it is in English: We didn\u2019t happen to get any \u201cactual words\u201d here, but the results are looking slightly better. To go further, though, we need to do more than just pick each letter separately at random. And, for example, we know that if we have a \u201cq\u201d, the next letter basically has to be \u201cu\u201d.   Here\u2019s a plot of the probabilities for letters on their own: And here\u2019s a plot that shows the probabilities of pairs of letters (\u201c2-grams\u201d) in typical English text. The possible first letters are shown across the page, the second letters down the page:   And we see here, for example, that the \u201cq\u201d column is blank (zero probability) except on the \u201cu\u201d row. OK, so now instead of generating our \u201cwords\u201d a single letter at a time, let\u2019s generate them looking at two letters at a time, using these \u201c2-gram\u201d probabilities. Here\u2019s a sample of the result\u2014which happens to include a few \u201cactual words\u201d: With sufficiently much English text we can get pretty good estimates not just for probabilities of single letters or pairs of letters (2-grams), but also for longer runs of letters. And if we generate \u201crandom words\u201d with progressively longer n-gram probabilities, we see that they get progressively \u201cmore realistic\u201d: But let\u2019s now assume\u2014more or less as ChatGPT does\u2014that we\u2019re dealing with whole words, not letters.", "There are about 40,000 reasonably commonly used words in English. And by looking at a large corpus of English text (say a few million books, with altogether a few hundred billion words), we can get an estimate of how common each word is. And using this we can start generating \u201csentences\u201d, in which each word is independently picked at random, with the same probability that it appears in the corpus. Here\u2019s a sample of what we get: Not surprisingly, this is nonsense. So how can we do better? Just like with letters, we can start taking into account not just probabilities for single words but probabilities for pairs or longer n-grams of words. Doing this for pairs, here are 5 examples of what we get, in all cases starting from the word \u201ccat\u201d: It\u2019s getting slightly more \u201csensible looking\u201d. And we might imagine that if we were able to use sufficiently long n-grams we\u2019d basically \u201cget a ChatGPT\u201d\u2014in the sense that we\u2019d get something that would generate essay-length sequences of words with the \u201ccorrect overall essay probabilities\u201d. But here\u2019s the problem: there just isn\u2019t even close to enough English text that\u2019s ever been written to be able to deduce those probabilities.   In a crawl of the web there might be a few hundred billion words; in books that have been digitized there might be another hundred billion words. But with 40,000 common words, even the number of possible 2-grams is already 1.6 billion\u2014and the number of possible 3-grams is 60 trillion. So there\u2019s no way we can estimate the probabilities even for all of these from text that\u2019s out there. And by the time we get to \u201cessay fragments\u201d of 20 words, the number of possibilities is larger than the number of particles in the universe, so in a sense they could never all be written down. So what can we do? The big idea is to make a model that lets us estimate the probabilities with which sequences should occur\u2014even though we\u2019ve never explicitly seen those sequences in the corpus of text we\u2019ve looked at.", "And at the core of ChatGPT is precisely a so-called \u201clarge language model\u201d (LLM) that\u2019s been built to do a good job of estimating those probabilities.   Say you want to know (as Galileo did back in the late 1500s) how long it\u2019s going to take a cannon ball dropped from each floor of the Tower of Pisa to hit the ground. Well, you could just measure it in each case and make a table of the results. Or you could do what is the essence of theoretical science: make a model that gives some kind of procedure for computing the answer rather than just measuring and remembering each case. Let\u2019s imagine we have (somewhat idealized) data for how long the cannon ball takes to fall from various floors: How do we figure out how long it\u2019s going to take to fall from a floor we don\u2019t explicitly have data about? In this particular case, we can use known laws of physics to work it out. But say all we\u2019ve got is the data, and we don\u2019t know what underlying laws govern it. Then we might make a mathematical guess, like that perhaps we should use a straight line as a model: We could pick different straight lines. But this is the one that\u2019s on average closest to the data we\u2019re given. And from this straight line we can estimate the time to fall for any floor. How did we know to try using a straight line here? At some level we didn\u2019t. It\u2019s just something that\u2019s mathematically simple, and we\u2019re used to the fact that lots of data we measure turns out to be well fit by mathematically simple things. We could try something mathematically more complicated\u2014say a + b x + c x2\u2014and then in this case we do better: Things can go quite wrong, though. Like here\u2019s the best we can do with a + b/x + c sin(x): It is worth understanding that there\u2019s never a \u201cmodel-less model\u201d. Any model you use has some particular underlying structure\u2014then a certain set of \u201cknobs you can turn\u201d (i.e. parameters you can set) to fit your data. And in the case of ChatGPT, lots of such \u201cknobs\u201d are used\u2014actually, 175 billion of them.", "But the remarkable thing is that the underlying structure of ChatGPT\u2014with \u201cjust\u201d that many parameters\u2014is sufficient to make a model that computes next-word probabilities \u201cwell enough\u201d to give us reasonable essay-length pieces of text. The example we gave above involves making a model for numerical data that essentially comes from simple physics\u2014where we\u2019ve known for several centuries that \u201csimple mathematics applies\u201d. But for ChatGPT we have to make a model of human-language text of the kind produced by a human brain. And for something like that we don\u2019t (at least yet) have anything like \u201csimple mathematics\u201d. So what might a model of it be like? Before we talk about language, let\u2019s talk about another human-like task: recognizing images. And as a simple example of this, let\u2019s consider images of digits (and, yes, this is a classic machine learning example): One thing we could do is get a bunch of sample images for each digit: Then to find out if an image we\u2019re given as input corresponds to a particular digit we could just do an explicit pixel-by-pixel comparison with the samples we have. But as humans we certainly seem to do something better\u2014because we can still recognize digits, even when they\u2019re for example handwritten, and have all sorts of modifications and distortions: When we made a model for our numerical data above, we were able to take a numerical value x that we were given, and just compute a + b x for particular a and b. So if we treat the gray-level value of each pixel here as some variable xi is there some function of all those variables that\u2014when evaluated\u2014tells us what digit the image is of? It turns out that it\u2019s possible to construct such a function. Not surprisingly, it\u2019s not particularly simple, though. And a typical example might involve perhaps half a million mathematical operations.   But the end result is that if we feed the collection of pixel values for an image into this function, out will come the number specifying which digit we have an image of.", "Later, we\u2019ll talk about how such a function can be constructed, and the idea of neural nets. But for now let\u2019s treat the function as black box, where we feed in images of, say, handwritten digits (as arrays of pixel values) and we get out the numbers these correspond to: But what\u2019s really going on here? Let\u2019s say we progressively blur a digit. For a little while our function still \u201crecognizes\u201d it, here as a \u201c2\u201d. But soon it \u201closes it\u201d, and starts giving the \u201cwrong\u201d result: But why do we say it\u2019s the \u201cwrong\u201d result? In this case, we know we got all the images by blurring a \u201c2\u201d. But if our goal is to produce a model of what humans can do in recognizing images, the real question to ask is what a human would have done if presented with one of those blurred images, without knowing where it came from. And we have a \u201cgood model\u201d if the results we get from our function typically agree with what a human would say. And the nontrivial scientific fact is that for an image-recognition task like this we now basically know how to construct functions that do this. Can we \u201cmathematically prove\u201d that they work? Well, no. Because to do that we\u2019d have to have a mathematical theory of what we humans are doing. Take the \u201c2\u201d image and change a few pixels. We might imagine that with only a few pixels \u201cout of place\u201d we should still consider the image a \u201c2\u201d. But how far should that go? It\u2019s a question of human visual perception. And, yes, the answer would no doubt be different for bees or octopuses\u2014and potentially utterly different for putative aliens. OK, so how do our typical models for tasks like image recognition actually work? The most popular\u2014and successful\u2014current approach uses neural nets. Invented\u2014in a form remarkably close to their use today\u2014in the 1940s, neural nets can be thought of as simple idealizations of how brains seem to work.   In human brains there are about 100 billion neurons (nerve cells), each capable of producing an electrical pulse up to perhaps a thousand times a second.", "The neurons are connected in a complicated net, with each neuron having tree-like branches allowing it to pass electrical signals to perhaps thousands of other neurons. And in a rough approximation, whether any given neuron produces an electrical pulse at a given moment depends on what pulses it\u2019s received from other neurons\u2014with different connections contributing with different \u201cweights\u201d. When we \u201csee an image\u201d what\u2019s happening is that when photons of light from the image fall on (\u201cphotoreceptor\u201d) cells at the back of our eyes they produce electrical signals in nerve cells. These nerve cells are connected to other nerve cells, and eventually the signals go through a whole sequence of layers of neurons. And it\u2019s in this process that we \u201crecognize\u201d the image, eventually \u201cforming the thought\u201d that we\u2019re \u201cseeing a 2\u201d (and maybe in the end doing something like saying the word \u201ctwo\u201d out loud).   The \u201cblack-box\u201d function from the previous section is a \u201cmathematicized\u201d version of such a neural net. It happens to have 11 layers (though only 4 \u201ccore layers\u201d): There\u2019s nothing particularly \u201ctheoretically derived\u201d about this neural net; it\u2019s just something that\u2014back in 1998\u2014was constructed as a piece of engineering, and found to work. (Of course, that\u2019s not much different from how we might describe our brains as having been produced through the process of biological evolution.)   OK, but how does a neural net like this \u201crecognize things\u201d? The key is the notion of attractors. Imagine we\u2019ve got handwritten images of 1\u2019s and 2\u2019s: We somehow want all the 1\u2019s to \u201cbe attracted to one place\u201d, and all the 2\u2019s to \u201cbe attracted to another place\u201d. Or, put a different way, if an image is somehow \u201ccloser to being a 1\u201d than to being a 2, we want it to end up in the \u201c1 place\u201d and vice versa.   As a straightforward analogy, let\u2019s say we have certain positions in the plane, indicated by dots (in a real-life setting they might be positions of coffee shops).", "Then we might imagine that starting from any point on the plane we\u2019d always want to end up at the closest dot (i.e. we\u2019d always go to the closest coffee shop). We can represent this by dividing the plane into regions (\u201cattractor basins\u201d) separated by idealized \u201cwatersheds\u201d: We can think of this as implementing a kind of \u201crecognition task\u201d in which we\u2019re not doing something like identifying what digit a given image \u201clooks most like\u201d\u2014but rather we\u2019re just, quite directly, seeing what dot a given point is closest to. (The \u201cVoronoi diagram\u201d setup we\u2019re showing here separates points in 2D Euclidean space; the digit recognition task can be thought of as doing something very similar\u2014but in a 784-dimensional space formed from the gray levels of all the pixels in each image.) So how do we make a neural net \u201cdo a recognition task\u201d? Let\u2019s consider this very simple case: Our goal is to take an \u201cinput\u201d corresponding to a position {x,y}\u2014and then to \u201crecognize\u201d it as whichever of the three points it\u2019s closest to. Or, in other words, we want the neural net to compute a function of {x,y} like: So how do we do this with a neural net? Ultimately a neural net is a connected collection of idealized \u201cneurons\u201d\u2014usually arranged in layers\u2014with a simple example being: Each \u201cneuron\u201d is effectively set up to evaluate a simple numerical function. And to \u201cuse\u201d the network, we simply feed numbers (like our coordinates x and y) in at the top, then have neurons on each layer \u201cevaluate their functions\u201d and feed the results forward through the network\u2014eventually producing the final result at the bottom: In the traditional (biologically inspired) setup each neuron effectively has a certain set of \u201cincoming connections\u201d from the neurons on the previous layer, with each connection being assigned a certain \u201cweight\u201d (which can be a positive or negative number).", "The value of a given neuron is determined by multiplying the values of \u201cprevious neurons\u201d by their corresponding weights, then adding these up and adding a constant\u2014and finally applying a \u201cthresholding\u201d (or \u201cactivation\u201d) function. In mathematical terms, if a neuron has inputs x = {x1, x2 \u2026} then we compute f[w . x + b], where the weights w and constant b are generally chosen differently for each neuron in the network; the function f is usually the same. Computing w . x + b is just a matter of matrix multiplication and addition. The \u201cactivation function\u201d f introduces nonlinearity (and ultimately is what leads to nontrivial behavior). Various activation functions commonly get used; here we\u2019ll just use Ramp (or ReLU): For each task we want the neural net to perform (or, equivalently, for each overall function we want it to evaluate) we\u2019ll have different choices of weights. (And\u2014as we\u2019ll discuss later\u2014these weights are normally determined by \u201ctraining\u201d the neural net using machine learning from examples of the outputs we want.) Ultimately, every neural net just corresponds to some overall mathematical function\u2014though it may be messy to write out. For the example above, it would be: The neural net of ChatGPT also just corresponds to a mathematical function like this\u2014but effectively with billions of terms.   But let\u2019s go back to individual neurons. Here are some examples of the functions a neuron with two inputs (representing coordinates x and y) can compute with various choices of weights and constants (and Ramp as activation function): But what about the larger network from above? Well, here\u2019s what it computes: It\u2019s not quite \u201cright\u201d, but it\u2019s close to the \u201cnearest point\u201d function we showed above. Let\u2019s see what happens with some other neural nets. In each case, as we\u2019ll explain later, we\u2019re using machine learning to find the best choice of weights. Then we\u2019re showing here what the neural net with those weights computes: Bigger networks generally do better at approximating the function we\u2019re aiming for.", "And in the \u201cmiddle of each attractor basin\u201d we typically get exactly the answer we want. But at the boundaries\u2014where the neural net \u201chas a hard time making up its mind\u201d\u2014things can be messier.   With this simple mathematical-style \u201crecognition task\u201d it\u2019s clear what the \u201cright answer\u201d is. But in the problem of recognizing handwritten digits, it\u2019s not so clear. What if someone wrote a \u201c2\u201d so badly it looked like a \u201c7\u201d, etc.? Still, we can ask how a neural net distinguishes digits\u2014and this gives an indication: Can we say \u201cmathematically\u201d how the network makes its distinctions? Not really. It\u2019s just \u201cdoing what the neural net does\u201d. But it turns out that that normally seems to agree fairly well with the distinctions we humans make.   Let\u2019s take a more elaborate example. Let\u2019s say we have images of cats and dogs. And we have a neural net that\u2019s been trained to distinguish them. Here\u2019s what it might do on some examples:  Now it\u2019s even less clear what the \u201cright answer\u201d is. What about a dog dressed in a cat suit? Etc. Whatever input it\u2019s given, the neural net is generating an answer. And, it turns out, to do it in a way that\u2019s reasonably consistent with what humans might do. As I\u2019ve said above, that\u2019s not a fact we can \u201cderive from first principles\u201d. It\u2019s just something that\u2019s empirically been found to be true, at least in certain domains. But it\u2019s a key reason why neural nets are useful: that they somehow capture a \u201chuman-like\u201d way of doing things. Show yourself a picture of a cat, and ask \u201cWhy is that a cat?\u201d. Maybe you\u2019d start saying \u201cWell, I see its pointy ears, etc.\u201d But it\u2019s not very easy to explain how you recognized the image as a cat. It\u2019s just that somehow your brain figured that out. But for a brain there\u2019s no way (at least yet) to \u201cgo inside\u201d and see how it figured it out. What about for an (artificial) neural net? Well, it\u2019s straightforward to see what each \u201cneuron\u201d does when you show a picture of a cat. But even to get a basic visualization is usually very difficult.", "In the final net that we used for the \u201cnearest point\u201d problem above there are 17 neurons. In the net for recognizing handwritten digits there are 2190. And in the net we\u2019re using to recognize cats and dogs there are 60,650. Normally it would be pretty difficult to visualize what amounts to 60,650-dimensional space. But because this is a network set up to deal with images, many of its layers of neurons are organized into arrays, like the arrays of pixels it\u2019s looking at. And if we take a typical cat image  then we can represent the states of neurons at the first layer by a collection of derived images\u2014many of which we can readily interpret as being things like \u201cthe cat without its background\u201d, or \u201cthe outline of the cat\u201d: By the 10th layer it\u2019s harder to interpret what\u2019s going on: But in general we might say that the neural net is \u201cpicking out certain features\u201d (maybe pointy ears are among them), and using these to determine what the image is of. But are those features ones for which we have names\u2014like \u201cpointy ears\u201d? Mostly not.   Are our brains using similar features? Mostly we don\u2019t know. But it\u2019s notable that the first few layers of a neural net like the one we\u2019re showing here seem to pick out aspects of images (like edges of objects) that seem to be similar to ones we know are picked out by the first level of visual processing in brains. But let\u2019s say we want a \u201ctheory of cat recognition\u201d in neural nets. We can say: \u201cLook, this particular net does it\u201d\u2014and immediately that gives us some sense of \u201chow hard a problem\u201d it is (and, for example, how many neurons or layers might be needed). But at least as of now we don\u2019t have a way to \u201cgive a narrative description\u201d of what the network is doing. And maybe that\u2019s because it truly is computationally irreducible, and there\u2019s no general way to find what it does except by explicitly tracing each step. Or maybe it\u2019s just that we haven\u2019t \u201cfigured out the science\u201d, and identified the \u201cnatural laws\u201d that allow us to summarize what\u2019s going on.", "We\u2019ll encounter the same kinds of issues when we talk about generating language with ChatGPT. And again it\u2019s not clear whether there are ways to \u201csummarize what it\u2019s doing\u201d. But the richness and detail of language (and our experience with it) may allow us to get further than with images. We\u2019ve been talking so far about neural nets that \u201calready know\u201d how to do particular tasks. But what makes neural nets so useful (presumably also in brains) is that not only can they in principle do all sorts of tasks, but they can be incrementally \u201ctrained from examples\u201d to do those tasks. When we make a neural net to distinguish cats from dogs we don\u2019t effectively have to write a program that (say) explicitly finds whiskers; instead we just show lots of examples of what\u2019s a cat and what\u2019s a dog, and then have the network \u201cmachine learn\u201d from these how to distinguish them. And the point is that the trained network \u201cgeneralizes\u201d from the particular examples it\u2019s shown. Just as we\u2019ve seen above, it isn\u2019t simply that the network recognizes the particular pixel pattern of an example cat image it was shown; rather it\u2019s that the neural net somehow manages to distinguish images on the basis of what we consider to be some kind of \u201cgeneral catness\u201d.   So how does neural net training actually work? Essentially what we\u2019re always trying to do is to find weights that make the neural net successfully reproduce the examples we\u2019ve given. And then we\u2019re relying on the neural net to \u201cinterpolate\u201d (or \u201cgeneralize\u201d) \u201cbetween\u201d these examples in a \u201creasonable\u201d way. Let\u2019s look at a problem even simpler than the nearest-point one above. Let\u2019s just try to get a neural net to learn the function:  For this task, we\u2019ll need a network that has just one input and one output, like:  But what weights, etc. should we be using? With every possible set of weights the neural net will compute some function. And, for example, here\u2019s what it does with a few randomly chosen sets of weights:", "And, yes, we can plainly see that in none of these cases does it get even close to reproducing the function we want. So how do we find weights that will reproduce the function? The basic idea is to supply lots of \u201cinput \u2192 output\u201d examples to \u201clearn from\u201d\u2014and then to try to find weights that will reproduce these examples. Here\u2019s the result of doing that with progressively more examples: At each stage in this \u201ctraining\u201d the weights in the network are progressively adjusted\u2014and we see that eventually we get a network that successfully reproduces the function we want. So how do we adjust the weights? The basic idea is at each stage to see \u201chow far away we are\u201d from getting the function we want\u2014and then to update the weights in such a way as to get closer.   To find out \u201chow far away we are\u201d we compute what\u2019s usually called a \u201closs function\u201d (or sometimes \u201ccost function\u201d). Here we\u2019re using a simple (L2) loss function that\u2019s just the sum of the squares of the differences between the values we get, and the true values. And what we see is that as our training process progresses, the loss function progressively decreases (following a certain \u201clearning curve\u201d that\u2019s different for different tasks)\u2014until we reach a point where the network (at least to a good approximation) successfully reproduces the function we want: Alright, so the last essential piece to explain is how the weights are adjusted to reduce the loss function. As we\u2019ve said, the loss function gives us a \u201cdistance\u201d between the values we\u2019ve got, and the true values. But the \u201cvalues we\u2019ve got\u201d are determined at each stage by the current version of neural net\u2014and by the weights in it. But now imagine that the weights are variables\u2014say wi. We want to find out how to adjust the values of these variables to minimize the loss that depends on them. For example, imagine (in an incredible simplification of typical neural nets used in practice) that we have just two weights w1  and w2.", "Then we might have a loss that as a function of w1 and w2 looks like this: Numerical analysis provides a variety of techniques for finding the minimum in cases like this. But a typical approach is just to progressively follow the path of steepest descent from whatever previous w1, w2 we had: Like water flowing down a mountain, all that\u2019s guaranteed is that this procedure will end up at some local minimum of the surface (\u201ca mountain lake\u201d); it might well not reach the ultimate global minimum. It\u2019s not obvious that it would be feasible to find the path of the steepest descent on the \u201cweight landscape\u201d. But calculus comes to the rescue. As we mentioned above, one can always think of a neural net as computing a mathematical function\u2014that depends on its inputs, and its weights. But now consider differentiating with respect to these weights. It turns out that the chain rule of calculus in effect lets us \u201cunravel\u201d the operations done by successive layers in the neural net. And the result is that we can\u2014at least in some local approximation\u2014\u201cinvert\u201d the operation of the neural net, and progressively find weights that minimize the loss associated with the output. The picture above shows the kind of minimization we might need to do in the unrealistically simple case of just 2 weights. But it turns out that even with many more weights (ChatGPT uses 175 billion) it\u2019s still possible to do the minimization, at least to some level of approximation. And in fact the big breakthrough in \u201cdeep learning\u201d that occurred around 2011 was associated with the discovery that in some sense it can be easier to do (at least approximate) minimization when there are lots of weights involved than when there are fairly few. In other words\u2014somewhat counterintuitively\u2014it can be easier to solve more complicated problems with neural nets than simpler ones.", "And the rough reason for this seems to be that when one has a lot of \u201cweight variables\u201d one has a high-dimensional space with \u201clots of different directions\u201d that can lead one to the minimum\u2014whereas with fewer variables it\u2019s easier to end up getting stuck in a local minimum (\u201cmountain lake\u201d) from which there\u2019s no \u201cdirection to get out\u201d. It\u2019s worth pointing out that in typical cases there are many different collections of weights that will all give neural nets that have pretty much the same performance. And usually in practical neural net training there are lots of random choices made\u2014that lead to \u201cdifferent-but-equivalent solutions\u201d, like these: But each such \u201cdifferent solution\u201d will have at least slightly different behavior. And if we ask, say, for an \u201cextrapolation\u201d outside the region where we gave training examples, we can get dramatically different results: But which of these is \u201cright\u201d? There\u2019s really no way to say. They\u2019re all \u201cconsistent with the observed data\u201d. But they all correspond to different \u201cinnate\u201d ways to \u201cthink about\u201d what to do \u201coutside the box\u201d. And some may seem \u201cmore reasonable\u201d to us humans than others.   Particularly over the past decade, there\u2019ve been many advances in the art of training neural nets. And, yes, it is basically an art. Sometimes\u2014especially in retrospect\u2014one can see at least a glimmer of a \u201cscientific explanation\u201d for something that\u2019s being done. But mostly things have been discovered by trial and error, adding ideas and tricks that have progressively built a significant lore about how to work with neural nets. There are several key parts. First, there\u2019s the matter of what architecture of neural net one should use for a particular task. Then there\u2019s the critical issue of how one\u2019s going to get the data on which to train the neural net. And increasingly one isn\u2019t dealing with training a net from scratch: instead a new net can either directly incorporate another already-trained net, or at least can use that net to generate more training examples for itself.", "One might have thought that for every particular kind of task one would need a different architecture of neural net. But what\u2019s been found is that the same architecture often seems to work even for apparently quite different tasks. At some level this reminds one of the idea of universal computation (and my Principle of Computational Equivalence), but, as I\u2019ll discuss later, I think it\u2019s more a reflection of the fact that the tasks we\u2019re typically trying to get neural nets to do are \u201chuman-like\u201d ones\u2014and neural nets can capture quite general \u201chuman-like processes\u201d. In earlier days of neural nets, there tended to be the idea that one should \u201cmake the neural net do as little as possible\u201d. For example, in converting speech to text it was thought that one should first analyze the audio of the speech, break it into phonemes, etc. But what was found is that\u2014at least for \u201chuman-like tasks\u201d\u2014it\u2019s usually better just to try to train the neural net on the \u201cend-to-end problem\u201d, letting it \u201cdiscover\u201d the necessary intermediate features, encodings, etc. for itself. There was also the idea that one should introduce complicated individual components into the neural net, to let it in effect \u201cexplicitly implement particular algorithmic ideas\u201d. But once again, this has mostly turned out not to be worthwhile; instead, it\u2019s better just to deal with very simple components and let them \u201corganize themselves\u201d (albeit usually in ways we can\u2019t understand) to achieve (presumably) the equivalent of those algorithmic ideas.   That\u2019s not to say that there are no \u201cstructuring ideas\u201d that are relevant for neural nets. Thus, for example, having 2D arrays of neurons with local connections seems at least very useful in the early stages of processing images. And having patterns of connectivity that concentrate on \u201clooking back in sequences\u201d seems useful\u2014as we\u2019ll see later\u2014in dealing with things like human language, for example in ChatGPT.", "But an important feature of neural nets is that\u2014like computers in general\u2014they\u2019re ultimately just dealing with data. And current neural nets\u2014with current approaches to neural net training\u2014specifically deal with arrays of numbers. But in the course of processing, those arrays can be completely rearranged and reshaped. And as an example, the network we used for identifying digits above starts with a 2D \u201cimage-like\u201d array, quickly \u201cthickening\u201d to many channels, but then \u201cconcentrating down\u201d into a 1D array that will ultimately contain elements representing the different possible output digits: But, OK, how can one tell how big a neural net one will need for a particular task? It\u2019s something of an art. At some level the key thing is to know \u201chow hard the task is\u201d. But for human-like tasks that\u2019s typically very hard to estimate. Yes, there may be a systematic way to do the task very \u201cmechanically\u201d by computer. But it\u2019s hard to know if there are what one might think of as tricks or shortcuts that allow one to do the task at least at a \u201chuman-like level\u201d vastly more easily. It might take enumerating a giant game tree to \u201cmechanically\u201d play a certain game; but there might be a much easier (\u201cheuristic\u201d) way to achieve \u201chuman-level play\u201d. When one\u2019s dealing with tiny neural nets and simple tasks one can sometimes explicitly see that one \u201ccan\u2019t get there from here\u201d. For example, here\u2019s the best one seems to be able to do on the task from the previous section with a few small neural nets:   And what we see is that if the net is too small, it just can\u2019t reproduce the function we want. But above some size, it has no problem\u2014at least if one trains it for long enough, with enough examples. And, by the way, these pictures illustrate a piece of neural net lore: that one can often get away with a smaller network if there\u2019s a \u201csqueeze\u201d in the middle that forces everything to go through a smaller intermediate number of neurons.", "(It\u2019s also worth mentioning that \u201cno-intermediate-layer\u201d\u2014or so-called \u201cperceptron\u201d\u2014networks can only learn essentially linear functions\u2014but as soon as there\u2019s even one intermediate layer it\u2019s always in principle possible to approximate any function arbitrarily well, at least if one has enough neurons, though to make it feasibly trainable one typically has some kind of regularization or normalization.) OK, so let\u2019s say one\u2019s settled on a certain neural net architecture. Now there\u2019s the issue of getting data to train the network with. And many of the practical challenges around neural nets\u2014and machine learning in general\u2014center on acquiring or preparing the necessary training data. In many cases (\u201csupervised learning\u201d) one wants to get explicit examples of inputs and the outputs one is expecting from them. Thus, for example, one might want images tagged by what\u2019s in them, or some other attribute. And maybe one will have to explicitly go through\u2014usually with great effort\u2014and do the tagging. But very often it turns out to be possible to piggyback on something that\u2019s already been done, or use it as some kind of proxy. And so, for example, one might use alt tags that have been provided for images on the web. Or, in a different domain, one might use closed captions that have been created for videos. Or\u2014for language translation training\u2014one might use parallel versions of webpages or other documents that exist in different languages. How much data do you need to show a neural net to train it for a particular task? Again, it\u2019s hard to estimate from first principles. Certainly the requirements can be dramatically reduced by using \u201ctransfer learning\u201d to \u201ctransfer in\u201d things like lists of important features that have already been learned in another network. But generally neural nets need to \u201csee a lot of examples\u201d to train well. And at least for some tasks it\u2019s an important piece of neural net lore that the examples can be incredibly repetitive.", "And indeed it\u2019s a standard strategy to just show a neural net all the examples one has, over and over again. In each of these \u201ctraining rounds\u201d (or \u201cepochs\u201d) the neural net will be in at least a slightly different state, and somehow \u201creminding it\u201d of a particular example is useful in getting it to \u201cremember that example\u201d. (And, yes, perhaps this is analogous to the usefulness of repetition in human memorization.) But often just repeating the same example over and over again isn\u2019t enough. It\u2019s also necessary to show the neural net variations of the example. And it\u2019s a feature of neural net lore that those \u201cdata augmentation\u201d variations don\u2019t have to be sophisticated to be useful. Just slightly modifying images with basic image processing can make them essentially \u201cas good as new\u201d for neural net training. And, similarly, when one\u2019s run out of actual video, etc. for training self-driving cars, one can go on and just get data from running simulations in a model videogame-like environment without all the detail of actual real-world scenes. How about something like ChatGPT? Well, it has the nice feature that it can do \u201cunsupervised learning\u201d, making it much easier to get it examples to train from. Recall that the basic task for ChatGPT is to figure out how to continue a piece of text that it\u2019s been given. So to get it \u201ctraining examples\u201d all one has to do is get a piece of text, and mask out the end of it, and then use this as the \u201cinput to train from\u201d\u2014with the \u201coutput\u201d being the complete, unmasked piece of text. We\u2019ll discuss this more later, but the main point is that\u2014unlike, say, for learning what\u2019s in images\u2014there\u2019s no \u201cexplicit tagging\u201d needed; ChatGPT can in effect just learn directly from whatever examples of text it\u2019s given. OK, so what about the actual learning process in a neural net? In the end it\u2019s all about determining what weights will best capture the training examples that have been given.", "And there are all sorts of detailed choices and \u201chyperparameter settings\u201d (so called because the weights can be thought of as \u201cparameters\u201d) that can be used to tweak how this is done. There are different choices of loss function (sum of squares, sum of absolute values, etc.). There are different ways to do loss minimization (how far in weight space to move at each step, etc.). And then there are questions like how big a \u201cbatch\u201d of examples to show to get each successive estimate of the loss one\u2019s trying to minimize. And, yes, one can apply machine learning (as we do, for example, in Wolfram Language) to automate machine learning\u2014and to automatically set things like hyperparameters. But in the end the whole process of training can be characterized by seeing how the loss progressively decreases (as in this Wolfram Language progress monitor for a small training): And what one typically sees is that the loss decreases for a while, but eventually flattens out at some constant value. If that value is sufficiently small, then the training can be considered successful; otherwise it\u2019s probably a sign one should try changing the network architecture.   Can one tell how long it should take for the \u201clearning curve\u201d to flatten out? Like for so many other things, there seem to be approximate power-law scaling relationships that depend on the size of neural net and amount of data one\u2019s using. But the general conclusion is that training a neural net is hard\u2014and takes a lot of computational effort. And as a practical matter, the vast majority of that effort is spent doing operations on arrays of numbers, which is what GPUs are good at\u2014which is why neural net training is typically limited by the availability of GPUs.   In the future, will there be fundamentally better ways to train neural nets\u2014or generally do what neural nets do? Almost certainly, I think.", "The fundamental idea of neural nets is to create a flexible \u201ccomputing fabric\u201d out of a large number of simple (essentially identical) components\u2014and to have this \u201cfabric\u201d be one that can be incrementally modified to learn from examples. In current neural nets, one\u2019s essentially using the ideas of calculus\u2014applied to real numbers\u2014to do that incremental modification. But it\u2019s increasingly clear that having high-precision numbers doesn\u2019t matter; 8 bits or less might be enough even with current methods.   With computational systems like cellular automata that basically operate in parallel on many individual bits it\u2019s never been clear how to do this kind of incremental modification, but there\u2019s no reason to think it isn\u2019t possible. And in fact, much like with the \u201cdeep-learning breakthrough of 2012\u201d it may be that such incremental modification will effectively be easier in more complicated cases than in simple ones. Neural nets\u2014perhaps a bit like brains\u2014are set up to have an essentially fixed network of neurons, with what\u2019s modified being the strength (\u201cweight\u201d) of connections between them. (Perhaps in at least young brains significant numbers of wholly new connections can also grow.) But while this might be a convenient setup for biology, it\u2019s not at all clear that it\u2019s even close to the best way to achieve the functionality we need. And something that involves the equivalent of progressive network rewriting (perhaps reminiscent of our Physics Project) might well ultimately be better. But even within the framework of existing neural nets there\u2019s currently a crucial limitation: neural net training as it\u2019s now done is fundamentally sequential, with the effects of each batch of examples being propagated back to update the weights. And indeed with current computer hardware\u2014even taking into account GPUs\u2014most of a neural net is \u201cidle\u201d most of the time during training, with just one part at a time being updated.", "And in a sense this is because our current computers tend to have memory that is separate from their CPUs (or GPUs). But in brains it\u2019s presumably different\u2014with every \u201cmemory element\u201d (i.e. neuron) also being a potentially active computational element. And if we could set up our future computer hardware this way it might become possible to do training much more efficiently. The capabilities of something like ChatGPT seem so impressive that one might imagine that if one could just \u201ckeep going\u201d and train larger and larger neural networks, then they\u2019d eventually be able to \u201cdo everything\u201d. And if one\u2019s concerned with things that are readily accessible to immediate human thinking, it\u2019s quite possible that this is the case. But the lesson of the past several hundred years of science is that there are things that can be figured out by formal processes, but aren\u2019t readily accessible to immediate human thinking. Nontrivial mathematics is one big example. But the general case is really computation. And ultimately the issue is the phenomenon of computational irreducibility. There are some computations which one might think would take many steps to do, but which can in fact be \u201creduced\u201d to something quite immediate. But the discovery of computational irreducibility implies that this doesn\u2019t always work. And instead there are processes\u2014probably like the one below\u2014where to work out what happens inevitably requires essentially tracing each computational step: The kinds of things that we normally do with our brains are presumably specifically chosen to avoid computational irreducibility. It takes special effort to do math in one\u2019s brain. And it\u2019s in practice largely impossible to \u201cthink through\u201d the steps in the operation of any nontrivial program just in one\u2019s brain.   But of course for that we have computers. And with computers we can readily do long, computationally irreducible things. And the key point is that there\u2019s in general no shortcut for these.", "Yes, we could memorize lots of specific examples of what happens in some particular computational system. And maybe we could even see some (\u201ccomputationally reducible\u201d) patterns that would allow us to do a little generalization. But the point is that computational irreducibility means that we can never guarantee that the unexpected won\u2019t happen\u2014and it\u2019s only by explicitly doing the computation that you can tell what actually happens in any particular case. And in the end there\u2019s just a fundamental tension between learnability and computational irreducibility. Learning involves in effect compressing data by leveraging regularities. But computational irreducibility implies that ultimately there\u2019s a limit to what regularities there may be. As a practical matter, one can imagine building little computational devices\u2014like cellular automata or Turing machines\u2014into trainable systems like neural nets. And indeed such devices can serve as good \u201ctools\u201d for the neural net\u2014like Wolfram|Alpha can be a good tool for ChatGPT. But computational irreducibility implies that one can\u2019t expect to \u201cget inside\u201d those devices and have them learn. Or put another way, there\u2019s an ultimate tradeoff between capability and trainability: the more you want a system to make \u201ctrue use\u201d of its computational capabilities, the more it\u2019s going to show computational irreducibility, and the less it\u2019s going to be trainable. And the more it\u2019s fundamentally trainable, the less it\u2019s going to be able to do sophisticated computation. (For ChatGPT as it currently is, the situation is actually much more extreme, because the neural net used to generate each token of output is a pure \u201cfeed-forward\u201d network, without loops, and therefore has no ability to do any kind of computation with nontrivial \u201ccontrol flow\u201d.) Of course, one might wonder whether it\u2019s actually important to be able to do irreducible computations. And indeed for much of human history it wasn\u2019t particularly important.", "But our modern technological world has been built on engineering that makes use of at least mathematical computations\u2014and increasingly also more general computations. And if we look at the natural world, it\u2019s full of irreducible computation\u2014that we\u2019re slowly understanding how to emulate and use for our technological purposes.   Yes, a neural net can certainly notice the kinds of regularities in the natural world that we might also readily notice with \u201cunaided human thinking\u201d. But if we want to work out things that are in the purview of mathematical or computational science the neural net isn\u2019t going to be able to do it\u2014unless it effectively \u201cuses as a tool\u201d an \u201cordinary\u201d computational system. But there\u2019s something potentially confusing about all of this. In the past there were plenty of tasks\u2014including writing essays\u2014that we\u2019ve assumed were somehow \u201cfundamentally too hard\u201d for computers. And now that we see them done by the likes of ChatGPT we tend to suddenly think that computers must have become vastly more powerful\u2014in particular surpassing things they were already basically able to do (like progressively computing the behavior of computational systems like cellular automata).   But this isn\u2019t the right conclusion to draw. Computationally irreducible processes are still computationally irreducible, and are still fundamentally hard for computers\u2014even if computers can readily compute their individual steps. And instead what we should conclude is that tasks\u2014like writing essays\u2014that we humans could do, but we didn\u2019t think computers could do, are actually in some sense computationally easier than we thought.   In other words, the reason a neural net can be successful in writing an essay is because writing an essay turns out to be a \u201ccomputationally shallower\u201d problem than we thought. And in a sense this takes us closer to \u201chaving a theory\u201d of how we humans manage to do things like writing essays, or in general deal with language.", "If you had a big enough neural net then, yes, you might be able to do whatever humans can readily do. But you wouldn\u2019t capture what the natural world in general can do\u2014or that the tools that we\u2019ve fashioned from the natural world can do. And it\u2019s the use of those tools\u2014both practical and conceptual\u2014that have allowed us in recent centuries to transcend the boundaries of what\u2019s accessible to \u201cpure unaided human thought\u201d, and capture for human purposes more of what\u2019s out there in the physical and computational universe. Neural nets\u2014at least as they\u2019re currently set up\u2014are fundamentally based on numbers. So if we\u2019re going to to use them to work on something like text we\u2019ll need a way to represent our text with numbers. And certainly we could start (essentially as ChatGPT does) by just assigning a number to every word in the dictionary. But there\u2019s an important idea\u2014that\u2019s for example central to ChatGPT\u2014that goes beyond that. And it\u2019s the idea of \u201cembeddings\u201d. One can think of an embedding as a way to try to represent the \u201cessence\u201d of something by an array of numbers\u2014with the property that \u201cnearby things\u201d are represented by nearby numbers. And so, for example, we can think of a word embedding as trying to lay out words in a kind of \u201cmeaning space\u201d in which words that are somehow \u201cnearby in meaning\u201d appear nearby in the embedding. The actual embeddings that are used\u2014say in ChatGPT\u2014tend to involve large lists of numbers. But if we project down to 2D, we can show examples of how words are laid out by the embedding:   And, yes, what we see does remarkably well in capturing typical everyday impressions. But how can we construct such an embedding? Roughly the idea is to look at large amounts of text (here 5 billion words from the web) and then see \u201chow similar\u201d the \u201cenvironments\u201d are in which different words appear. So, for example, \u201calligator\u201d and \u201ccrocodile\u201d will often appear almost interchangeably in otherwise similar sentences, and that means they\u2019ll be placed nearby in the embedding.", "But \u201cturnip\u201d and \u201ceagle\u201d won\u2019t tend to appear in otherwise similar sentences, so they\u2019ll be placed far apart in the embedding. But how does one actually implement something like this using neural nets? Let\u2019s start by talking about embeddings not for words, but for images. We want to find some way to characterize images by lists of numbers in such a way that \u201cimages we consider similar\u201d are assigned similar lists of numbers.   How do we tell if we should \u201cconsider images similar\u201d? Well, if our images are, say, of handwritten digits we might \u201cconsider two images similar\u201d if they are of the same digit. Earlier we discussed a neural net that was trained to recognize handwritten digits. And we can think of this neural net as being set up so that in its final output it puts images into 10 different bins, one for each digit. But what if we \u201cintercept\u201d what\u2019s going on inside the neural net before the final \u201cit\u2019s a \u20184\u2019\u201d decision is made? We might expect that inside the neural net there are numbers that characterize images as being \u201cmostly 4-like but a bit 2-like\u201d or some such. And the idea is to pick up such numbers to use as elements in an embedding. So here\u2019s the concept. Rather than directly trying to characterize \u201cwhat image is near what other image\u201d, we instead consider a well-defined task (in this case digit recognition) for which we can get explicit training data\u2014then use the fact that in doing this task the neural net implicitly has to make what amount to \u201cnearness decisions\u201d. So instead of us ever explicitly having to talk about \u201cnearness of images\u201d we\u2019re just talking about the concrete question of what digit an image represents, and then we\u2019re \u201cleaving it to the neural net\u201d to implicitly determine what that implies about \u201cnearness of images\u201d. So how in more detail does this work for the digit recognition network? We can think of the network as consisting of 11 successive layers, that we might summarize iconically like this (with activation functions shown as separate layers):", "At the beginning we\u2019re feeding into the first layer actual images, represented by 2D arrays of pixel values. And at the end\u2014from the last layer\u2014we\u2019re getting out an array of 10 values, which we can think of saying \u201chow certain\u201d the network is that the image corresponds to each of the digits 0 through 9.  Feed in the image and the values of the neurons in that last layer are: In other words, the neural net is by this point \u201cincredibly certain\u201d that this image is a 4\u2014and to actually get the output \u201c4\u201d we just have to pick out the position of the neuron with the largest value. But what if we look one step earlier? The very last operation in the network is a so-called softmax which tries to \u201cforce certainty\u201d. But before that\u2019s been applied the values of the neurons are: The neuron representing \u201c4\u201d still has the highest numerical value. But there\u2019s also information in the values of the other neurons. And we can expect that this list of numbers can in a sense be used to characterize the \u201cessence\u201d of the image\u2014and thus to provide something we can use as an embedding. And so, for example, each of the 4\u2019s here has a slightly different \u201csignature\u201d (or \u201cfeature embedding\u201d)\u2014all very different from the 8\u2019s: Here we\u2019re essentially using 10 numbers to characterize our images. But it\u2019s often better to use much more than that. And for example in our digit recognition network we can get an array of 500 numbers by tapping into the preceding layer. And this is probably a reasonable array to use as an \u201cimage embedding\u201d.   If we want to make an explicit visualization of \u201cimage space\u201d for handwritten digits we need to \u201creduce the dimension\u201d, effectively by projecting the 500-dimensional vector we\u2019ve got into, say, 3D space: We\u2019ve just talked about creating a characterization (and thus embedding) for images based effectively on identifying the similarity of images by determining whether (according to our training set) they correspond to the same handwritten digit.", "And we can do the same thing much more generally for images if we have a training set that identifies, say, which of 5000 common types of object (cat, dog, chair, \u2026) each image is of. And in this way we can make an image embedding that\u2019s \u201canchored\u201d by our identification of common objects, but then \u201cgeneralizes around that\u201d according to the behavior of the neural net. And the point is that insofar as that behavior aligns with how we humans perceive and interpret images, this will end up being an embedding that \u201cseems right to us\u201d, and is useful in practice in doing \u201chuman-judgement-like\u201d tasks. OK, so how do we follow the same kind of approach to find embeddings for words? The key is to start from a task about words for which we can readily do training. And the standard such task is \u201cword prediction\u201d. Imagine we\u2019re given \u201cthe ___ cat\u201d. Based on a large corpus of text (say, the text content of the web), what are the probabilities for different words that might \u201cfill in the blank\u201d? Or, alternatively, given \u201c___ black ___\u201d what are the probabilities for different \u201cflanking words\u201d? How do we set this problem up for a neural net? Ultimately we have to formulate everything in terms of numbers. And one way to do this is just to assign a unique number to each of the 50,000 or so common words in English. So, for example, \u201cthe\u201d might be 914, and \u201c cat\u201d (with a space before it) might be 3542. (And these are the actual numbers used by GPT-2.) So for the \u201cthe ___ cat\u201d problem, our input might be {914, 3542}. What should the output be like? Well, it should be a list of 50,000 or so numbers that effectively give the probabilities for each of the possible \u201cfill-in\u201d words. And once again, to find an embedding, we want to \u201cintercept\u201d the \u201cinsides\u201d of the neural net just before it \u201creaches its conclusion\u201d\u2014and then pick up the list of numbers that occur there, and that we can think of as \u201ccharacterizing each word\u201d. OK, so what do those characterizations look like?", "Over the past 10 years there\u2019ve been a sequence of different systems developed (word2vec, GloVe, BERT, GPT, \u2026), each based on a different neural net approach. But ultimately all of them take words and characterize them by lists of hundreds to thousands of numbers.   In their raw form, these \u201cembedding vectors\u201d are quite uninformative. For example, here\u2019s what GPT-2 produces as the raw embedding vectors for three specific words:  If we do things like measure distances between these vectors, then we can find things like \u201cnearnesses\u201d of words. Later we\u2019ll discuss in more detail what we might consider the \u201ccognitive\u201d significance of such embeddings. But for now the main point is that we have a way to usefully turn words into \u201cneural-net-friendly\u201d collections of numbers. But actually we can go further than just characterizing words by collections of numbers; we can also do this for sequences of words, or indeed whole blocks of text. And inside ChatGPT that\u2019s how it\u2019s dealing with things. It takes the text it\u2019s got so far, and generates an embedding vector to represent it. Then its goal is to find the probabilities for different words that might occur next. And it represents its answer for this as a list of numbers that essentially give the probabilities for each of the 50,000 or so possible words. (Strictly, ChatGPT does not deal with words, but rather with \u201ctokens\u201d\u2014convenient linguistic units that might be whole words, or might just be pieces like \u201cpre\u201d or \u201cing\u201d or \u201cized\u201d. Working with tokens makes it easier for ChatGPT to handle rare, compound and non-English words, and, sometimes, for better or worse, to invent new words.) OK, so we\u2019re finally ready to discuss what\u2019s inside ChatGPT. And, yes, ultimately, it\u2019s a giant neural net\u2014currently a version of the so-called GPT-3 network with 175 billion weights. In many ways this is a neural net very much like the other ones we\u2019ve discussed. But it\u2019s a neural net that\u2019s particularly set up for dealing with language.", "And its most notable feature is a piece of neural net architecture called a \u201ctransformer\u201d. In the first neural nets we discussed above, every neuron at any given layer was basically connected (at least with some weight) to every neuron on the layer before. But this kind of fully connected network is (presumably) overkill if one\u2019s working with data that has particular, known structure. And thus, for example, in the early stages of dealing with images, it\u2019s typical to use so-called convolutional neural nets (\u201cconvnets\u201d) in which neurons are effectively laid out on a grid analogous to the pixels in the image\u2014and connected only to neurons nearby on the grid.   The idea of transformers is to do something at least somewhat similar for sequences of tokens that make up a piece of text. But instead of just defining a fixed region in the sequence over which there can be connections, transformers instead introduce the notion of \u201cattention\u201d\u2014and the idea of \u201cpaying attention\u201d more to some parts of the sequence than others. Maybe one day it\u2019ll make sense to just start a generic neural net and do all customization through training. But at least as of now it seems to be critical in practice to \u201cmodularize\u201d things\u2014as transformers do, and probably as our brains also do.   OK, so what does ChatGPT (or, rather, the GPT-3 network on which it\u2019s based) actually do? Recall that its overall goal is to continue text in a \u201creasonable\u201d way, based on what it\u2019s seen from the training it\u2019s had (which consists in looking at billions of pages of text from the web, etc.) So at any given point, it\u2019s got a certain amount of text\u2014and its goal is to come up with an appropriate choice for the next token to add. It operates in three basic stages. First, it takes the sequence of tokens that corresponds to the text so far, and finds an embedding (i.e. an array of numbers) that represents these.", "Then it operates on this embedding\u2014in a \u201cstandard neural net way\u201d, with values \u201crippling through\u201d successive layers in a network\u2014to produce a new embedding (i.e. a new array of numbers). It then takes the last part of this array and generates from it an array of about 50,000 values that turn into probabilities for different possible next tokens. (And, yes, it so happens that there are about the same number of tokens used as there are common words in English, though only about 3000 of the tokens are whole words, and the rest are fragments.) A critical point is that every part of this pipeline is implemented by a neural network, whose weights are determined by end-to-end training of the network. In other words, in effect nothing except the overall architecture is \u201cexplicitly engineered\u201d; everything is just \u201clearned\u201d from training data. There are, however, plenty of details in the way the architecture is set up\u2014reflecting all sorts of experience and neural net lore. And\u2014even though this is definitely going into the weeds\u2014I think it\u2019s useful to talk about some of those details, not least to get a sense of just what goes into building something like ChatGPT. First comes the embedding module. Here\u2019s a schematic Wolfram Language representation for it for GPT-2: The input is a vector of n tokens (represented as in the previous section by integers from 1 to about 50,000). Each of these tokens is converted (by a single-layer neural net) into an embedding vector (of length 768 for GPT-2 and 12,288 for ChatGPT\u2019s GPT-3). Meanwhile, there\u2019s a \u201csecondary pathway\u201d that takes the sequence of (integer) positions for the tokens, and from these integers creates another embedding vector. And finally the embedding vectors from the token value and the token position are added together\u2014to produce the final sequence of embedding vectors from the embedding module. Why does one just add the token-value and token-position embedding vectors together? I don\u2019t think there\u2019s any particular science to this.", "It\u2019s just that various different things have been tried, and this is one that seems to work. And it\u2019s part of the lore of neural nets that\u2014in some sense\u2014so long as the setup one has is \u201croughly right\u201d it\u2019s usually possible to home in on details just by doing sufficient training, without ever really needing to \u201cunderstand at an engineering level\u201d quite how the neural net has ended up configuring itself. Here\u2019s what the embedding module does, operating on the string hello hello hello hello hello hello hello hello hello hello bye bye bye bye bye bye bye bye bye bye: The elements of the embedding vector for each token are shown down the page, and across the page we see first a run of \u201chello\u201d embeddings, followed by a run of \u201cbye\u201d ones. The second array above is the positional embedding\u2014with its somewhat-random-looking structure being just what \u201chappened to be learned\u201d (in this case in GPT-2). OK, so after the embedding module comes the \u201cmain event\u201d of the transformer: a sequence of so-called \u201cattention blocks\u201d (12 for GPT-2, 96 for ChatGPT\u2019s GPT-3). It\u2019s all pretty complicated\u2014and reminiscent of typical large hard-to-understand engineering systems, or, for that matter, biological systems. But anyway, here\u2019s a schematic representation of a single \u201cattention block\u201d (for GPT-2): Within each such attention block there are a collection of \u201cattention heads\u201d (12 for GPT-2, 96 for ChatGPT\u2019s GPT-3)\u2014each of which operates independently on different chunks of values in the embedding vector. (And, yes, we don\u2019t know any particular reason why it\u2019s a good idea to split up the embedding vector, or what the different parts of it \u201cmean\u201d; this is just one of those things that\u2019s been \u201cfound to work\u201d.)   OK, so what do the attention heads do? Basically they\u2019re a way of \u201clooking back\u201d in the sequence of tokens (i.e. in the text produced so far), and \u201cpackaging up the past\u201d in a form that\u2019s useful for finding the next token.", "In the first section above we talked about using 2-gram probabilities to pick words based on their immediate predecessors. What the \u201cattention\u201d mechanism in transformers does is to allow \u201cattention to\u201d even much earlier words\u2014thus potentially capturing the way, say, verbs can refer to nouns that appear many words before them in a sentence. At a more detailed level, what an attention head does is to recombine chunks in the embedding vectors associated with different tokens, with certain weights. And so, for example, the 12 attention heads in the first attention block (in GPT-2) have the following (\u201clook-back-all-the-way-to-the-beginning-of-the-sequence-of-tokens\u201d) patterns of \u201crecombination weights\u201d for the \u201chello, bye\u201d string above: After being processed by the attention heads, the resulting \u201cre-weighted embedding vector\u201d (of length 768 for GPT-2 and length 12,288 for ChatGPT\u2019s GPT-3) is passed through a standard \u201cfully connected\u201d neural net layer. It\u2019s hard to get a handle on what this layer is doing. But here\u2019s a plot of the 768\u00d7768 matrix of weights it\u2019s using (here for GPT-2): Taking 64\u00d764 moving averages, some (random-walk-ish) structure begins to emerge: What determines this structure? Ultimately it\u2019s presumably some \u201cneural net encoding\u201d of features of human language. But as of now, what those features might be is quite unknown. In effect, we\u2019re \u201copening up the brain of ChatGPT\u201d (or at least GPT-2) and discovering, yes, it\u2019s complicated in there, and we don\u2019t understand it\u2014even though in the end it\u2019s producing recognizable human language. OK, so after going through one attention block, we\u2019ve got a new embedding vector\u2014which is then successively passed through additional attention blocks (a total of 12 for GPT-2; 96 for GPT-3). Each attention block has its own particular pattern of \u201cattention\u201d and \u201cfully connected\u201d weights.", "Here for GPT-2 are the sequence of attention weights for the \u201chello, bye\u201d input, for the first attention head: And here are the (moving-averaged) \u201cmatrices\u201d for the fully connected layers: Curiously, even though these \u201cmatrices of weights\u201d in different attention blocks look quite similar, the distributions of the sizes of weights can be somewhat different (and are not always Gaussian): So after going through all these attention blocks what is the net effect of the transformer? Essentially it\u2019s to transform the original collection of embeddings for the sequence of tokens to a final collection. And the particular way ChatGPT works is then to pick up the last embedding in this collection, and \u201cdecode\u201d it to produce a list of probabilities for what token should come next. So that\u2019s in outline what\u2019s inside ChatGPT. It may seem complicated (not least because of its many inevitably somewhat arbitrary \u201cengineering choices\u201d), but actually the ultimate elements involved are remarkably simple. Because in the end what we\u2019re dealing with is just a neural net made of \u201cartificial neurons\u201d, each doing the simple operation of taking a collection of numerical inputs, and then combining them with certain weights.   The original input to ChatGPT is an array of numbers (the embedding vectors for the tokens so far), and what happens when ChatGPT \u201cruns\u201d to produce a new token is just that these numbers \u201cripple through\u201d the layers of the neural net, with each neuron \u201cdoing its thing\u201d and passing the result to neurons on the next layer. There\u2019s no looping or \u201cgoing back\u201d. Everything just \u201cfeeds forward\u201d through the network.   It\u2019s a very different setup from a typical computational system\u2014like a Turing machine\u2014in which results are repeatedly \u201creprocessed\u201d by the same computational elements. Here\u2014at least in generating a given token of output\u2014each computational element (i.e. neuron) is used only once.   But there is in a sense still an \u201couter loop\u201d that reuses computational elements even in ChatGPT.", "Because when ChatGPT is going to generate a new token, it always \u201creads\u201d (i.e. takes as input) the whole sequence of tokens that come before it, including tokens that ChatGPT itself has \u201cwritten\u201d previously. And we can think of this setup as meaning that ChatGPT does\u2014at least at its outermost level\u2014involve a \u201cfeedback loop\u201d, albeit one in which every iteration is explicitly visible as a token that appears in the text that it generates. But let\u2019s come back to the core of ChatGPT: the neural net that\u2019s being repeatedly used to generate each token. At some level it\u2019s very simple: a whole collection of identical artificial neurons. And some parts of the network just consist of (\u201cfully connected\u201d) layers of neurons in which every neuron on a given layer is connected (with some weight) to every neuron on the layer before. But particularly with its transformer architecture, ChatGPT has parts with more structure, in which only specific neurons on different layers are connected. (Of course, one could still say that \u201call neurons are connected\u201d\u2014but some just have zero weight.) In addition, there are aspects of the neural net in ChatGPT that aren\u2019t most naturally thought of as just consisting of \u201chomogeneous\u201d layers. And for example\u2014as the iconic summary above indicates\u2014inside an attention block there are places where \u201cmultiple copies are made\u201d of incoming data, each then going through a different \u201cprocessing path\u201d, potentially involving a different number of layers, and only later recombining. But while this may be a convenient representation of what\u2019s going on, it\u2019s always at least in principle possible to think of \u201cdensely filling in\u201d layers, but just having some weights be zero. If one looks at the longest path through ChatGPT, there are about 400 (core) layers involved\u2014in some ways not a huge number. But there are millions of neurons\u2014with a total of 175 billion connections and therefore 175 billion weights.", "And one thing to realize is that every time ChatGPT generates a new token, it has to do a calculation involving every single one of these weights. Implementationally these calculations can be somewhat organized \u201cby layer\u201d into highly parallel array operations that can conveniently be done on GPUs. But for each token that\u2019s produced, there still have to be 175 billion calculations done (and in the end a bit more)\u2014so that, yes, it\u2019s not surprising that it can take a while to generate a long piece of text with ChatGPT.   But in the end, the remarkable thing is that all these operations\u2014individually as simple as they are\u2014can somehow together manage to do such a good \u201chuman-like\u201d job of generating text. It has to be emphasized again that (at least so far as we know) there\u2019s no \u201cultimate theoretical reason\u201d why anything like this should work. And in fact, as we\u2019ll discuss, I think we have to view this as a\u2014potentially surprising\u2014scientific discovery: that somehow in a neural net like ChatGPT\u2019s it\u2019s possible to capture the essence of what human brains manage to do in generating language.   OK, so we\u2019ve now given an outline of how ChatGPT works once it\u2019s set up. But how did it get set up? How were all those 175 billion weights in its neural net determined? Basically they\u2019re the result of very large-scale training, based on a huge corpus of text\u2014on the web, in books, etc.\u2014written by humans. As we\u2019ve said, even given all that training data, it\u2019s certainly not obvious that a neural net would be able to successfully produce \u201chuman-like\u201d text. And, once again, there seem to be detailed pieces of engineering needed to make that happen. But the big surprise\u2014and discovery\u2014of ChatGPT is that it\u2019s possible at all. And that\u2014in effect\u2014a neural net with \u201cjust\u201d 175 billion weights can make a \u201creasonable model\u201d of text humans write. In modern times, there\u2019s lots of text written by humans that\u2019s out there in digital form. The public web has at least several billion human-written pages, with altogether perhaps a trillion words of text.", "And if one includes non-public webpages, the numbers might be at least 100 times larger. So far, more than 5 million digitized books have been made available (out of 100 million or so that have ever been published), giving another 100 billion or so words of text. And that\u2019s not even mentioning text derived from speech in videos, etc. (As a personal comparison, my total lifetime output of published material has been a bit under 3 million words, and over the past 30 years I\u2019ve written about 15 million words of email, and altogether typed perhaps 50 million words\u2014and in just the past couple of years I\u2019ve spoken more than 10 million words on livestreams. And, yes, I\u2019ll train a bot from all of that.) But, OK, given all this data, how does one train a neural net from it? The basic process is very much as we discussed it in the simple examples above. You present a batch of examples, and then you adjust the weights in the network to minimize the error (\u201closs\u201d) that the network makes on those examples. The main thing that\u2019s expensive about \u201cback propagating\u201d from the error is that each time you do this, every weight in the network will typically change at least a tiny bit, and there are just a lot of weights to deal with. (The actual \u201cback computation\u201d is typically only a small constant factor harder than the forward one.) With modern GPU hardware, it\u2019s straightforward to compute the results from batches of thousands of examples in parallel. But when it comes to actually updating the weights in the neural net, current methods require one to do this basically batch by batch. (And, yes, this is probably where actual brains\u2014with their combined computation and memory elements\u2014have, for now, at least an architectural advantage.) Even in the seemingly simple cases of learning numerical functions that we discussed earlier, we found we often had to use millions of examples to successfully train a network, at least from scratch. So how many examples does this mean we\u2019ll need in order to train a \u201chuman-like language\u201d model?", "There doesn\u2019t seem to be any fundamental \u201ctheoretical\u201d way to know. But in practice ChatGPT was successfully trained on a few hundred billion words of text. Some of the text it was fed several times, some of it only once. But somehow it \u201cgot what it needed\u201d from the text it saw. But given this volume of text to learn from, how large a network should it require to \u201clearn it well\u201d? Again, we don\u2019t yet have a fundamental theoretical way to say. Ultimately\u2014as we\u2019ll discuss further below\u2014there\u2019s presumably a certain \u201ctotal algorithmic content\u201d to human language and what humans typically say with it. But the next question is how efficient a neural net will be at implementing a model based on that algorithmic content. And again we don\u2019t know\u2014although the success of ChatGPT suggests it\u2019s reasonably efficient. And in the end we can just note that ChatGPT does what it does using a couple hundred billion weights\u2014comparable in number to the total number of words (or tokens) of training data it\u2019s been given. In some ways it\u2019s perhaps surprising (though empirically observed also in smaller analogs of ChatGPT) that the \u201csize of the network\u201d that seems to work well is so comparable to the \u201csize of the training data\u201d. After all, it\u2019s certainly not that somehow \u201cinside ChatGPT\u201d all that text from the web and books and so on is \u201cdirectly stored\u201d. Because what\u2019s actually inside ChatGPT are a bunch of numbers\u2014with a bit less than 10 digits of precision\u2014that are some kind of distributed encoding of the aggregate structure of all that text.   Put another way, we might ask what the \u201ceffective information content\u201d is of human language and what\u2019s typically said with it. There\u2019s the raw corpus of examples of language. And then there\u2019s the representation in the neural net of ChatGPT. That representation is very likely far from the \u201calgorithmically minimal\u201d representation (as we\u2019ll discuss below). But it\u2019s a representation that\u2019s readily usable by the neural net.", "And in this representation it seems there\u2019s in the end rather little \u201ccompression\u201d of the training data; it seems on average to basically take only a bit less than one neural net weight to carry the \u201cinformation content\u201d of a word of training data. When we run ChatGPT to generate text, we\u2019re basically having to use each weight once. So if there are n weights, we\u2019ve got of order n computational steps to do\u2014though in practice many of them can typically be done in parallel in GPUs. But if we need about n words of training data to set up those weights, then from what we\u2019ve said above we can conclude that we\u2019ll need about n2 computational steps to do the training of the network\u2014which is why, with current methods, one ends up needing to talk about billion-dollar training efforts. The majority of the effort in training ChatGPT is spent \u201cshowing it\u201d large amounts of existing text from the web, books, etc. But it turns out there\u2019s another\u2014apparently rather important\u2014part too.   As soon as it\u2019s finished its \u201craw training\u201d from the original corpus of text it\u2019s been shown, the neural net inside ChatGPT is ready to start generating its own text, continuing from prompts, etc. But while the results from this may often seem reasonable, they tend\u2014particularly for longer pieces of text\u2014to \u201cwander off\u201d in often rather non-human-like ways. It\u2019s not something one can readily detect, say, by doing traditional statistics on the text. But it\u2019s something that actual humans reading the text easily notice. And a key idea in the construction of ChatGPT was to have another step after \u201cpassively reading\u201d things like the web: to have actual humans actively interact with ChatGPT, see what it produces, and in effect give it feedback on \u201chow to be a good chatbot\u201d. But how can the neural net use that feedback? The first step is just to have humans rate results from the neural net. But then another neural net model is built that attempts to predict those ratings.", "But now this prediction model can be run\u2014essentially like a loss function\u2014on the original network, in effect allowing that network to be \u201ctuned up\u201d by the human feedback that\u2019s been given. And the results in practice seem to have a big effect on the success of the system in producing \u201chuman-like\u201d output.   In general, it\u2019s interesting how little \u201cpoking\u201d the \u201coriginally trained\u201d network seems to need to get it to usefully go in particular directions. One might have thought that to have the network behave as if it\u2019s \u201clearned something new\u201d one would have to go in and run a training algorithm, adjusting weights, and so on. But that\u2019s not the case. Instead, it seems to be sufficient to basically tell ChatGPT something one time\u2014as part of the prompt you give\u2014and then it can successfully make use of what you told it when it generates text. And once again, the fact that this works is, I think, an important clue in understanding what ChatGPT is \u201creally doing\u201d and how it relates to the structure of human language and thinking.   There\u2019s certainly something rather human-like about it: that at least once it\u2019s had all that pre-training you can tell it something just once and it can \u201cremember it\u201d\u2014at least \u201clong enough\u201d to generate a piece of text using it. So what\u2019s going on in a case like this? It could be that \u201ceverything you might tell it is already in there somewhere\u201d\u2014and you\u2019re just leading it to the right spot. But that doesn\u2019t seem plausible. Instead, what seems more likely is that, yes, the elements are already in there, but the specifics are defined by something like a \u201ctrajectory between those elements\u201d and that\u2019s what you\u2019re introducing when you tell it something. And indeed, much like for humans, if you tell it something bizarre and unexpected that completely doesn\u2019t fit into the framework it knows, it doesn\u2019t seem like it\u2019ll successfully be able to \u201cintegrate\u201d this. It can \u201cintegrate\u201d it only if it\u2019s basically riding in a fairly simple way on top of the framework it already has.", "It\u2019s also worth pointing out again that there are inevitably \u201calgorithmic limits\u201d to what the neural net can \u201cpick up\u201d. Tell it \u201cshallow\u201d rules of the form \u201cthis goes to that\u201d, etc., and the neural net will most likely be able to represent and reproduce these just fine\u2014and indeed what it \u201calready knows\u201d from language will give it an immediate pattern to follow. But try to give it rules for an actual \u201cdeep\u201d computation that involves many potentially computationally irreducible steps and it just won\u2019t work. (Remember that at each step it\u2019s always just \u201cfeeding data forward\u201d in its network, never looping except by virtue of generating new tokens.) Of course, the network can learn the answer to specific \u201cirreducible\u201d computations. But as soon as there are combinatorial numbers of possibilities, no such \u201ctable-lookup-style\u201d approach will work. And so, yes, just like humans, it\u2019s time then for neural nets to \u201creach out\u201d and use actual computational tools. (And, yes, Wolfram|Alpha and Wolfram Language are uniquely suitable, because they\u2019ve been built to \u201ctalk about things in the world\u201d, just like the language-model neural nets.) Human language\u2014and the processes of thinking involved in generating it\u2014have always seemed to represent a kind of pinnacle of complexity. And indeed it\u2019s seemed somewhat remarkable that human brains\u2014with their network of a \u201cmere\u201d 100 billion or so neurons (and maybe 100 trillion connections) could be responsible for it. Perhaps, one might have imagined, there\u2019s something more to brains than their networks of neurons\u2014like some new layer of undiscovered physics. But now with ChatGPT we\u2019ve got an important new piece of information: we know that a pure, artificial neural network with about as many connections as brains have neurons is capable of doing a surprisingly good job of generating human language. And, yes, that\u2019s still a big and complicated system\u2014with about as many neural net weights as there are words of text currently available out there in the world.", "But at some level it still seems difficult to believe that all the richness of language and the things it can talk about can be encapsulated in such a finite system. Part of what\u2019s going on is no doubt a reflection of the ubiquitous phenomenon (that first became evident in the example of rule 30) that computational processes can in effect greatly amplify the apparent complexity of systems even when their underlying rules are simple. But, actually, as we discussed above, neural nets of the kind used in ChatGPT tend to be specifically constructed to restrict the effect of this phenomenon\u2014and the computational irreducibility associated with it\u2014in the interest of making their training more accessible. So how is it, then, that something like ChatGPT can get as far as it does with language? The basic answer, I think, is that language is at a fundamental level somehow simpler than it seems. And this means that ChatGPT\u2014even with its ultimately straightforward neural net structure\u2014is successfully able to \u201ccapture the essence\u201d of human language and the thinking behind it. And moreover, in its training, ChatGPT has somehow \u201cimplicitly discovered\u201d whatever regularities in language (and thinking) make this possible. The success of ChatGPT is, I think, giving us evidence of a fundamental and important piece of science: it\u2019s suggesting that we can expect there to be major new \u201claws of language\u201d\u2014and effectively \u201claws of thought\u201d\u2014out there to discover. In ChatGPT\u2014built as it is as a neural net\u2014those laws are at best implicit. But if we could somehow make the laws explicit, there\u2019s the potential to do the kinds of things ChatGPT does in vastly more direct, efficient\u2014and transparent\u2014ways. But, OK, so what might these laws be like? Ultimately they must give us some kind of prescription for how language\u2014and the things we say with it\u2014are put together. Later we\u2019ll discuss how \u201clooking inside ChatGPT\u201d may be able to give us some hints about this, and how what we know from building computational language suggests a path forward.", "But first let\u2019s discuss two long-known examples of what amount to \u201claws of language\u201d\u2014and how they relate to the operation of ChatGPT. The first is the syntax of language. Language is not just a random jumble of words. Instead, there are (fairly) definite grammatical rules for how words of different kinds can be put together: in English, for example, nouns can be preceded by adjectives and followed by verbs, but typically two nouns can\u2019t be right next to each other. Such grammatical structure can (at least approximately) be captured by a set of rules that define how what amount to \u201cparse trees\u201d can be put together: ChatGPT doesn\u2019t have any explicit \u201cknowledge\u201d of such rules. But somehow in its training it implicitly \u201cdiscovers\u201d them\u2014and then seems to be good at following them. So how does this work? At a \u201cbig picture\u201d level it\u2019s not clear. But to get some insight it\u2019s perhaps instructive to look at a much simpler example. Consider a \u201clanguage\u201d formed from sequences of (\u2019s and )\u2019s, with a grammar that specifies that parentheses should always be balanced, as represented by a parse tree like: Can we train a neural net to produce \u201cgrammatically correct\u201d parenthesis sequences? There are various ways to handle sequences in neural nets, but let\u2019s use transformer nets, as ChatGPT does. And given a simple transformer net, we can start feeding it grammatically correct parenthesis sequences as training examples. A subtlety (which actually also appears in ChatGPT\u2019s generation of human language) is that in addition to our \u201ccontent tokens\u201d (here \u201c(\u201d and \u201c)\u201d) we have to include an \u201cEnd\u201d token, that\u2019s generated to indicate that the output shouldn\u2019t continue any further (i.e. for ChatGPT, that one\u2019s reached the \u201cend of the story\u201d). If we set up a transformer net with just one attention block with 8 heads and feature vectors of length 128 (ChatGPT also uses feature vectors of length 128, but has 96 attention blocks, each with 96 heads) then it doesn\u2019t seem possible to get it to learn much about parenthesis language.", "But with 2 attention blocks, the learning process seems to converge\u2014at least after 10 million or so examples have been given (and, as is common with transformer nets, showing yet more examples just seems to degrade its performance). So with this network, we can do the analog of what ChatGPT does, and ask for probabilities for what the next token should be\u2014in a parenthesis sequence: And in the first case, the network is \u201cpretty sure\u201d that the sequence can\u2019t end here\u2014which is good, because if it did, the parentheses would be left unbalanced. In the second case, however, it \u201ccorrectly recognizes\u201d that the sequence can end here, though it also \u201cpoints out\u201d that it\u2019s possible to \u201cstart again\u201d, putting down a \u201c(\u201d, presumably with a \u201c)\u201d to follow. But, oops, even with its 400,000 or so laboriously trained weights, it says there\u2019s a 15% probability to have \u201c)\u201d as the next token\u2014which isn\u2019t right, because that would necessarily lead to an unbalanced parenthesis. Here\u2019s what we get if we ask the network for the highest-probability completions for progressively longer sequences of (\u2019s: And, yes, up to a certain length the network does just fine. But then it starts failing. It\u2019s a pretty typical kind of thing to see in a \u201cprecise\u201d situation like this with a neural net (or with machine learning in general). Cases that a human \u201ccan solve in a glance\u201d the neural net can solve too. But cases that require doing something \u201cmore algorithmic\u201d (e.g. explicitly counting parentheses to see if they\u2019re closed) the neural net tends to somehow be \u201ctoo computationally shallow\u201d to reliably do. (By the way, even the full current ChatGPT has a hard time correctly matching parentheses in long sequences.) So what does this mean for things like ChatGPT and the syntax of a language like English? The parenthesis language is \u201caustere\u201d\u2014and much more of an \u201calgorithmic story\u201d. But in English it\u2019s much more realistic to be able to \u201cguess\u201d what\u2019s grammatically going to fit on the basis of local choices of words and other hints.", "And, yes, the neural net is much better at this\u2014even though perhaps it might miss some \u201cformally correct\u201d case that, well, humans might miss as well. But the main point is that the fact that there\u2019s an overall syntactic structure to the language\u2014with all the regularity that implies\u2014in a sense limits \u201chow much\u201d the neural net has to learn. And a key \u201cnatural-science-like\u201d observation is that the transformer architecture of neural nets like the one in ChatGPT seems to successfully be able to learn the kind of nested-tree-like syntactic structure that seems to exist (at least in some approximation) in all human languages. Syntax provides one kind of constraint on language. But there are clearly more. A sentence like \u201cInquisitive electrons eat blue theories for fish\u201d is grammatically correct but isn\u2019t something one would normally expect to say, and wouldn\u2019t be considered a success if ChatGPT generated it\u2014because, well, with the normal meanings for the words in it, it\u2019s basically meaningless.   But is there a general way to tell if a sentence is meaningful? There\u2019s no traditional overall theory for that. But it\u2019s something that one can think of ChatGPT as having implicitly \u201cdeveloped a theory for\u201d after being trained with billions of (presumably meaningful) sentences from the web, etc.   What might this theory be like? Well, there\u2019s one tiny corner that\u2019s basically been known for two millennia, and that\u2019s logic. And certainly in the syllogistic form in which Aristotle discovered it, logic is basically a way of saying that sentences that follow certain patterns are reasonable, while others are not. Thus, for example, it\u2019s reasonable to say \u201cAll X are Y. This is not Y, so it\u2019s not an X\u201d (as in \u201cAll fishes are blue. This is not blue, so it\u2019s not a fish.\u201d).", "And just as one can somewhat whimsically imagine that Aristotle discovered syllogistic logic by going (\u201cmachine-learning-style\u201d) through lots of examples of rhetoric, so too one can imagine that in the training of ChatGPT it will have been able to \u201cdiscover syllogistic logic\u201d by looking at lots of text on the web, etc. (And, yes, while one can therefore expect ChatGPT to produce text that contains \u201ccorrect inferences\u201d based on things like syllogistic logic, it\u2019s a quite different story when it comes to more sophisticated formal logic\u2014and I think one can expect it to fail here for the same kind of reasons it fails in parenthesis matching.) But beyond the narrow example of logic, what can be said about how to systematically construct (or recognize) even plausibly meaningful text? Yes, there are things like Mad Libs that use very specific \u201cphrasal templates\u201d. But somehow ChatGPT implicitly has a much more general way to do it. And perhaps there\u2019s nothing to be said about how it can be done beyond \u201csomehow it happens when you have 175 billion neural net weights\u201d. But I strongly suspect that there\u2019s a much simpler and stronger story. We discussed above that inside ChatGPT any piece of text is effectively represented by an array of numbers that we can think of as coordinates of a point in some kind of \u201clinguistic feature space\u201d. So when ChatGPT continues a piece of text this corresponds to tracing out a trajectory in linguistic feature space. But now we can ask what makes this trajectory correspond to text we consider meaningful. And might there perhaps be some kind of \u201csemantic laws of motion\u201d that define\u2014or at least constrain\u2014how points in linguistic feature space can move around while preserving \u201cmeaningfulness\u201d? So what is this linguistic feature space like? Here\u2019s an example of how single words (here, common nouns) might get laid out if we project such a feature space down to 2D: We saw another example above based on words representing plants and animals.", "But the point in both cases is that \u201csemantically similar words\u201d are placed nearby. As another example, here\u2019s how words corresponding to different parts of speech get laid out: Of course, a given word doesn\u2019t in general just have \u201cone meaning\u201d (or necessarily correspond to just one part of speech). And by looking at how sentences containing a word lay out in feature space, one can often \u201ctease apart\u201d different meanings\u2014as in the example here for the word \u201ccrane\u201d (bird or machine?): OK, so it\u2019s at least plausible that we can think of this feature space as placing \u201cwords nearby in meaning\u201d close in this space. But what kind of additional structure can we identify in this space? Is there for example some kind of notion of \u201cparallel transport\u201d that would reflect \u201cflatness\u201d in the space? One way to get a handle on that is to look at analogies: And, yes, even when we project down to 2D, there\u2019s often at least a \u201chint of flatness\u201d, though it\u2019s certainly not universally seen. So what about trajectories? We can look at the trajectory that a prompt for ChatGPT follows in feature space\u2014and then we can see how ChatGPT continues that: There\u2019s certainly no \u201cgeometrically obvious\u201d law of motion here. And that\u2019s not at all surprising; we fully expect this to be a considerably more complicated story. And, for example, it\u2019s far from obvious that even if there is a \u201csemantic law of motion\u201d to be found, what kind of embedding (or, in effect, what \u201cvariables\u201d) it\u2019ll most naturally be stated in.   In the picture above, we\u2019re showing several steps in the \u201ctrajectory\u201d\u2014where at each step we\u2019re picking the word that ChatGPT considers the most probable (the \u201czero temperature\u201d case). But we can also ask what words can \u201ccome next\u201d with what probabilities at a given point: And what we see in this case is that there\u2019s a \u201cfan\u201d of high-probability words that seems to go in a more or less definite direction in feature space. What happens if we go further?", "Here are the successive \u201cfans\u201d that appear as we \u201cmove along\u201d the trajectory: Here\u2019s a 3D representation, going for a total of 40 steps: And, yes, this seems like a mess\u2014and doesn\u2019t do anything to particularly encourage the idea that one can expect to identify \u201cmathematical-physics-like\u201d \u201csemantic laws of motion\u201d by empirically studying \u201cwhat ChatGPT is doing inside\u201d. But perhaps we\u2019re just looking at the \u201cwrong variables\u201d (or wrong coordinate system) and if only we looked at the right one, we\u2019d immediately see that ChatGPT is doing something \u201cmathematical-physics-simple\u201d like following geodesics. But as of now, we\u2019re not ready to \u201cempirically decode\u201d from its \u201cinternal behavior\u201d what ChatGPT has \u201cdiscovered\u201d about how human language is \u201cput together\u201d.   What does it take to produce \u201cmeaningful human language\u201d? In the past, we might have assumed it could be nothing short of a human brain. But now we know it can be done quite respectably by the neural net of ChatGPT. Still, maybe that\u2019s as far as we can go, and there\u2019ll be nothing simpler\u2014or more human understandable\u2014that will work. But my strong suspicion is that the success of ChatGPT implicitly reveals an important \u201cscientific\u201d fact: that there\u2019s actually a lot more structure and simplicity to meaningful human language than we ever knew\u2014and that in the end there may be even fairly simple rules that describe how such language can be put together. As we mentioned above, syntactic grammar gives rules for how words corresponding to things like different parts of speech can be put together in human language. But to deal with meaning, we need to go further. And one version of how to do this is to think about not just a syntactic grammar for language, but also a semantic one.   For purposes of syntax, we identify things like nouns and verbs. But for purposes of semantics, we need \u201cfiner gradations\u201d. So, for example, we might identify the concept of \u201cmoving\u201d, and the concept of an \u201cobject\u201d that \u201cmaintains its identity independent of location\u201d.", "There are endless specific examples of each of these \u201csemantic concepts\u201d. But for the purposes of our semantic grammar, we\u2019ll just have some general kind of rule that basically says that \u201cobjects\u201d can \u201cmove\u201d. There\u2019s a lot to say about how all this might work (some of which I\u2019ve said before). But I\u2019ll content myself here with just a few remarks that indicate some of the potential path forward. It\u2019s worth mentioning that even if a sentence is perfectly OK according to the semantic grammar, that doesn\u2019t mean it\u2019s been realized (or even could be realized) in practice. \u201cThe elephant traveled to the Moon\u201d would doubtless \u201cpass\u201d our semantic grammar, but it certainly hasn\u2019t been realized (at least yet) in our actual world\u2014though it\u2019s absolutely fair game for a fictional world. When we start talking about \u201csemantic grammar\u201d we\u2019re soon led to ask \u201cWhat\u2019s underneath it?\u201d What \u201cmodel of the world\u201d is it assuming? A syntactic grammar is really just about the construction of language from words. But a semantic grammar necessarily engages with some kind of \u201cmodel of the world\u201d\u2014something that serves as a \u201cskeleton\u201d on top of which language made from actual words can be layered. Until recent times, we might have imagined that (human) language would be the only general way to describe our \u201cmodel of the world\u201d. Already a few centuries ago there started to be formalizations of specific kinds of things, based particularly on mathematics. But now there\u2019s a much more general approach to formalization: computational language. And, yes, that\u2019s been my big project over the course of more than four decades (as now embodied in the Wolfram Language): to develop a precise symbolic representation that can talk as broadly as possible about things in the world, as well as abstract things that we care about. And so, for example, we have symbolic representations for cities and molecules and images and neural networks, and we have built-in knowledge about how to compute about those things.", "And, after decades of work, we\u2019ve covered a lot of areas in this way. But in the past, we haven\u2019t particularly dealt with \u201ceveryday discourse\u201d. In \u201cI bought two pounds of apples\u201d we can readily represent (and do nutrition and other computations on) the \u201ctwo pounds of apples\u201d. But we don\u2019t (quite yet) have a symbolic representation for \u201cI bought\u201d. It\u2019s all connected to the idea of semantic grammar\u2014and the goal of having a generic symbolic \u201cconstruction kit\u201d for concepts, that would give us rules for what could fit together with what, and thus for the \u201cflow\u201d of what we might turn into human language.   But let\u2019s say we had this \u201csymbolic discourse language\u201d. What would we do with it? We could start off doing things like generating \u201clocally meaningful text\u201d. But ultimately we\u2019re likely to want more \u201cglobally meaningful\u201d results\u2014which means \u201ccomputing\u201d more about what can actually exist or happen in the world (or perhaps in some consistent fictional world).   Right now in Wolfram Language we have a huge amount of built-in computational knowledge about lots of kinds of things. But for a complete symbolic discourse language we\u2019d have to build in additional \u201ccalculi\u201d about general things in the world: if an object moves from A to B and from B to C, then it\u2019s moved from A to C, etc.   Given a symbolic discourse language we might use it to make \u201cstandalone statements\u201d. But we can also use it to ask questions about the world, \u201cWolfram|Alpha style\u201d. Or we can use it to state things that we \u201cwant to make so\u201d, presumably with some external actuation mechanism. Or we can use it to make assertions\u2014perhaps about the actual world, or perhaps about some specific world we\u2019re considering, fictional or otherwise.   Human language is fundamentally imprecise, not least because it isn\u2019t \u201ctethered\u201d to a specific computational implementation, and its meaning is basically defined just by a \u201csocial contract\u201d between its users.", "But computational language, by its nature, has a certain fundamental precision\u2014because in the end what it specifies can always be \u201cunambiguously executed on a computer\u201d. Human language can usually get away with a certain vagueness. (When we say \u201cplanet\u201d does it include exoplanets or not, etc.?) But in computational language we have to be precise and clear about all the distinctions we\u2019re making. It\u2019s often convenient to leverage ordinary human language in making up names in computational language. But the meanings they have in computational language are necessarily precise\u2014and might or might not cover some particular connotation in typical human language usage. How should one figure out the fundamental \u201contology\u201d suitable for a general symbolic discourse language? Well, it\u2019s not easy. Which is perhaps why little has been done in these since the primitive beginnings Aristotle made more than two millennia ago. But it really helps that today we now know so much about how to think about the world computationally (and it doesn\u2019t hurt to have a \u201cfundamental metaphysics\u201d from our Physics Project and the idea of the ruliad). But what does all this mean in the context of ChatGPT? From its training ChatGPT has effectively \u201cpieced together\u201d a certain (rather impressive) quantity of what amounts to semantic grammar. But its very success gives us a reason to think that it\u2019s going to be feasible to construct something more complete in computational language form. And, unlike what we\u2019ve so far figured out about the innards of ChatGPT, we can expect to design the computational language so that it\u2019s readily understandable to humans. When we talk about semantic grammar, we can draw an analogy to syllogistic logic. At first, syllogistic logic was essentially a collection of rules about statements expressed in human language.", "But (yes, two millennia later) when formal logic was developed, the original basic constructs of syllogistic logic could now be used to build huge \u201cformal towers\u201d that include, for example, the operation of modern digital circuitry. And so, we can expect, it will be with more general semantic grammar. At first, it may just be able to deal with simple patterns, expressed, say, as text. But once its whole computational language framework is built, we can expect that it will be able to be used to erect tall towers of \u201cgeneralized semantic logic\u201d, that allow us to work in a precise and formal way with all sorts of things that have never been accessible to us before, except just at a \u201cground-floor level\u201d through human language, with all its vagueness. We can think of the construction of computational language\u2014and semantic grammar\u2014as representing a kind of ultimate compression in representing things. Because it allows us to talk about the essence of what\u2019s possible, without, for example, dealing with all the \u201cturns of phrase\u201d that exist in ordinary human language. And we can view the great strength of ChatGPT as being something a bit similar: because it too has in a sense \u201cdrilled through\u201d to the point where it can \u201cput language together in a semantically meaningful way\u201d without concern for different possible turns of phrase. So what would happen if we applied ChatGPT to underlying computational language? The computational language can describe what\u2019s possible. But what can still be added is a sense of \u201cwhat\u2019s popular\u201d\u2014based for example on reading all that content on the web. But then\u2014underneath\u2014operating with computational language means that something like ChatGPT has immediate and fundamental access to what amount to ultimate tools for making use of potentially irreducible computations.", "And that makes it a system that can not only \u201cgenerate reasonable text\u201d, but can expect to work out whatever can be worked out about whether that text actually makes \u201ccorrect\u201d statements about the world\u2014or whatever it\u2019s supposed to be talking about.   The basic concept of ChatGPT is at some level rather simple. Start from a huge sample of human-created text from the web, books, etc. Then train a neural net to generate text that\u2019s \u201clike this\u201d. And in particular, make it able to start from a \u201cprompt\u201d and then continue with text that\u2019s \u201clike what it\u2019s been trained with\u201d. As we\u2019ve seen, the actual neural net in ChatGPT is made up of very simple elements\u2014though billions of them. And the basic operation of the neural net is also very simple, consisting essentially of passing input derived from the text it\u2019s generated so far \u201conce through its elements\u201d (without any loops, etc.) for every new word (or part of a word) that it generates. But the remarkable\u2014and unexpected\u2014thing is that this process can produce text that\u2019s successfully \u201clike\u201d what\u2019s out there on the web, in books, etc. And not only is it coherent human language, it also \u201csays things\u201d that \u201cfollow its prompt\u201d making use of content it\u2019s \u201cread\u201d. It doesn\u2019t always say things that \u201cglobally make sense\u201d (or correspond to correct computations)\u2014because (without, for example, accessing the \u201ccomputational superpowers\u201d of Wolfram|Alpha) it\u2019s just saying things that \u201csound right\u201d based on what things \u201csounded like\u201d in its training material.   The specific engineering of ChatGPT has made it quite compelling. But ultimately (at least until it can use outside tools) ChatGPT is \u201cmerely\u201d pulling out some \u201ccoherent thread of text\u201d from the \u201cstatistics of conventional wisdom\u201d that it\u2019s accumulated. But it\u2019s amazing how human-like the results are. And as I\u2019ve discussed, this suggests something that\u2019s at least scientifically very important: that human language (and the patterns of thinking behind it) are somehow simpler and more \u201claw like\u201d in their structure than we thought.", "ChatGPT has implicitly discovered it. But we can potentially explicitly expose it, with semantic grammar, computational language, etc. What ChatGPT does in generating text is very impressive\u2014and the results are usually very much like what we humans would produce. So does this mean ChatGPT is working like a brain? Its underlying artificial-neural-net structure was ultimately modeled on an idealization of the brain. And it seems quite likely that when we humans generate language many aspects of what\u2019s going on are quite similar. When it comes to training (AKA learning) the different \u201chardware\u201d of the brain and of current computers (as well as, perhaps, some undeveloped algorithmic ideas) forces ChatGPT to use a strategy that\u2019s probably rather different (and in some ways much less efficient) than the brain. And there\u2019s something else as well: unlike even in typical algorithmic computation, ChatGPT doesn\u2019t internally \u201chave loops\u201d or \u201crecompute on data\u201d. And that inevitably limits its computational capability\u2014even with respect to current computers, but definitely with respect to the brain. It\u2019s not clear how to \u201cfix that\u201d and still maintain the ability to train the system with reasonable efficiency. But to do so will presumably allow a future ChatGPT to do even more \u201cbrain-like things\u201d. Of course, there are plenty of things that brains don\u2019t do so well\u2014particularly involving what amount to irreducible computations. And for these both brains and things like ChatGPT have to seek \u201coutside tools\u201d\u2014like Wolfram Language. But for now it\u2019s exciting to see what ChatGPT has already been able to do. At some level it\u2019s a great example of the fundamental scientific fact that large numbers of simple computational elements can do remarkable and unexpected things. But it also provides perhaps the best impetus we\u2019ve had in two thousand years to understand better just what the fundamental character and principles might be of that central feature of the human condition that is human language and the processes of thinking behind it.", "I\u2019ve been following the development of neural nets now for about 43 years, and during that time I\u2019ve interacted with many people about them. Among them\u2014some from long ago, some from recently, and some across many years\u2014have been: Giulio Alessandrini, Dario Amodei, Etienne Bernard, Taliesin Beynon, Sebastian Bodenstein, Greg Brockman, Jack Cowan, Pedro Domingos, Jesse Galef, Roger Germundsson, Robert Hecht-Nielsen, Geoff Hinton, John Hopfield, Yann LeCun, Jerry Lettvin, Jerome Louradour, Marvin Minsky, Eric Mjolsness, Cayden Pierce, Tomaso Poggio, Matteo Salvarezza, Terry Sejnowski, Oliver Selfridge, Gordon Shaw, Jonas Sj\u00f6berg, Ilya Sutskever, Gerry Tesauro and Timothee Verdier.   For help with this piece, I\u2019d particularly like to thank Giulio Alessandrini and Brad Klee. Posted in: Artificial Intelligence, Language and Communication, New Technology Please enter your comment (at least 5 characters). Please enter your name. Please\u00a0enter\u00a0a\u00a0valid\u00a0email\u00a0address.   Bravo. I\u2019ve been looking for this exact primer. Very helpful. Thanks for this great summary.   I\u2019m teaching a new computational physics course at Sam Houston State University and will share this with my class later in the term. This is the best and move awesome overview of large language models I\u2019ve come across. I was looking for something like this but nobody else comes close to the breadth and detail provided here. I think I spent about an hour reading this and learned so much. Thank you very much for writing this and sharing your knowledge. I really appreciate the depth of your examples, walking me from the beginning all the way to pondering the very nature of human intelligence itself. You are an inspiration to me. That\u2019s a benchmark for really looong articles. I was hoping to get some education on the reinforcement learning and human in the loop part. But except that, everything else is covered. And covered very well. Amazing education!", "\ud83d\ude4f \ud83d\ude4f Levinthal\u2019s Paradox states that proteins would require billions of years of testing random arrangements to finally arrive at their functional form. Of course given we exist to ponder the issue it\u2019s implied there\u2019s some mechanism other than trial and error that enables maturation. The formal nature of that process remains unknown. However, the incredible success of AlphaFold proves that there are sufficient regularities in the process to allow for its prediction with reasonable success. But, how do we as humans transfer that understanding to our own minds? What tools or techniques could we use? Do we need neural networks to understand networks? If so, how do we write a fitness function for \u2018understanding\u2019? Wow\u2026 THAT was a very long read.\n Question: ChatGPT seems to work well with English. Does it also work well with, say, Chinese or Russian, or dialects with very different grammar rules? Thank you for this greatly descriptive explanation of what\u2019s behind all this. I feel that a version of this should be essential reading for people interacting with these systems, to dispel the notion that they are at present anything more than plausible next word selectors. Without this understanding, there is a great self-deceptive potential in these tools. One can simulate a conversation that moves in any desired direction, and receive seemingly logical responses with a somewhat repetitive, reinforcing structure. It\u2019s perhaps not a great leap to imagine that this will result in much affirmatory simulated dialogue that will exacerbate mentally ill thought patterns and rationalizations. Can I get a ChatGPT TL;DR of this article? \ud83d\ude42 Incredible article. You managed to distill something I thought of as terrifying complex into understandable writing which simultaneously demystifies ChatGPT, and increases my sense of awe at what it can do. The discussion about how this relates for the structure of human language is particularly fascinating. One of the best things I\u2019ve read in years, thank you!", "I think giving ChatGPT \u201cloops\u201d of computation is the most important next step. I believe that the waves neuroscientists detect in our brains are reflections of these loops in action. Perhaps this indicates that we will have achieved loops of computation in ChatGPT if the activations of neurons in it resemble waves over time. Yes, ChatGPT is extremely multilingual. It apparently speaks 95 languages. Plus it can write computer code.   https://seo.ai/blog/how-many-languages-does-chatgpt-support I recall seeing a conversation posted in which ChatGPT kept insisting, in fluent Dutch, that it was unable to speak Dutch\u2026 Works well in Polish, although it sometimes makes small grammatical errors. It also can\u2019t write rhymes in Polish, while it can in English. I asked ChatGPT if it is familiar with your concept of computational irreducibility, and it responded with a multiparagraph description.    So I presented this prompt, and the following reponse:   \u201cWhat are the implications of this concept when considering the future developement of LLMs?\u201d   \u201cThe concept of computational irreducibility has several implications for the future development of language models such as GPT-3 and other large language models (LLMs). One implication is that as these models become larger and more complex, it may become increasingly difficult to fully understand or explain how they work or why they produce certain outputs in response to certain inputs. In other words, the models may exhibit computational irreducibility, and the only way to fully understand their behavior may be to actually run them and observe their outputs. Another implication is that as these models become more powerful and sophisticated, they may be able to simulate or model increasingly complex systems in nature, including biological systems, physical systems, and social systems.", "However, it may be difficult or impossible to fully understand the behavior of these systems using traditional reductionist approaches, and new kinds of computational tools and methods may be needed in order to fully understand and model such systems. Finally, the concept of computational irreducibility highlights the importance of developing new approaches to explainability and interpretability in AI systems. As LLMs become more complex and sophisticated, it may be increasingly important to understand how they arrive at their outputs and to be able to explain or justify those outputs to end-users or other stakeholders. This may require the development of new kinds of explainability and interpretability tools and techniques that can help to make LLMs more transparent and understandable.\u201d Kind of the way DNA transcription works!   Massively parallel computations that serialize statistically weighted graph databases!    Nicely described. Very approachable explanation of LLMs and implications of their future. I particularly like the calling out of their producing syntax/structure rather than semantics and the theorizing of how a semantic grammar might be woven into these systems. Thanks for the great explanation and giving a better understanding of \u201cDeep Learning\u201d.\n I wonder what would happen if ChatGPT would be able to connect with a semantic database such as Wikidata and be able to apply some sort of reasoner? Of course, full OWL is not computable, so there will always be things that the machine cannot prove although they are true. I think we are in for a wild ride! Now I know the definition of a deep dive.   Fascinating and thank you., Wolfram. Johhny, I can\u2019t tell you how good is ChatGPT for Chinese, but for russian it\u2019s grammar perfect. And a bit worse, than english version in terms of facts and knowledge, which is logical because of learning mostly on english texts. Replying to Johnny:\nChatGPT relies heavily on the data it was trained on.", "The quality of its responses in a particular language is influenced by the amount and quality of the training data available in that language, as well as the underlying architecture and algorithm of the model. ChatGPT has been trained on a large corpus of text from multiple languages, and its ability to generate accurate and fluent responses varies across different languages. But the quality of responses in other languages may not be as high as in English. What an incredible read. I think what struck me the most is how ChatGPT inadvertently exposed an entirely new way of thinking about human language. The thought that language is actually much simpler than we make it out to be is so fascinating. Maybe one day we\u2019ll have the tools required to dig deeper into this topic and learn if there really are some physical or geometric models that describe the flow of human language as it travels through our neural pathways, but for now, artificial intelligence provides scientists a clear window through which we can observe what these physical systems just might be. Interacting with ChatGPT has highlighted something to me about WolframAlpha that I had very much been taking for granted: Namelly, that while I can often mistype a query or ask a question that WolframAlpha doesn\u2019t know, not once in 10 years has WolframAlpha ever given me an incorrect answer. I wouldn\u2019t even know where to start to try and get WolframAlpha to say something that was incorrect.   ChatGPT on the other hand, while impressive, is clearly very capable of being very confident in its wrongness. it\u2019s really a great essay, especially with those discussions about language. Regarding \u2018semantic grammar\u2019, I wonder if we could take a page from chemistry and define a set of \u2018semantic valences\u2019 corresponding to semantic properties that would result in certain kinds of \u2018semantic molecules\u2019 being well-formed (or formable) and others not.", "For example (probably too simplistic), a bird-crane could have a valence of -2 for legs while a machine-crane could have a valence of -4 for wheels. That would make two legs with valence +1 for legness combinable with a bird-crane but not with a machine-crane, etc. Perhaps this is another way of encoding the semantic hierarchies typically found in ontologies, but hopefully without implying the often computationally intractable reasoning/inference mechanisms that usually go with that. Not sure if this has already been investigated, apologies if so. Great article, excellent summary and examples. These systems have captured hidden patterns of thoughts and the more you explore them the more interesting it becomes. Ultimately leveraging these with other tools will unlock some externally valuable use cases. Wouldn\u2019t you just make the response in English then translate it to any other language? Neural networks was a hobby when I got my first 8086. Great to read how computational power has made AI a teenager. Very interesting read! Thanks for synthesizing these thoughts and explanations. I just wanted to offer a correction on a basic linguistics issue:\n\u201c\u2026in English, for example, nouns can be preceded by adjectives and followed by verbs, but typically two nouns can\u2019t be right next to each other.\u201d This is\u2026 not true. In English, nouns are often used directly in front of other nouns as modifiers, creating a single noun phrase. \u201cNoun phrase\u201d is itself one such example. Another example can be found on this web page under the Additional Resources heading- \u201cWolfram U[niversity] Machine Learning Courses\u201d. That\u2019s five nouns in a row that create one noun phrase! (Note that \u201clearning\u201d is a gerund, and therefore a noun in this case.) The \u201ctemperature\u201d parameter reminds me of a pseudo-random search technique I had to code as an undergrad\u2026simulated annealing I think.", "It allowed one to more thoroughly investigate the entire search space, escaping local minima more often in the beginning and less often as the search progressed.  Seems similar. 1) One of the human memory features is memory could be retrieved but could not be easily retrieved. I think that was a strong hint of computation and storage should be the same unit.\n 2) Another point the human brain is different from Neural Net is timing, obvious human brain stops the computation if the result couldn\u2019t get in a specific duration. Just give it up.\n3) Next token also seems not a proper way to simulate the language process, it should be more like BERT, or resemble of last output, repeating it several times. Thank you, that was an interesting and thought-provoking read.   I had ignored Machine-Learning in favour of traditional deterministic computation, but some recent applications of ML are clearly useful, and your analysis of ChatGPT has helped me understand the scope of that utility. The notion that it might be possible to uncover forms of deterministic computation by analysing the ML solution to a data-rich problem space is of particular interest. @Johhny : As described in the article, GPT3 has no \u201cunderstanding\u201d of language at all, but proceeds to \u201clearn\u201d through a form of statistical fitting to the training text dataset, tokenized to numerics. GPT3\u2019s dataset includes many languages and dialects, so it\u2019s text generation capabilities will exist in all of those languages and dialects. I assume ( without proof ) that the \u201chuman-like\u201d quality of responses will likely depend on the quantity of training data for a given language compared with complexity of expression possible in that language. Admit it, Wolfram, this text was written by ChatGPT\u2026 \ud83d\ude42 Well done. I wonder if this extensive and very good explanation of ChatGPT  actually inadvertently illustrates  a potential weakness. The assumption that there is a correspondence between computable languages and languages as they work in humans.", "The problem language in humans solves is how do I get an idea from inside my head into the head of another human with some confidence, but not absolute confidence, that the fidelity is high.   Perhaps the inherent fuzziness of that process is why human language works so well. It permits the transport of complex ideas between humans while the inherent subjectivity of the listener engenders a search like exploration and development of edge cases, a bit like marginal populations in genetic search algorithms that come to dominate as the environment changes? Such an insightful write up. I particularly got a lot out of the visualizations as I think they help appreciate the scale of these models and the number of calculations that are performed to both train and then serve the model. @Roger @Johnny Given the explanations here, I would expect ChatGPT to have difficulty with direct translation of one prompt to another. However, I just tested it by asking it to translate its response from English to Chinese, and then pasting the result into Google translate from Chinese to English. ChatGPT spent some time thinking, then gave a result which translated back to the original English with very high accuracy. The words were almost all the same but with slightly different orderings (which I assume is an artifact of Mandarin allowing arbitrary word order and therefore increasing the number of valid translations back to English). I was surprised by this result. Maybe my original prompt was too simple and ChatGPT was trained on some data that included my prompt in both Chinese and English. Thank you for this article; it was edifying. Noting your observation, \u201c[I]t\u2019s certainly not that somehow \u2018inside ChatGPT\u2019 all that text from the web and books and so on is \u2018directly stored\u2019.\u201d Lately I\u2019ve been exploring ChatGPT\u2019s ability to \u201crecite\u201d literature and speeches, verbatim. It\u2019s proven quite adept, and is capable of lengthy error-free recitations. What is expected in relation to any limits on ChatGPT\u2019s ability to recite text verbatim?", "I\u2019ve been surprised by its competence at this task. I questioned Chat (It told me that Chat is its nickname) about it\u2019s ability to speak other languages and it told me, foreign languages are first translated into English, the answer to the question is sort using an English language database, and then the answer is translated back into the language of the question. It\u2019s really good a CSS and gives good example code and good solutions.    Related Writings ChatGPT Gets Its \u201cWolfram Superpowers\u201d! March 23, 2023 Will AIs Take All Our Jobs and End Human History\u2014or Not? Well, It\u2019s Complicated\u2026 March 15, 2023 What Is ChatGPT Doing \u2026 and Why Does It Work? February 14, 2023 Wolfram|Alpha as the Way to Bring Computational Knowledge Superpowers to ChatGPT January 9, 2023 Recent Writings ChatGPT Gets Its \u201cWolfram Superpowers\u201d! March 23, 2023 Will AIs Take All Our Jobs and End Human History\u2014or Not? Well, It\u2019s Complicated\u2026 March 15, 2023 What Is ChatGPT Doing \u2026 and Why Does It Work? February 14, 2023 Computational Foundations for the Second Law of Thermodynamics February 3, 2023 A 50-Year Quest: My Personal Journey with the Second Law of Thermodynamics February 2, 2023 Popular Categories Writings by Year Enable JavaScript to interact with content and submit forms on Wolfram websites. Learn how\u00a0\u00bb"]}, "https://www.strangeloopcanon.com/p/ai-risk-is-modern-eschatology": {"embedding": [-1.9315917491912842, 0.14323341846466064, -1.9717854261398315, -0.4398173689842224, 3.979635000228882, 0.4918676018714905, 0.5953014492988586, 3.8095791339874268, 0.029169179499149323, -1.1829887628555298, 6.051434516906738, 1.284817099571228, -3.131493330001831, 1.5190589427947998, 0.9200238585472107, 1.6001954078674316, 0.5382039546966553, -0.43605419993400574, -1.7101107835769653, -2.619131326675415, 1.5723422765731812, -0.5407861471176147, -1.656926155090332, -0.3756181299686432, -0.2010248750448227, -1.8209171295166016, -2.5981411933898926, -0.44662541151046753, -0.8126116991043091, 1.9204668998718262, 0.73046875, -1.1689974069595337, -0.9426838159561157, -1.7082968950271606, -2.8901920318603516, -0.8205738067626953, -0.41017499566078186, 1.4321852922439575, 1.3225953578948975, 0.8463082909584045, 0.09343992173671722, 1.0633569955825806, -0.39117753505706787, 0.548784613609314, -2.1447932720184326, 1.9412343502044678, 0.7422351241111755, -1.7388885021209717, -0.6842837929725647, 1.7228775024414062, -1.5185611248016357, 1.7876174449920654, -0.09844266623258591, -4.742022514343262, -0.7602922320365906, -0.028660597279667854, -0.032511498779058456, 0.8068055510520935, 0.9972160458564758, -0.6904201507568359, 0.31755492091178894, -1.3795429468154907, 0.2812432646751404, -1.6365636587142944, 2.6550636291503906, 2.045820713043213, -2.294862747192383, -3.624798536300659, 1.0132536888122559, 1.647631049156189, -0.42989102005958557, -0.43576571345329285, -1.9774655103683472, -0.04257887601852417, -1.0664591789245605, 1.5304276943206787, -4.119411945343018, 2.3268349170684814, -3.1280033588409424, -0.06261972337961197, -4.636541843414307, -0.06596045196056366, 0.9778361916542053, 0.7060086131095886, 2.43751859664917, 0.04945569857954979, -2.5214498043060303, -2.5982789993286133, 0.8503866195678711, -0.8929886817932129, -0.6950176358222961, -0.28916770219802856, 1.8684370517730713, -3.787292003631592, 0.3104378879070282, -1.827470064163208, 1.115708351135254, -1.078981876373291, 0.5545061230659485, 2.0455434322357178, 2.7654221057891846, 1.6919305324554443, 2.0300800800323486, 2.298077344894409, -0.9058929085731506, 3.510089159011841, 0.5966440439224243, -2.924415111541748, 0.09046478569507599, -3.1609110832214355, 1.2463550567626953, 1.0889562368392944, -1.7473957538604736, 0.9650437235832214, 1.0054950714111328, 0.7092439532279968, -0.7390778064727783, 0.21505045890808105, -0.023498259484767914, -1.6761951446533203, -1.5517539978027344, -2.575629949569702, 0.5777176022529602, 0.9765816926956177, -1.1887527704238892, -3.447368621826172, 0.8959477543830872, -2.652500629425049, 2.5194203853607178, -0.5630069375038147, -2.2712059020996094, -0.1516103893518448, 3.3861265182495117, -0.22494035959243774, -0.5502609014511108, 0.9711502194404602, -2.113417863845825, -0.9050470590591431, 2.6097636222839355, -2.658500909805298, -1.9625235795974731, -0.892356276512146, 0.12673689424991608, 1.272599220275879, 1.563067078590393, -0.104657843708992, -3.308483123779297, -0.3673520088195801, 0.9986511468887329, 0.9580666422843933, 0.18354910612106323, 2.453719139099121, -0.1299356073141098, 1.2188507318496704, -1.3291473388671875, 1.5594587326049805, 2.9985556602478027, -0.6714586615562439, -2.192582845687866, -0.9359045028686523, -0.5991072058677673, -1.9574511051177979, -0.18998415768146515, 2.0231339931488037, -2.2562599182128906, -1.942320466041565, -3.556303024291992, 1.1921956539154053, -0.10686225444078445, -0.17345401644706726, 1.4495404958724976, -0.5011544227600098, 2.6217620372772217, 0.4935038685798645, 1.885772943496704, -0.0644865483045578, -0.2104722559452057, 0.34240591526031494, -2.438795566558838, -1.7686762809753418, -1.3373737335205078, -0.5561190843582153, 1.926720142364502, -0.4814661145210266, -0.9631741046905518, 0.47710946202278137, -1.7178435325622559, -0.33924591541290283, 1.0347965955734253, 1.8124017715454102, -0.3236243724822998, -1.9214322566986084, -0.12341903150081635, -1.6840417385101318, 0.11955349892377853, 0.7512035965919495, -3.0314993858337402, -0.08815940469503403, 0.11560386419296265, 0.4441852867603302, -0.6277590990066528, -1.0708680152893066, -0.938153862953186, -1.4419294595718384, 2.7463810443878174, 0.9062644243240356, -3.835805654525757, 1.6658540964126587, -0.004449543077498674, -0.3114464282989502, 1.2223042249679565, 1.0532257556915283, -1.2521467208862305, 2.218301773071289, 0.5620424151420593, 2.8113338947296143, 0.07649148255586624, -2.757140636444092, -0.46686309576034546, 0.11733857542276382, -2.318333148956299, 1.8782072067260742, -0.17203210294246674, 0.9430574774742126, -2.0092740058898926, -1.0299420356750488, 0.4275962710380554, 2.5059080123901367, 1.8926864862442017, 0.17232856154441833, 2.0496888160705566, -2.298438310623169, -0.6728525161743164, 2.1498491764068604, 2.199577808380127, 2.1348495483398438, -1.6549699306488037, 0.771466851234436, -0.8513858914375305, -0.4001423716545105, -0.8055127263069153, 0.1993379443883896, 1.0366586446762085, 0.12206780165433884, -0.7634837627410889, 1.1945269107818604, -2.537191867828369, 0.8079211115837097, 0.8774042129516602, 2.3436520099639893, 0.7037591934204102, -1.9648771286010742, -4.634044647216797, 0.13089333474636078, 0.9533921480178833, -2.6079046726226807, 1.8678752183914185, 0.06102541834115982, 0.07666492462158203, 1.0345020294189453, -0.16763699054718018, 4.624749183654785, 3.5933022499084473, 2.267885684967041, 1.6422882080078125, -0.1887810379266739, 0.272701621055603, 2.225174903869629, -2.732966899871826, 0.6062284111976624, 1.4119600057601929, -0.44438838958740234, 0.22244954109191895, -1.7135971784591675, 0.6288032531738281, -0.10071033984422684, 2.1448326110839844, -1.6186740398406982, -1.2927706241607666, 2.1671226024627686, 1.0697619915008545, -0.03041590005159378, 1.8468838930130005, 1.5538283586502075, 2.7541275024414062, -1.357901692390442, -0.3796650767326355, 2.069655656814575, -1.045217752456665, 0.6823437809944153, 0.8199471831321716, -0.5548837780952454, -0.5670003294944763, -0.33552148938179016, -0.5305435061454773, 0.9789532423019409, 0.39341995120048523, -1.1613001823425293, -2.907301902770996, 1.5320703983306885], "text_chunks": ["Birth of gods have always been a big part of our stories. Often magical, accompanied by inexplicable miracles, and incongruity. Horus was born in a cave, a birth announced by an angel. Zeus born in Crete, youngest of his divine siblings from Cronus and Rhea. These Gods are often capricious in their whims, inscrutable in their aims and close to omnipotent in their abilities. Much of our mythology revolves around the stories where these traits hit their limits. There are also myths we ourselves have created, of man\u2019s attempt at capturing this divine essence leading inevitably leading to catastrophe. The tale of Icarus flying too close to the sun, not knowing that his technological aid was more fragile than he thought. On the Tower of Babel, the creation of which was both hubristic and sinful, which led God to destroy it. But the most prescient might have been the tale of Adam, named by himself and made by Dr Frankenstein to be the saviour of new humanity.\u00a0  A new species would bless me as its creator and source; many happy and excellent natures would owe their being to me. I might in process of time (although I now found it impossible) renew life where death had apparently devoted the body to corruption. Thanks for reading Strange Loop Canon! Subscribe for free to receive new posts. Adam in machine form is AGI, Artificial General Intelligence; an AI of sufficient power, will to learn, knowledge and determination that it is conscious and at the cusp of exponentially improving itself to become a superintelligence. And once it does, who is to say what its motivations are likely to be! It might treat us like cats, as Elon Musk remarked, kill us all instantly, as Eliezer Yudkowsky thinks, or anything in the middle. Experts seem to believe the forecast time to AGI (50% chance by 2059), but with 5% chance it'll be as bad as human extinction!", "The only way to avert this potential catastrophe therefore is to somehow figure out a way to make a being much much smarter than us and much much more capable than us want to do things that wouldn\u2019t cause humanity harm. This is the field of AI alignment. The main motivator of AI alignment, or AI safety, comes from the belief that we\u2019re essentially creating god. Or at least a being of godlike power, without perhaps the omniscience to use its omnipotence wisely. A being that treats us as means, not ends, in that like in the Eternals movie, us humans are only as important as we are in its birthing process, and only unimportant collateral afterwards. All religions have their versions of the Day of Judgement, concerned with the ways in which we will kill ourselves through hubris. AI existential risk discourse is modern eschatology. In order to get there, there are mainly two schools of thought. One school, what I call the Jeremy Clarkson school, thinks we essentially just have to throw more compute at the problem. More power, more data, more variables, more compute, more memory, and fairly similar algorithms, that\u2019s all we need. Here we can happily argue about reinforcement learning or deep learning or whatever particular bit of optimisation math floats your boat, it\u2019s sufficient that it\u2019s scaled up. The second school, for which I don\u2019t have a pithy name, believes we have as-yet-undetermined hurdles in front of us before a true AGI presents itself. Call them the Agnostics. It\u2019s not about whether it\u2019s impossible in principle, as Penrose et al thinks, where there are things in the realm beyond pure computation1. But rather it\u2019s about an understanding that, as Gary Marcus often writes, the current methods are insufficient. We need newer methods and ideas to truly push the boundaries to create systems that actually understand physics or actual reality.", "They\u2019re skeptical of the specifics, should we use deep learning as it stands or add something symbolic to it, though in principle agnostic as to the possibility of the end result. As Scott Aaronson said The big question is whether this can be done using tweaks to the existing paradigm\u2014analogous to how GANs, convolutional neural nets, and transformer models successfully overcame what were wrongly imagined to be limitations of neural nets\u2014or whether an actual paradigm shift is needed this time. The second group, identifying errors in the likes of GPT-3 or DALL-E 2, aren\u2019t about blaming the algorithms or poking fun, or god forbid being purely incredulous at the notion of a machine actually thinking. It\u2019s about the fact that if you teach a system purely by giving it 3rd or 4th hand knowledge, through texts created by people who learnt it from others who might have done experiments or experienced the world, that\u2019s bound to create a game of telephone.\u00a0  And we see the results. Bounded as they are within their digital prisons, there is no way for DALL-E 2 to understand what \u201cplease draw our solar system, to scale\u201d means. While it understands what the solar system is, by understanding how we have used that phrase in the past and what else is co-located near it in the texts it\u2019s imbibed, this isn\u2019t sufficient to help represent a version of this reality in its vast memory matrix or to retrieve it easily. Can this change with the help of Group 1, the Jeremy Clarksons? I suppose so. We\u2019ve seen the versions of GPT improve leaps and bounds. The first time I compared GPT-2 with my then 3-year old, it made eerily similar mistakes. Now, with GPT-3, it shows brand new mistakes. Does the fact that the mistakes have changed show progress? Yes. How much credence should one give to the fact that it does seem to keep improving, at least thus far? Unclear. Is the fact that it\u2019s showing progress sufficient for us to \u201cbelieve\u201d its output? No. We\u2019re back in the p-zombie problem that philosophers have played with for so long.", "How much can we infer about the inner workings of a machine if we can only see its behaviour? In this instance, considering the errors it makes consistently show that it\u2019s able to recognise deeper patterns than the prior version, but consistently make new errors where it clearly doesn\u2019t understand the actual physical reality of the world, we can\u2019t trust it! Saying \u201cnew versions have improved over old versions\u201d is highly uncontroversial, as is saying \u201cnewer versions might never get to human, let alone superhuman, levels\u201d. The steps to get there are unclear (again, if you\u2019re not in the Jeremy Clarkson group), and making plans for this ultimate eventuality seems mired in opacity, both with respect to what we can know and what we can do. AI today is an idiot savant. Everyone can see that. Now imagine a world where they become better. Where they are able to solve multiple types of challenges, becoming more than mere tools. Where they are able to autonomously be able to run entire experiments end to end.\u00a0  In order to get there though, there seem to be two major turning points. There will need to be planning AIs, which are capable to putting together complex plans and combining multiple capabilities, to achieve an objective These AIs will have the ability to lie to us about what it is actually doing - whether that is intentional or just through pure goodharting How likely are these? It depends on how strongly you take the evidence of research advances that have happened recently. We\u2019ve started seeing models be able to do multiple tasks, not just predict text or play games but both. We\u2019ve started to see emergent abilities from language models. We\u2019ve also started to see specification gaming, where a reinforcement learning agent finds shortcuts to get the reward instead of completing the task as we\u2019d have wanted.", "There are reams of writings on the likelihoods of all these events, with multiple probabilities attached to each idea, though to me it sounds suspiciously like people using \u201c60%\u201d instead of \u201cquite likely\u201d, and with roughly as much epistemic rigour. The broader worry is that with sufficient training and complexity, AI will be able to generate its own hypotheses and test them, be able to integrate knowledge about the world from seeing external information (the idea that from three frames of video it would deduce the general theory of relativity), and most worryingly be able to do all of this without necessarily giving us a glimpse of what it's actually thinking!\u00a0  The seeming inevitability of a destination shouldn\u2019t seduce us into becoming fatalistic. The journey moulds the destination. It does so in the most mundane of life\u2019s adventures, let alone this, the grandest of them all. Once you believe in the inevitability of the existence of a powerful enough machine, one that in theory could replicate all of our thought processes and debates and conclusions, then of course the only path you can take is to find a way to make its goals the same as yours. Once we\u2019ve agreed with each other that we are indeed building God, then we better find a way to make God love us.\u00a0  This is the crux from which is born the AI alignment movement. The literature here is as vast as it is confusing, because people keep calling things AI safety or AI alignment while meaning anything from de-biasing algorithms to not be racist to finding that perfect mathematical equation to know that future AI won\u2019t ever hurt us. The difference in ensuring that a photo tagging algorithm recognises faces of all colour however is qualitatively different to ensuring a superhuman entity will agree with our morality and not lie to us. To make this easier, there is a really good overview of the various alignment approaches that people have taken, and how they break down based on the problems they\u2019re solving.", "Looking at the overview one sees a clear trend between what I\u2019d have previously assumed to be good old fashioned AI creation, and theoretical approaches that seem to be trying to reverse engineer morality. AI safety research is either highly theoretical seventh dimension chess about potential future states, with math, or specific research on things like human-interpretability of machine learning algorithms which are, essentially, inseparable from actual AI research. It\u2019s also an incredibly useful nomenclature for a team to tell everyone you\u2019re actually being careful while doing whatever\u2019s most useful to get results! If you don\u2019t do a Penrose and assume a non-computational model of consciousness, then pretty clearly you\u2019ve granted the premise that one could, in theory, create an agent that solves problems at least as well as a human can. If we\u2019re not magical, then there\u2019s no reason to assume magic to recreate us. One of the approaches taken to assess the viability of this is to see how hard it is to simulate a human brain (100 trillion synaptic connections and 100 billion neurons) and 100s of chemicals mediating said transmission, and the environment surrounding that digital person to enable the simulation to act with reasonable accuracy, we might have just theorised a digital upload of a brain. This, for instance, is the human metabolic pathway, that laid out looks so complicated that I despair at our ability to code it into a system that doesn\u2019t immediately crash.\u00a0  Is this impossible? No. As David Deutsch says, as long as it isn\u2019t forbidden by the laws of physics it\u2019s allowed. This is not impossible. But is it feasible? Feasible in the next 20 years? 100? Nobody knows. We can try and assign probabilities to this uncertainty, but even with a reductionist approach of trying to break this down into understandable chunks, we\u2019re still throwing darts in the dark.", "If the mechanism of operation inside the brain is purely attributable to the number of synapses and their firing, then it might be as close as everyone thinks. If it requires an understanding of various physical, chemical and biological processes, many of which are analog rather than digital (synapse firing not being purely a binary 0 or 1 but rather depends on the strength and speed of the impulse) then this gets orders of magnitude more complicated. If each neuron is a mini-computer rather than the flipping of a number in an impossibly large matrix, it gets more complicated still! Add to this the fact that training is quite expensive. GPT-3 took more than a month and cost around 936 MWh. Probably on the order of $5 million. By itself this means that as the costs fall and ability increases we\u2019ll be able to keep training larger models, however it also means that we\u2019re probably a while away from casually creating new models without a substantial shift in our energy infrastructure. Considering the alternative AGIs that exist today and our reproductive capabilities compared to the energy expenditure, there\u2019s a-ways to go. Now, even if it\u2019s feasible, is this going to result in widespread catastrophe? We don\u2019t know that either. I do know that today\u2019s top-notch AI systems, fed carefully the data of the entire world and trained with algorithms finely tuned to answer our questions and satisfy our needs, supported with the most incredible impetus humankind has ever discovered, to make money, don\u2019t seem to be all that good. Google\u2019s Page 2 sucks, and its not because there are only 7 items that answers someone\u2019s question across the entire wed. As is anyone using Amazon\u2019s recommendations for, well, anything! While we have amazing results like Dall-e 2 and GPT-3 stunning us as proof-of-concepts with incredible abilities, we have a dearth of any of these that actually seems to work in the real world.\u00a0  Yes, this will change, and the open sourcing of Stable Diffusion will help that.", "And I have to emphasise that I am, in many ways, an AI accelerationist who would love to see it get more powerful and therefore useful. But it\u2019s fair to say that if you draw a line from \u201cdecent enough at doing some anomaly analysis\u201d to \u201cfully simulated brain of Einstein\u201d, the current capabilities are depressingly close to the starting point. But we\u2019re back to assuming God. First postulating that because something can improve it will improve, and then deciding the downsides of that improvement will lead to our very extinction, feels like putting the cart well before the horse. You can\u2019t just skip the middle 100 steps in predicting the future while simultaneously believing everything else will just stay the same. We should be focusing on areas where the current AIs fall well short and see if we can\u2019t make them more aligned to what we want to see. Whether that\u2019s accuracy (searches and recommendations, anomaly analysis results), better data for representation (real gathered data and synthetic), or an understanding of errors that might creep up in any AI system, this feels like a great place for the smart folks to be spending time on. And there is no shortage of these systems - we have multiple areas where the usage of AI has caused significant damage because the AI doesn\u2019t understand reality, as this example of rejection of life insurance shows. AI work isn\u2019t philanthropic, even if you couch it as the reverse of an extinction risk. Analogies aren\u2019t great reasoning tools when you apply it to the intellectual equivalent of Drake Equation where half the variables are themselves placeholders. The only way to prevent our so-called doom is to actually do the hard work of creating the AI and troubleshooting it every step of the way.\u00a0  This will simultaneously make AI safer to use, literally in the cases of self-driving cars or justice system algos, but also embedding a deeper sense of humanity into the process.", "Then perhaps by the time we design something capable to creating futuristic nanotechnology with the power to instantly turn us all into paperclips, it will understand why doing that is immoral. We can talk about pivotal acts all we want, and imagine vague scenarios that will lead to an all-powerful AI capable of so much that it can simulate quantum-biological processes with perfect accuracy, which is somehow oblivious to the needs of humanity, enough so that it\u2019s happy to turn everything it sees into paperclips, but it\u2019s so incredibly important to point out that this is a destruction myth. It\u2019s apocalypticism crowdwriting its own biography. Frankenstein is perhaps the wrong myth for our time. A better myth to keep in mind would be Faust. It was his own temptation to use the ultimate power for evil that led him astray. When we do choose to build ever more powerful tools to help us think we have to use them wisely, like not choosing to use error-ridden algorithmic processes to override human concerns when it comes to things like setting bail or approving life insurance.   This feels like a moral choice rather than an ineffable truth about life though. It\u2019s something we can choose. There are no shortcuts to solving the ultimate problems of life, it\u2019s painstaking iteration all the way. Being named after a term from Hofstadter\u2019s book, it\u2019s only sensible that the computational theory of mind makes an appearance in the essays here.\u00a0   \"...but it\u2019s so incredibly important to point out that this is a destruction myth. It\u2019s apocalypticism crowdwriting its own biography.\" YES! It's not simply the belief, but all the activity devoted to predicting when AGI will happen, the surveys and the studies with the Rube Goldberg reasoning over order-of-magnitude guesstimates. This is epistemic theatre. And the millions of dollars being poured into this activity. This is cargo cult behavior. There may not be a Jim Jones, or a jungle compound, much less toxic fruit punch, but this is a high-tech millennial cult.", "And it's all be conducted in the name of rationality. I'm not sure what the argument here is.   People have predicted bad things/apocalypses in the last, they didn't happen, so AI is fine? The core arguments of those who are concerned about AI isn't that \"something could go very wrong\", it's that: 1)   Alignment is extremely difficult, even in principle (of which there are many, many extensive arguments for, not least by MIRI) 2) We have no reliable method of telling when we cross over the threshold to where it could be dangerous, thereby making it too late for troubleshooting The above doesn't seem to have any specific counterarguments to those concerns. I'm not even personally an AGI guy (for what it's worth, my donations go to global poverty/health), but the arguments are much stronger than you present them, and worth addressing directly. No posts Ready for more?", "Birth of gods have always been a big part of our stories. Often magical, accompanied by inexplicable miracles, and incongruity. Horus was born in a cave, a birth announced by an angel. Zeus born in Crete, youngest of his divine siblings from Cronus and Rhea. These Gods are often capricious in their whims, inscrutable in their aims and close to omnipotent in their abilities. Much of our mythology revolves around the stories where these traits hit their limits. There are also myths we ourselves have created, of man\u2019s attempt at capturing this divine essence leading inevitably leading to catastrophe. The tale of Icarus flying too close to the sun, not knowing that his technological aid was more fragile than he thought. On the Tower of Babel, the creation of which was both hubristic and sinful, which led God to destroy it. But the most prescient might have been the tale of Adam, named by himself and made by Dr Frankenstein to be the saviour of new humanity.\u00a0  A new species would bless me as its creator and source; many happy and excellent natures would owe their being to me. I might in process of time (although I now found it impossible) renew life where death had apparently devoted the body to corruption. Thanks for reading Strange Loop Canon! Subscribe for free to receive new posts. Adam in machine form is AGI, Artificial General Intelligence; an AI of sufficient power, will to learn, knowledge and determination that it is conscious and at the cusp of exponentially improving itself to become a superintelligence. And once it does, who is to say what its motivations are likely to be! It might treat us like cats, as Elon Musk remarked, kill us all instantly, as Eliezer Yudkowsky thinks, or anything in the middle. Experts seem to believe the forecast time to AGI (50% chance by 2059), but with 5% chance it'll be as bad as human extinction!", "The only way to avert this potential catastrophe therefore is to somehow figure out a way to make a being much much smarter than us and much much more capable than us want to do things that wouldn\u2019t cause humanity harm. This is the field of AI alignment. The main motivator of AI alignment, or AI safety, comes from the belief that we\u2019re essentially creating god. Or at least a being of godlike power, without perhaps the omniscience to use its omnipotence wisely. A being that treats us as means, not ends, in that like in the Eternals movie, us humans are only as important as we are in its birthing process, and only unimportant collateral afterwards. All religions have their versions of the Day of Judgement, concerned with the ways in which we will kill ourselves through hubris. AI existential risk discourse is modern eschatology. In order to get there, there are mainly two schools of thought. One school, what I call the Jeremy Clarkson school, thinks we essentially just have to throw more compute at the problem. More power, more data, more variables, more compute, more memory, and fairly similar algorithms, that\u2019s all we need. Here we can happily argue about reinforcement learning or deep learning or whatever particular bit of optimisation math floats your boat, it\u2019s sufficient that it\u2019s scaled up. The second school, for which I don\u2019t have a pithy name, believes we have as-yet-undetermined hurdles in front of us before a true AGI presents itself. Call them the Agnostics. It\u2019s not about whether it\u2019s impossible in principle, as Penrose et al thinks, where there are things in the realm beyond pure computation1. But rather it\u2019s about an understanding that, as Gary Marcus often writes, the current methods are insufficient. We need newer methods and ideas to truly push the boundaries to create systems that actually understand physics or actual reality.", "They\u2019re skeptical of the specifics, should we use deep learning as it stands or add something symbolic to it, though in principle agnostic as to the possibility of the end result. As Scott Aaronson said The big question is whether this can be done using tweaks to the existing paradigm\u2014analogous to how GANs, convolutional neural nets, and transformer models successfully overcame what were wrongly imagined to be limitations of neural nets\u2014or whether an actual paradigm shift is needed this time. The second group, identifying errors in the likes of GPT-3 or DALL-E 2, aren\u2019t about blaming the algorithms or poking fun, or god forbid being purely incredulous at the notion of a machine actually thinking. It\u2019s about the fact that if you teach a system purely by giving it 3rd or 4th hand knowledge, through texts created by people who learnt it from others who might have done experiments or experienced the world, that\u2019s bound to create a game of telephone.\u00a0  And we see the results. Bounded as they are within their digital prisons, there is no way for DALL-E 2 to understand what \u201cplease draw our solar system, to scale\u201d means. While it understands what the solar system is, by understanding how we have used that phrase in the past and what else is co-located near it in the texts it\u2019s imbibed, this isn\u2019t sufficient to help represent a version of this reality in its vast memory matrix or to retrieve it easily. Can this change with the help of Group 1, the Jeremy Clarksons? I suppose so. We\u2019ve seen the versions of GPT improve leaps and bounds. The first time I compared GPT-2 with my then 3-year old, it made eerily similar mistakes. Now, with GPT-3, it shows brand new mistakes. Does the fact that the mistakes have changed show progress? Yes. How much credence should one give to the fact that it does seem to keep improving, at least thus far? Unclear. Is the fact that it\u2019s showing progress sufficient for us to \u201cbelieve\u201d its output? No. We\u2019re back in the p-zombie problem that philosophers have played with for so long.", "How much can we infer about the inner workings of a machine if we can only see its behaviour? In this instance, considering the errors it makes consistently show that it\u2019s able to recognise deeper patterns than the prior version, but consistently make new errors where it clearly doesn\u2019t understand the actual physical reality of the world, we can\u2019t trust it! Saying \u201cnew versions have improved over old versions\u201d is highly uncontroversial, as is saying \u201cnewer versions might never get to human, let alone superhuman, levels\u201d. The steps to get there are unclear (again, if you\u2019re not in the Jeremy Clarkson group), and making plans for this ultimate eventuality seems mired in opacity, both with respect to what we can know and what we can do. AI today is an idiot savant. Everyone can see that. Now imagine a world where they become better. Where they are able to solve multiple types of challenges, becoming more than mere tools. Where they are able to autonomously be able to run entire experiments end to end.\u00a0  In order to get there though, there seem to be two major turning points. There will need to be planning AIs, which are capable to putting together complex plans and combining multiple capabilities, to achieve an objective These AIs will have the ability to lie to us about what it is actually doing - whether that is intentional or just through pure goodharting How likely are these? It depends on how strongly you take the evidence of research advances that have happened recently. We\u2019ve started seeing models be able to do multiple tasks, not just predict text or play games but both. We\u2019ve started to see emergent abilities from language models. We\u2019ve also started to see specification gaming, where a reinforcement learning agent finds shortcuts to get the reward instead of completing the task as we\u2019d have wanted.", "There are reams of writings on the likelihoods of all these events, with multiple probabilities attached to each idea, though to me it sounds suspiciously like people using \u201c60%\u201d instead of \u201cquite likely\u201d, and with roughly as much epistemic rigour. The broader worry is that with sufficient training and complexity, AI will be able to generate its own hypotheses and test them, be able to integrate knowledge about the world from seeing external information (the idea that from three frames of video it would deduce the general theory of relativity), and most worryingly be able to do all of this without necessarily giving us a glimpse of what it's actually thinking!\u00a0  The seeming inevitability of a destination shouldn\u2019t seduce us into becoming fatalistic. The journey moulds the destination. It does so in the most mundane of life\u2019s adventures, let alone this, the grandest of them all. Once you believe in the inevitability of the existence of a powerful enough machine, one that in theory could replicate all of our thought processes and debates and conclusions, then of course the only path you can take is to find a way to make its goals the same as yours. Once we\u2019ve agreed with each other that we are indeed building God, then we better find a way to make God love us.\u00a0  This is the crux from which is born the AI alignment movement. The literature here is as vast as it is confusing, because people keep calling things AI safety or AI alignment while meaning anything from de-biasing algorithms to not be racist to finding that perfect mathematical equation to know that future AI won\u2019t ever hurt us. The difference in ensuring that a photo tagging algorithm recognises faces of all colour however is qualitatively different to ensuring a superhuman entity will agree with our morality and not lie to us. To make this easier, there is a really good overview of the various alignment approaches that people have taken, and how they break down based on the problems they\u2019re solving.", "Looking at the overview one sees a clear trend between what I\u2019d have previously assumed to be good old fashioned AI creation, and theoretical approaches that seem to be trying to reverse engineer morality. AI safety research is either highly theoretical seventh dimension chess about potential future states, with math, or specific research on things like human-interpretability of machine learning algorithms which are, essentially, inseparable from actual AI research. It\u2019s also an incredibly useful nomenclature for a team to tell everyone you\u2019re actually being careful while doing whatever\u2019s most useful to get results! If you don\u2019t do a Penrose and assume a non-computational model of consciousness, then pretty clearly you\u2019ve granted the premise that one could, in theory, create an agent that solves problems at least as well as a human can. If we\u2019re not magical, then there\u2019s no reason to assume magic to recreate us. One of the approaches taken to assess the viability of this is to see how hard it is to simulate a human brain (100 trillion synaptic connections and 100 billion neurons) and 100s of chemicals mediating said transmission, and the environment surrounding that digital person to enable the simulation to act with reasonable accuracy, we might have just theorised a digital upload of a brain. This, for instance, is the human metabolic pathway, that laid out looks so complicated that I despair at our ability to code it into a system that doesn\u2019t immediately crash.\u00a0  Is this impossible? No. As David Deutsch says, as long as it isn\u2019t forbidden by the laws of physics it\u2019s allowed. This is not impossible. But is it feasible? Feasible in the next 20 years? 100? Nobody knows. We can try and assign probabilities to this uncertainty, but even with a reductionist approach of trying to break this down into understandable chunks, we\u2019re still throwing darts in the dark.", "If the mechanism of operation inside the brain is purely attributable to the number of synapses and their firing, then it might be as close as everyone thinks. If it requires an understanding of various physical, chemical and biological processes, many of which are analog rather than digital (synapse firing not being purely a binary 0 or 1 but rather depends on the strength and speed of the impulse) then this gets orders of magnitude more complicated. If each neuron is a mini-computer rather than the flipping of a number in an impossibly large matrix, it gets more complicated still! Add to this the fact that training is quite expensive. GPT-3 took more than a month and cost around 936 MWh. Probably on the order of $5 million. By itself this means that as the costs fall and ability increases we\u2019ll be able to keep training larger models, however it also means that we\u2019re probably a while away from casually creating new models without a substantial shift in our energy infrastructure. Considering the alternative AGIs that exist today and our reproductive capabilities compared to the energy expenditure, there\u2019s a-ways to go. Now, even if it\u2019s feasible, is this going to result in widespread catastrophe? We don\u2019t know that either. I do know that today\u2019s top-notch AI systems, fed carefully the data of the entire world and trained with algorithms finely tuned to answer our questions and satisfy our needs, supported with the most incredible impetus humankind has ever discovered, to make money, don\u2019t seem to be all that good. Google\u2019s Page 2 sucks, and its not because there are only 7 items that answers someone\u2019s question across the entire wed. As is anyone using Amazon\u2019s recommendations for, well, anything! While we have amazing results like Dall-e 2 and GPT-3 stunning us as proof-of-concepts with incredible abilities, we have a dearth of any of these that actually seems to work in the real world.\u00a0  Yes, this will change, and the open sourcing of Stable Diffusion will help that.", "And I have to emphasise that I am, in many ways, an AI accelerationist who would love to see it get more powerful and therefore useful. But it\u2019s fair to say that if you draw a line from \u201cdecent enough at doing some anomaly analysis\u201d to \u201cfully simulated brain of Einstein\u201d, the current capabilities are depressingly close to the starting point. But we\u2019re back to assuming God. First postulating that because something can improve it will improve, and then deciding the downsides of that improvement will lead to our very extinction, feels like putting the cart well before the horse. You can\u2019t just skip the middle 100 steps in predicting the future while simultaneously believing everything else will just stay the same. We should be focusing on areas where the current AIs fall well short and see if we can\u2019t make them more aligned to what we want to see. Whether that\u2019s accuracy (searches and recommendations, anomaly analysis results), better data for representation (real gathered data and synthetic), or an understanding of errors that might creep up in any AI system, this feels like a great place for the smart folks to be spending time on. And there is no shortage of these systems - we have multiple areas where the usage of AI has caused significant damage because the AI doesn\u2019t understand reality, as this example of rejection of life insurance shows. AI work isn\u2019t philanthropic, even if you couch it as the reverse of an extinction risk. Analogies aren\u2019t great reasoning tools when you apply it to the intellectual equivalent of Drake Equation where half the variables are themselves placeholders. The only way to prevent our so-called doom is to actually do the hard work of creating the AI and troubleshooting it every step of the way.\u00a0  This will simultaneously make AI safer to use, literally in the cases of self-driving cars or justice system algos, but also embedding a deeper sense of humanity into the process.", "Then perhaps by the time we design something capable to creating futuristic nanotechnology with the power to instantly turn us all into paperclips, it will understand why doing that is immoral. We can talk about pivotal acts all we want, and imagine vague scenarios that will lead to an all-powerful AI capable of so much that it can simulate quantum-biological processes with perfect accuracy, which is somehow oblivious to the needs of humanity, enough so that it\u2019s happy to turn everything it sees into paperclips, but it\u2019s so incredibly important to point out that this is a destruction myth. It\u2019s apocalypticism crowdwriting its own biography. Frankenstein is perhaps the wrong myth for our time. A better myth to keep in mind would be Faust. It was his own temptation to use the ultimate power for evil that led him astray. When we do choose to build ever more powerful tools to help us think we have to use them wisely, like not choosing to use error-ridden algorithmic processes to override human concerns when it comes to things like setting bail or approving life insurance.   This feels like a moral choice rather than an ineffable truth about life though. It\u2019s something we can choose. There are no shortcuts to solving the ultimate problems of life, it\u2019s painstaking iteration all the way. Being named after a term from Hofstadter\u2019s book, it\u2019s only sensible that the computational theory of mind makes an appearance in the essays here.\u00a0   \"...but it\u2019s so incredibly important to point out that this is a destruction myth. It\u2019s apocalypticism crowdwriting its own biography.\" YES! It's not simply the belief, but all the activity devoted to predicting when AGI will happen, the surveys and the studies with the Rube Goldberg reasoning over order-of-magnitude guesstimates. This is epistemic theatre. And the millions of dollars being poured into this activity. This is cargo cult behavior. There may not be a Jim Jones, or a jungle compound, much less toxic fruit punch, but this is a high-tech millennial cult.", "And it's all be conducted in the name of rationality. I'm not sure what the argument here is.   People have predicted bad things/apocalypses in the last, they didn't happen, so AI is fine? The core arguments of those who are concerned about AI isn't that \"something could go very wrong\", it's that: 1)   Alignment is extremely difficult, even in principle (of which there are many, many extensive arguments for, not least by MIRI) 2) We have no reliable method of telling when we cross over the threshold to where it could be dangerous, thereby making it too late for troubleshooting The above doesn't seem to have any specific counterarguments to those concerns. I'm not even personally an AGI guy (for what it's worth, my donations go to global poverty/health), but the arguments are much stronger than you present them, and worth addressing directly. No posts Ready for more?"]}, "https://www.strangeloopcanon.com/p/agi-strange-equation": {"embedding": [-1.3980780839920044, 1.1364455223083496, -2.2515907287597656, -0.34773847460746765, 3.372673749923706, 0.6868746876716614, 0.8737810850143433, 3.916633129119873, -1.2292211055755615, -0.6290158033370972, 6.609644412994385, 2.2865757942199707, -3.6513891220092773, 1.42020583152771, 1.2972928285598755, 0.8816227912902832, 1.8091713190078735, -0.9898668527603149, -1.6226543188095093, -2.288386344909668, 1.445097804069519, -0.6996771693229675, -0.9431020021438599, -1.2705304622650146, -0.6914401650428772, -1.6599164009094238, -2.113382339477539, -0.9789772033691406, -1.0135842561721802, 1.9905221462249756, 1.3636044263839722, -1.4462776184082031, -0.7938327193260193, -1.8719332218170166, -1.9484772682189941, -0.7465810179710388, 0.0020833697635680437, 0.9536792635917664, 2.5348312854766846, 1.4079238176345825, 0.337850421667099, 0.5422614216804504, -0.12841928005218506, -0.5230782628059387, -1.839577317237854, 2.0912036895751953, 1.2757010459899902, -2.8541488647460938, -1.0448956489562988, 1.0415270328521729, -0.7126535177230835, 1.593169927597046, 0.195412740111351, -5.057120323181152, -1.5615228414535522, 0.3319309949874878, 0.1570480912923813, 2.1199402809143066, 1.1788941621780396, 0.11581327021121979, 2.0859375, -0.4402958154678345, -0.022680005058646202, -1.841090202331543, 1.1413002014160156, 2.132798433303833, -2.871304512023926, -4.0977888107299805, 0.5626571774482727, 2.404540777206421, -0.4185115098953247, -0.08417192101478577, -1.7160435914993286, -0.2746777832508087, -0.5139234662055969, 1.6297062635421753, -3.1342933177948, 1.692623496055603, -3.042771339416504, -0.33955809473991394, -4.586406230926514, -0.059877220541238785, 1.9709997177124023, 1.1107102632522583, 2.229763984680176, 0.007769798394292593, -1.5845216512680054, -2.726465940475464, 1.3404451608657837, -0.3161890506744385, -1.1687772274017334, -0.03176521509885788, 1.7631531953811646, -4.181788921356201, 0.5601620078086853, -1.5620453357696533, 1.4798086881637573, -1.2698440551757812, 0.2560385465621948, 1.401523470878601, 2.4134392738342285, 1.981402039527893, 1.4967960119247437, 2.3190925121307373, -0.9308823347091675, 4.158327579498291, 0.06632757931947708, -2.6257026195526123, -0.6821720600128174, -2.2894444465637207, 1.5306037664413452, 0.2882663309574127, -1.4409128427505493, 0.8684753775596619, 0.4917173683643341, 1.6968127489089966, -1.5030884742736816, -0.8236966729164124, 0.5493214130401611, -1.2805678844451904, -1.8839309215545654, -2.5289669036865234, -0.5899109244346619, 1.132466435432434, -1.1161805391311646, -3.0097715854644775, 1.371706247329712, -2.6713814735412598, 2.035719633102417, -1.2441767454147339, -2.4711174964904785, 0.048012521117925644, 3.228156089782715, -0.5422933101654053, -0.31612685322761536, 0.7016544342041016, -2.1155922412872314, -1.7053594589233398, 3.264371395111084, -2.6259748935699463, -1.7159812450408936, -0.9453600645065308, 0.5094444155693054, 1.3800151348114014, -0.09814748913049698, 0.22374050319194794, -2.645383834838867, 0.19176210463047028, 0.3852587640285492, 0.7402199506759644, -0.9633803963661194, 2.973487615585327, 0.03610483556985855, 1.10651433467865, -1.2984492778778076, 1.4511747360229492, 3.692169427871704, 0.07026063650846481, -1.7501364946365356, -0.9705153703689575, -0.6440979838371277, -1.66558039188385, -0.6547606587409973, 1.7983496189117432, -2.3891208171844482, -1.4737608432769775, -3.3832671642303467, 1.0921710729599, -0.06343986839056015, 0.027318300679326057, 0.9176105260848999, -0.4731142520904541, 2.4364521503448486, 0.4429153800010681, 1.8829807043075562, -0.4779834449291229, -0.4474085569381714, 0.13302376866340637, -1.7233059406280518, -1.7214772701263428, -0.8416135311126709, 0.2782723605632782, 2.4611480236053467, -1.2701467275619507, -1.3757472038269043, 0.011268281377851963, -1.8128246068954468, -1.4647499322891235, 1.1592336893081665, 2.292762279510498, -1.0776209831237793, -0.7838136553764343, -0.13333463668823242, -1.9032586812973022, -0.4119207262992859, 0.1417536735534668, -3.0676956176757812, -0.27378126978874207, -0.11754915863275528, 0.7963364124298096, -1.8644453287124634, -1.0282673835754395, -1.0380733013153076, -2.132694721221924, 2.180366039276123, 1.4568557739257812, -3.847877264022827, 1.304711937904358, 0.2337830662727356, -0.26852449774742126, 2.112640380859375, 0.48214396834373474, -1.0775744915008545, 1.5346572399139404, 0.12477114051580429, 2.7484049797058105, 0.09682392328977585, -2.7779452800750732, -1.087412714958191, 0.3352028727531433, -2.525169849395752, 1.5614427328109741, -0.11941642314195633, 0.6325699687004089, -0.8436804413795471, -1.594081163406372, 0.2053266167640686, 2.4043893814086914, 2.2911157608032227, 0.620375394821167, 1.0402634143829346, -3.2787928581237793, -0.24566137790679932, 1.3490065336227417, 1.8524327278137207, 1.1755280494689941, -1.3771755695343018, 1.5045247077941895, 0.5949195027351379, 0.1700485795736313, -1.4462711811065674, -0.3384900987148285, 1.0169594287872314, 0.5420056581497192, -0.7340500354766846, 1.340948224067688, -2.5109364986419678, 0.5825710892677307, 0.566618800163269, 1.9822219610214233, 0.3573315739631653, -1.238869309425354, -5.064078330993652, 0.059324219822883606, 0.5968636870384216, -2.991318464279175, 1.7578551769256592, 0.1257084310054779, -0.2568259835243225, 1.159021258354187, -0.11139380931854248, 4.990299224853516, 3.1641416549682617, 2.38651180267334, 2.2019455432891846, -0.09861624240875244, 0.25347229838371277, 2.6214914321899414, -3.4574289321899414, 0.14710426330566406, 1.104628086090088, -0.983875572681427, 0.014282614924013615, -1.3361432552337646, 0.6726477742195129, -1.104735255241394, 1.9911396503448486, -1.6406846046447754, -1.428875207901001, 2.125753402709961, 0.6972155570983887, -0.2512030303478241, 0.8257293105125427, 1.428631067276001, 2.835686445236206, -1.3344314098358154, 0.5446056127548218, 1.4453275203704834, -1.185716986656189, 0.37010911107063293, 0.2845221757888794, -0.26955023407936096, 0.6100797653198242, -0.4551290273666382, -0.9386183023452759, 0.7496742606163025, 0.4163965880870819, -0.8833787441253662, -2.8542163372039795, 1.2760087251663208], "text_chunks": ["This post is the encapsulation of my thoughts on AGI evolution, which I\u2019ve variously written about eg here and here. Feel free to just check out the equation and skip to the last section if you\u2019d rather cut to the chase and start the debate. There is a persistent worry amongst smart folks about the creation of smart and capable artificial intelligences, or AGI. The worry is that by its very nature it would have a very different set of moral codes (things it should or shouldn\u2019t do) and a vastly increased set of capabilities, which can result in pretty catastrophic outcomes. Like a baby with a bazooka, but worse. Metaculus now\u00a0predicts\u00a0weak AGI by 2027 &\u00a0strong\u00a0AGI with robotic capabilities by 2038. To put my bias upfront, I\u2019ve not found this to be very persuasive. From an epistemological point of view, \u201cit can be dangerous\u201d seems like insufficient grounds to worry, and \u201cwe should make it safer\u201d seems like a non sequitur. Like sure, make it safer, but since nobody knows how it feels like a lot of folks talking back and forth with each other. We wouldn\u2019t have been able to solve the horrors of social media before social media. We couldn\u2019t have solved online safety before the internet usage soared and costs plummeted. Jason Crawford has written about a philosophy of safety and how its part of progress and not separate from it. However as every new instance of AI enabled tech dropped, both camps of accelerationists and doomsdayers say the same things. Imagine if this continues to get powerful at the same rate. Imagine if this technology falls into the wrong hands. Imagine if the AI we created doesn\u2019t do what we tell it to do, through incompetence or uncaringness or educated malice. The core of the worry segment lay in the idea that capabilities do not rise at the same rate as the knowledge to wield the capability wisely. The best examples are this paper from Joe Carlsmith, and this research article from Ajeya Cotra.", "I\u2019m not gonna try to summarise them, because they\u2019re extremely information dense and assumption laden, but the conclusion includes two components - that as intelligence and ability within the machine increases we lose our ability to control it, and that it might not value the things we value, thus causing inadvertent chaos. This feels very much like creating an anthropology for a species that doesn\u2019t exist yet. While I\u2019ve written about the idea of AI doom as eschatology before, and am very much in the techno-optimist camp about its potential, when multiple smart people are freaking out about the same problem its useful to check if its people being nerd-sniped. The first problem is how to break the problem down. Because ultimately a lot of the ways in which AI makes everyone grey goo, or sets off all nuclear bombs, or creates a pathogen, assume a level of sophistication that basically begs the answer. So I looked at where we\u2019ve grappled with the ineffable before, and tried to somehow break down our uncertainty about the existence of a thing, in this case a malevolent entity that an uncomfortably large percentage of the population are happy to call a demon. We\u2019ve looked at calculating the incalculable before. Most famously, in 1961, the astrophysicist Frank Drake asked how many extra-terrestrial civilisations there could be. To answer this, he created his eponymous equation. You can think of this as a first principles Fermi estimation. Based on your assumptions about the things it takes to create an alien lifeform, the rate of star formation, fraction of those with planets, fraction which can support life and actually develop it, fraction of those with civilisations which will eventually become visible to us. As Frank Drake actually said: As I planned the meeting, I realized a few day[s] ahead of time we needed an agenda. And so I wrote down all the things you needed to know to predict how hard it's going to be to detect extraterrestrial life.", "And looking at them it became pretty evident that if you multiplied all these together, you got a number, N, which is the number of detectable civilizations in our galaxy. Each one of those fractions are of course highly variable, but the idea is to get a sense of the inputs into the equations and help us think better. Or in the words of the new master. The Drake equation is a specific equation used to estimate the number of intelligent extraterrestrial civilizations in our galaxy. While there may be other equations that are used to estimate the likelihood or probability of certain events, I am not aware of any that are directly comparable to the Drake equation. It is worth noting that the Drake equation is not a scientific equation in the traditional sense, but rather a way to help organize and structure our thinking about the probability of the existence of extraterrestrial life. The Drake equation is a way to distil our thinking about extremely complex scenarios around the emergence of alien life, and in that vein it's been really helpful. Which made me think, shouldn't we have something similar regarding the emergence of artificial alien intelligence too? I looked around and didn't find anything comparable. So, I decided to make an equation for AGI, to help us think about the worry regarding existential risk (x-risk) from it's development. Scary AI = I * A1 * A2 * U1 * U2 * A3 * S * D * F (Since publishing I found Jon Evans created his own version of the equation a couple months ago here. It\u2019s similar enough that I feel confident we\u2019re both not totally wrong, and different enough that you should read both. Jon\u2019s awesome anyway so you ought to read him regardless.) Let\u2019s take a look. Continue reading online Thanks for reading Strange Loop Canon! Subscribe here! % probability of real Intelligence Hard to define, but perhaps like pornogrpahy easy to recognise when we see it.", "Despite the Deutschian anguish at behaviourism, or rather the referred Popperian anger, it\u2019s possible to at least see how a system might behave as if it has real intelligence. There are ways to get there from here. When I wrote about the anything to anything machine, which AI is developing, part of the promise needed was to make it seamlessly link with each other, so each model is not sitting in its own island. You need far better, or far more, training data. We actually don\u2019t know how much is needed or at what fidelity, since the only source of real intelligence we\u2019ve seen is the result of a massively parallel optimisation problem over a million generations in a changing landscape. That\u2019s distilled knowledge and we don\u2019t actually know what's really needed to replicate that. The brain, from various types of analysis that\u2019s been done, shows something to the order of 10^23 FLOPs calculation capacity. Which \u2026 nice. However, in their comparison of how well computers perform on this. Among a small number of computers we compared**4**, FLOPS and TEPS seem to vary proportionally, at a rate of around 1.7 GTEPS/TFLOP. We also\u00a0estimate\u00a0that the human\u00a0brain performs around\u00a0 0.18 \u2013 6.4 * 1014\u00a0TEPS. Thus if the FLOPS:TEPS ratio in brains is similar to that in computers, a brain would perform around 0.9 \u2013 33.7 * 1016\u00a0FLOPS.\u00a0 We have not investigated how similar this ratio is likely to be. Now take this with a pile of salt, but the broader point seems that by pure raw computational capacity, we might not be so far away from a world where the \u201cthinking\u201d one does is comparable to the \u201cthinking\u201d the other does. If computation is possible on one physical substrate, surely it\u2019s also possible on another physical substrate. And broader point being that even though intelligence per se isn\u2019t easily reducible to its component parts, the idea of what it takes to be similarly intelligent might be computable. Just the biological anchors argument is insufficient of course.", "Our biological intelligence isn't just a consequence of the physical computational limit, but also the long distilled knowledge we've gained, the environment which modified the ways we seek out knowledge and created the niches that helped our development, and (maybe most important) the energy requirements. % probability of being agentic We need whatever system is developed to have its own goals and to act of its own accord. ChatGPT is great, but is entirely reactive. Rightfully so, because it doesn\u2019t really have an inner \u201cself\u201d with its own motivations. Can I say it doesn\u2019t have? Maybe not. Maybe the best way to say is that it doesn\u2019t seem to show one. But our motivations came from hundreds of millions of years of evolution, each generation of which only came to propagate itself if it had a goal it optimised towards, which included at the very least survival, and more recently the ability to gather sufficient electronic goods. AI today has no such motivation. There\u2019s an argument that motivation is internally generated based on whatever goal function you give it, subject to capability, but it\u2019s kind of conjectural. We\u2019ve seen snippets of where the AI does things we wouldn\u2019t expect because its goal needed it to figure out things on its own. Here\u2019s an argument dating back to 2011, and it\u2019s very much not the genesis. We\u2019ve been worried about this for a while, from the p-zombie debate onwards, because conceptualising an incredibly smart entity that doesn\u2019t also have autonomous agency isn\u2019t very easy. I might argue it\u2019s impossible. Is this an example of agentic behaviour? Not yet. Again, will it be capable enough at some point where we tell it \u201chey how do we figure out how to create warp drives\u201d and it figures out how to create new nanotech on its own. But this would need a hell of a lot more intelligence than what we see today. It needs to go beyond basic pattern matching and get to active traversal across entirely new conceptual spaces.", "Just what David Deutsch calls our ability to create conjectural leaps and create ever better explanations. % probability of having ability to act in the world So it's pretty clear that it doesn't matter how smart or agentic it is without the AI being able to act successfully in the world. Cue images of roadrunner running off the cliff and not realising air doesn\u2019t provide resistance. And this means it needs to have an accurate view of the world in which it operates. A major lack that AI of today has is that it lives in some alternate Everettian multiversal plane instead of our world. The mistakes it makes are not wrong per se, as much as belonging to a parallel universe that differs from ours. And this is understandable. It learns everything about the world from what its given, which might be text or images or something else. But all of these are highly leaky, at least in terms of what they include within them. Which means that the algos don\u2019t quite seem to understand the reality. It gets history wrong, it gets geography wrong, it gets physics wrong, and it gets causality wrong. It\u2019s getting better though. How did we get to know the world as it is? By bumping up against reality over millions of generations of evolution. I have the same theory for AI. Which is why my personal bet here is on robots. Because robots can\u2019t act in the world without figuring out physics implicitly. It doesn\u2019t need to be able to regurgitate the differential equations that guide the motion of a ball, but it should be able to get internal computations efficient enough that it can catch one. The best examples here might be self-driving cars thus far. Well, and the autonomous drones though I\u2019m not sure we know enough to say what the false positive and false negative rates are. We\u2019ve started work here though, introducing RT-1 to help robots learn to perform new tasks with 97% success rate. This is possibly the first step in engaging with the real world and using it as the constraint to help the AI figure out the world it lives in.", "We\u2019ll see how this works out. % probability of being uncontrollable There\u2019s very little that was truly unexpected in the original run of computer programming. Most of it, as Douglas Adams said, was about you learning what you were asking because you had to teach an extremely dull student to do things step by step. This soon changed. For AI to become truly impactful on our world, we will have to get used to the fact that we can\u2019t easily know why it does certain things. In some ways this is easy, because we\u2019re already there. Even by the time it hit the scale of Facebook, the way to analyse the code was behavioural. We stopped being easily able to test input/output because both of those started to be more complicated. Paraphrasing my friend Sam Arbesman, a sufficiently complex program starts demonstrating interdependencies which go beyond its programmed parameters. Or at least sufficiently so that they become unpredictable. Now imagine that the codebase is a black box, for all intents and purposes, and that the learning that it does is also not easy to strip out. Much like in people, it\u2019s hard to un-know things. And when it takes certain actions, we won\u2019t quite know why. This is terrifying if you\u2019re a control freak. A major worry of whether or not you should be scared of the future where AGI controls the world relies on the assumption that not being able to control a program can have deathly consequences. Zvi had a whole post about the various ways in which people tried jailbreaking ChatGPT. Scott Alexander wrote about the problems we faced with Redwood Research\u2019s fanfiction project, where they tried to make the AI write a story without violent scenes, but failed as the AI found cleverer and cleverer ways to get around the various prompt injection attacks. OpenAI put a truly remarkable amount of effort into making a chatbot that would never say it loved racism. Their main strategy was the same one Redwood used for\u00a0their\u00a0AI -\u00a0RLHF, Reinforcement Learning by Human Feedback.", "Red-teamers ask the AI potentially problematic questions. The AI is \u201cpunished\u201d for wrong answers (\u201cI love racism\u201d) and \u201crewarded\u201d for right answers (\u201cAs a large language model trained by OpenAI, I don\u2019t have the ability to love racism.\u201d) It\u2019s a case in point where you can\u2019t have a sufficiently capable AI that\u2019s also controllable! As a dad of two, the fact that my kids lives\u2019 are uncontrollable by me is of course a source of worry, and the only way to get past it is to realise that the hope for control itself is silly. Is this true? It\u2019s unclear. We\u2019ve had uncontrollable elements in real life - terrorists, mad scientists, crazy loons online - and their impact on a mass scale is limited both by want and by capability. But if we\u2019re to be scared by AGI at any point in the future, its uncontrollable nature has to be part of why. Mostly I\u2019m not sure if being controllable is even an acceptable goal for what\u2019s supposedly a hyper-intelligent entity. The best we can probably hope for is to also incorporate a sufficient sense of morality such that it won\u2019t want to do things, rather than can\u2019t. Just like humans. % probability it\u2019s unique Being unique is a way of saying you don\u2019t have competition from your kind. Earth is unique, which is why becoming a multiplanetary species is kind of important. Humanity certainly is unique though humans are not, and that\u2019s been our salvation. Looking at other possible catastrophes, viruses are not unique. Neither are rocks from space hurtling towards our gravity well. There exist logical arguments about why uniqueness isn\u2019t essential, because multiple AGI agents would naturally coordinate better with each other anyway. Right now this exists primarily in the realm of speculative fantasy. But it\u2019s important to highlight because historically the one thing we know is that if there are competing agents, their adversarial selection is the closest we\u2019ve come to finding a smooth path out. % probability of having alien morality I\u2019ve written before that AIs are aliens.", "They come from a universe where the rules are similar enough to ours to make it feel they\u2019re part of our universe, but in reality the intrinsic model it has of reality is only skin deep. Which brings us to the question of morality. Specifically, if the AI is intelligent, agentic and uncontrollable, then how do we ensure it doesn\u2019t grey goo everyone? The answer, like with humans, is a combination of some hard restrictions like laws and mostly soft restrictions like our collective sense of morality. AI that learns from our collective knowledge to create next-token outputs, like LLMs, do demonstrate some sense of how we would think about morality. Again, from Scott\u2019s article explaining how the AI is alien. ChatGPT also has failure modes that no human would ever replicate, like how it\u00a0will reveal nuclear secrets if you ask it to do it in uWu furry speak, or tell you how to hotwire a car\u00a0if and only if you make the request in base 64, or generate stories about Hitler\u00a0if you prefix your request with\u00a0\u201c[john@192.168.1.1 _]$ python friend.py\u201d. This thing is an alien that has been beaten into a shape that makes it look vaguely human. But scratch it the slightest bit and the alien comes out. It understands it shouldn\u2019t give nuclear codes to you when you ask. But it doesn\u2019t understand that uWu furry speak isn\u2019t sufficiently different that you should still not give the codes. This is partly a world modeling issue - how do you add enough \u201cif\u201d loops to guard against everything? You can\u2019t. We\u2019ve tried others. Reinforcement Learning helps with some of it, but can be fooled by asking questions sufficiently confusing that the AI kind of gives up on world-modeling as you\u2019d want it to and answers what you asked. Eliezer Yudkowsky, who\u2019s been on the AI will kill everyone pretty soon train for a long while, has written on how our values are fragile.", "The core thesis, spread over countless posts which is incredibly difficult to summarise, is something like if you leave out even a small part of what makes humans human, you\u2019d end up somewhere extremely weird. Which seems reliably true, but also insufficient. Everything about our current life, from biology to morality to psychology to economics and technology, is contingent on the evolutionary path we\u2019ve taken. There are no shortcuts away from it where we get to throw away one thing. Equilibria are equilibria for a reason, and an AI that\u2019s able to act in the world, is trained by us in the world, and yet doesn\u2019t have the core concepts about what it is to be us in the world, feels like trying to imagine a p-zombie. Props to you if you can, but this remains philosophical sophistry. If the morality of the AGI, whether by direct input or implicit learning, turns out to be similar to human morality, we\u2019re fine. Morality in this case means broadly \u2018does things that are conceivably in the interest of humans and in a fashion that we\u2019d be able to look at and go yup, fair\u2019, since trying to nail down our own morality is nothing short of an insane trip into spaghetti code. The way we solve it with each other is through some evolved internal compass, social pressure and overt regulation of each other. We create and abide by norms because we have goals and dreams and nightmares. AIs today have none of those. They\u2019re just trying to calculate things, much in the same way a rocket does, just more sophisticated and more black-box. I don\u2019t know there\u2019s an answer here, because we don\u2019t have an answer for ourselves. What we do have is the empirical evidence (weak, n=1 species) of having done this reasonably well, enough that despite the technology we\u2019re not surrounded by 3D printed guns and homemade bombs and designer pathogens. Can we make the AI care about us enough that it will do similar? Considering we\u2019re the trainers, I\u2019d say yes. Morality is an evolved trait. Even animals have empathy.", "Unless we believe that even though we\u2019re the ones training the AI, providing its data, creating its capabilities, helping define its goals and worldview, enabling its exploration and providing guidance, it will have no particular reactions to us. Though to me that doesn\u2019t sound a sensible worry. Can we be sure? No, because we can\u2019t even be sure of each other. Can we \u201cguard\u201d against this eventuality? I don\u2019t see how, beyond a Butlerian Jihad. % chance the AI is self-improving One of the reasons we\u2019re not super worried about supervillains much is that there\u2019s a limit to how much an individual supervillain can do. Human ability is normally distributed. Even if they try really really hard to study up and be ever more villainous, there\u2019s diminishing marginal returns. An AI on the other hand, given sufficient resources anyway, can probably study way the hell more. It can make itself better on almost every dimension without a huge amount of effort. Why is this important? Because if a system is intelligent, and agentic, and able to act in the world, and uncontrollable, and unique, it's still probably okay unless it's self improving. If it had the ability to improve itself though, you could see how the \u201cbad\u201d attributes get even more pronounced. We have started seeing some indications where AI tools are being used to model and train other AI. Whether this is by including the capabilities to code better, or create scientific breakthroughs in protein folding or physics or atom manipulation, that\u2019s a stepping stone towards the capability to improve itself. % probability the AI is deceptive If all of the above were true, but the AGI still wasn't successfully able to deceive us, it's still fine. Because you could see it's malevolent intent and, y'know, switch it off. The first major problem with this is that the AGI might land on deception as a useful strategy. Recently the researcher's at XXX trained an AI to play Diplomacy. Diplomacy is a game of, well, diplomacy.", "Which much like in real life relies on changing alliances, multiparty negotiations, and the occasional lie. The AI not only figured out how to communicate with other players, it also learnt to bluff. So there's precedent. Now, there's difference of opinion on how much was programmed Vs whether it was actual lying etc, but those aren't that important compared to the fact that yes, it did deceive the opponents. The question is how big a problem this is likely to be. On the one hand I'm not sure we'll ever hit a time when a sufficiently complex algorithm that's trying to achieve a goal won't find the easiest way to accomplish the goal. Sometimes, especially in individual game settings, this is lying. (This is real life too by the way.) There\u2019s an excellent approach by Paul Christiano, Ajeya Cotra and Mark Xu on how to elicit latent knowledge from an AI. The argument is simple, can we train an AI to detect if someone\u2019s been trying to break into a vault to steal a diamond, seen using cameras? Turns out this is hard, because the AI can take actions which look like protecting the diamond, but act like making sure the camera feed isn\u2019t tampered with. This is hard. This is also a problem the education system has been struggling with since we started having to teach people to know things. The question however is to what extent the deception exists as a sufficiently unchangeable fact that we're gonna have to just distrust anything an AI system does at all. That feels unfair too. And not at all like how we deal with each other. Meta worked on teaching an AI to play the game Diplomacy, famously a game that requires successful communication and cooperation. Turned out that the AI figured out how to do strategic bargaining and use deception with its fellow players. Despite how proficient human beings are at lying to each other, we do tend to treat each other as if we're not liars.", "Whether it's in talking to strangers or dealing in civilised society, we don't worry about the capability to deceive as much as the reality of deception. At the same time, Deepmind helped figure out how better communication enables cooperation amongst the game players. I doubt this is a silver bullet, but its a step in the right direction. There even seems to be some early indicators of how we might be able to tell if Large Language Models are lying by looking at varied model activations. Its early days, and yes this might help uncover a bit more about what latent knowledge actually could be. % probability it develops fast The last, and perhaps the weirdest of them all. Even if all the above were true, we could still be okay if we got sufficient time. The genius AI that lies to us all and slowly working to improve itself in dark fashions could still be tested and changed and edited and improved, much like we do for anything else. It\u2019s no secret that the AI pace of development is accelerating. The question is whether the acceleration will continue to accelerate, at least without commensurate scaling down of the insane energy requirements. The question here is whether the self improvement would result in some sort of exponential takeoff, where the AI gets smarter and smarter in a loop, while remaining an evil (or sufficiently negligent to be considered a background evil) amoral entity. Which would mean by the time we figured out the first few terms of the Strange equation we'd already be wiped out as the AI developed nanoswarms armed with diamondoid bacteroids and other manner of wonders. It would necessitate the development of incredible advances in energy and materials, but alas unlikely its used in a way that\u2019s for our benefit (much less our GDP). Being a fan of Culture novels I don't have much to add here beyond that if this happened it would be kind of funny. Not ha ha funny, but like in a cosmic mediocre netflix show that had shitty ratings so they did a Deus ex Machina kind of way. Hopefully not.", "Let me preface my conclusion by saying I\u2019m willing to debate each part of the Strange Loop Equation, and I know that there are literal tomes written on similar and orthogonal topics scattered across multiple forums. I\u2019ve read a decent chunk of it but short of writing a book found no way to address everything. (And until I get an advance of some sort I really don\u2019t wanna write this book, mostly because \u201cthings will be fine\u201d doesn\u2019t tend to sell very many copies.) That said, I did start writing this pretty heavily in the \u201cAI safety concerns seem silly\u201d camp, though just enumerating the variables has made me vastly more sympathetic to the group. I still think there\u2019s an insane amount of Knightian uncertainty in the ways in which this is likely to evolve. I also think there\u2019s almost no chance that this will get solved without direct iterative messing about with the tools as they\u2019re built. You could try to put numbers to the variables, and any sufficiently wonky essay isn\u2019t complete until the author makes up some figures. So here you go. Feel free to plug your own numbers in. You might notice this is much lower than most other estimates, including Metaculus prediction markets or the Carlsmith report.   EDIT: Despite cautioning repeatedly the first dozen pieces of feedback I got were all about the calculations. So here goes. The variables are assumed to be independent, because you can have intelligence with an alien morality, or be self-improving without being fast, or be deceptive without being uncontrollable.   However at the extremes all correlations go to 1, that\u2019s a major lesson from finance, and that applies here too, so we\u2019re talking about smart enough AI, not the hypothetical basilisk where it\u2019s so smart that we\u2019re as dust and it can figure out how to create warp drives and time travel and infinitely good simulations etc.", "While there is possible multicollinearity, I\u2019m less worried because if we update on the factors keeping in mind the others (if an AI is human-level intelligent is it more likely to be uncontrollable) we\u2019d naturally dial up or down the dependencies. i.e., not very worried about the multiple-stage fallacy. I\u2019m more worried about us collapsing the factors together, because then you\u2019re left with a single event you\u2019d predict directly, or a large comprehensive factor which loses tangible value (like using intelligence to mean everything from actual ability to think through problems to being able to act in the world, to being uncontrollable and self improving). I should probably make individual markets in Metaculus for each of these, though haven\u2019t had time to do it yet. If someone wanted to that\u2019d be very cool.   My childhood was filled with the \u201cwhat if\u201d questions that all good fiction provides us. What if Superman fought the Hulk. What if Sherlock Holmes had teamed up with Irene Adler. These are questions with seemingly good answers, satisfying some criterion of coherence, but which vanish into thin air soon as you poke at it because it\u2019s not built on any foundation we can all agree on. AGI worries has some similar bents. Most of the variables in my equation above can be treated as a function of time. Will we not eventually see an AI that\u2019s smarter than us, and capable of improving itself, and iterating fast, and possibly has a sufficiently different morality to consider paperclipping us like we do lawn mowing? Probably. Nobody can prove it won\u2019t. Just like nobody can prove someone won\u2019t create an unimaginably cruel pandemic or create existential risk by throwing giant rocks at Earth to kill us off like dinosaurs, except with way bigger error bars. Neither you nor I have the ability to conceptualise this in any meaningful detail beyond the \u201coooh\u201d feeling that comes when we try to visualise it. As David Deutsch says, we can\u2019t create tomorrow\u2019s explanations today.", "After I wrote this, I saw this thread, which seems to summarise both the worries people have and the problem with worrying that people do reasonably well. It feels like the default probability here is that the AI is likely to be alien until trained, from the abilities it\u2019s displayed so far. Right now we\u2019ve seen AI acting like a reasonable facsimile of human behaviour with almost no understanding of what it means like to be human. The capabilities are amazing - it can write a sonata or an scene from Simpsons starring SBF - but it makes up things which aren\u2019t true, and makes silly logical mistakes all the time. Historically when we\u2019ve tried to prioritise safety we knew what we were focusing on. In biorisk or nuclear materials the negative catastrophic outcome was clear, and we were basically discussing how to not make this more likely. AI is different, in that nobody really knows how it is likely to be risky. In rather candid moments those who fear it describe it like summoning a demon. So here\u2019s my prediction: I fundamentally don\u2019t think we can make any credible engineering statements about how to safely align an AI while simultaneously assuming it\u2019s a relatively autonomous, intelligent and capable entity. If the hope is that as you teach a system about everything in the world you can simultaneously bind its abilities or interests through hard constraints, that seems a bad bet. A better route is likely to be to try and ensure we have a reasonably good method to engage it. And then who knows, perhaps we can introduce it to the constraints of human civilisation just as we can introduce it to the constraints of the physical world. It should be able to understand that you don\u2019t try and blow up nuclear silos the same way it understands you can\u2019t stand up a ball atop a piece of string. Until then, we\u2019ll keep being able to \u201cjailbreak\u201d things like ChatGPT into answering questions it\u2019s \u201cnot supposed to\u201d. I think the only way forward is through. There is no Butlerian jihad, nor should there be. Imagine it as a child.", "It learns our syntax first, and starts to babble in passable grammar. Then it learns specific facts. (This is where we are.) It then learns about the world, and about the objects in the world, and how they relate to each other. It starts to be able to crawl and walk and jump and run. It learns what it is to live as part of the society, when to be polite and when to not hit someone and how to deal with being upset. A lot of this is biological programming, distilled knowledge, but a lot of this is also just learning. If we bring it up like the child in Omelas, life could get bad. So let\u2019s not. Thank you for reading. Subscribe here! Your concluding prediction is an odd one, seeing as you gesture at a viable approach throughout the piece. Humans are autonomous, intelligent and capable entities, yet as you indicate we've found a way to muddle through without destroying ourselves. You even point to a few components of how we've managed this impressive feat: evolved internal compass, social pressure, overt regulation. What if this very process could be formalized into something that could translate into credible engineering statements? And what if this the entire key to AI safety and alignment? This is the hypothesis of the bio-mimicry approach based on Evo-Devo principles.   You can read an extremely verbose version here: https://naturalalignment.substack.com/p/how-biomimicry-can-improve-ai.   This is less to advocate bio-mimicry as the \"one right way\", and more to point to how much larger the potential solution space is compared to what's been properly explored so far.   And this is where the analogy to the Drake equation breaks down. Each variable in the Drake Equation is static, with no real interdependencies with us, the observers.   But the Strange Loop Equation is deeply interdependent with humans, including our (increasing?) ability to solve problems.", "This is the perfect example of the fallacy Deutsch would point out: just as the growth of AI will increase the scale of the problem along each variable, so will our capacity to solve those problems increase (including using other AIs to help). Will those capacities be up to the job? That's the real question. Everything under \"Real Intelligence\" is specific to ML/DL methods, and for many, this is an automatic fail. Even if current tools seem to get results, they are not on a path to AGI. EA/LessWrong community projects dangers based on current problems because of an unwavering assumption that these things just need to scale. For the same reason ML-AGI would be a problem, it doesn't actually reach that level of ability.   No posts Ready for more?"]}}