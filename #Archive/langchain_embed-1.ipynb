{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze > requirements.txt\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.document_loaders import DirectoryLoader, UnstructuredHTMLLoader, UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, SpacyTextSplitter\n",
    "from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "def open_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as infile:\n",
    "        return infile.read()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = open_file('openai_api_key.txt')\n",
    "openai.api_key = open_file('openai_api_key.txt')\n",
    "openai_api_key = openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This post is the encapsulation of my thoughts on AGI evolution, which I’ve variously written about eg here and here. Feel free to just check out the equation and skip to the last section if you’d rather cut to the chase and start the debate.There is a persistent worry amongst smart folks about the creation of smart and capable artificial intelligences, or AGI. The worry is that by its very nature it would have a very different set of moral codes (things it should or shouldn’t do) and a vastly increased set of capabilities, which can result in pretty catastrophic outcomes. Like a baby with a bazooka, but worse. Metaculus now predicts weak AGI by 2027 & strong AGI with robotic capabilities by 2038.To put my bias upfront, I’ve not found this to be very persuasive. From an epistemological point of view, “it can be dangerous” seems like insufficient grounds to worry, and “we should make it safer” seems like a non sequitur. Like sure, make it safer, but since nobody knows how it feels like a lot of folks talking back and forth with each other. We wouldn’t have been able to solve the horrors of social media before social media. We couldn’t have solved online safety before the internet usage soared and costs plummeted. Jason Crawford has written about a philosophy of safety and how its part of progress and not separate from it.However as every new instance of AI enabled tech dropped, both camps of accelerationists and doomsdayers say the same things. Imagine if this continues to get powerful at the same rate. Imagine if this technology falls into the wrong hands. Imagine if the AI we created doesn’t do what we tell it to do, through incompetence or uncaringness or educated malice.The core of the worry segment lay in the idea that capabilities do not rise at the same rate as the knowledge to wield the capability wisely. The best examples are this paper from Joe Carlsmith, and this research article from Ajeya Cotra. I’m not gonna try to summarise them, because they’re extremely information dense and assumption laden, but the conclusion includes two components - that as intelligence and ability within the machine increases we lose our ability to control it, and that it might not value the things we value, thus causing inadvertent chaos.This feels very much like creating an anthropology for a species that doesn’t exist yet.While I’ve written about the idea of AI doom as eschatology before, and am very much in the techno-optimist camp about its potential, when multiple smart people are freaking out about the same problem its useful to check if its people being nerd-sniped.The first problem is how to break the problem down. Because ultimately a lot of the ways in which AI makes everyone grey goo, or sets off all nuclear bombs, or creates a pathogen, assume a level of sophistication that basically begs the answer. So I looked at where we’ve grappled with the ineffable before, and tried to somehow break down our uncertainty about the existence of a thing, in this case a malevolent entity that an uncomfortably large percentage of the population are happy to call a demon.We’ve looked at calculating the incalculable before. Most famously, in 1961, the astrophysicist Frank Drake asked how many extra-terrestrial civilisations there could be. To answer this, he created his eponymous equation.You can think of this as a first principles Fermi estimation. Based on your assumptions about the things it takes to create an alien lifeform, the rate of star formation, fraction of those with planets, fraction which can support life and actually develop it, fraction of those with civilisations which will eventually become visible to us. As Frank Drake actually said:As I planned the meeting, I realized a few day[s] ahead of time we needed an agenda. And so I wrote down all the things you needed to know to predict how hard it's going to be to detect extraterrestrial life. And looking at them it became pretty evident that if you multiplied all these together, you got a number, N, which is the number of detectable civilizations in our galaxy.Each one of those fractions are of course highly variable, but the idea is to get a sense of the inputs into the equations and help us think better. Or in the words of the new master.The Drake equation is a specific equation used to estimate the number of intelligent extraterrestrial civilizations in our galaxy. While there may be other equations that are used to estimate the likelihood or probability of certain events, I am not aware of any that are directly comparable to the Drake equation. It is worth noting that the Drake equation is not a scientific equation in the traditional sense, but rather a way to help organize and structure our thinking about the probability of the existence of extraterrestrial life.The Drake equation is a way to distil our thinking about extremely complex scenarios around the emergence of alien life, and in that vein it's been really helpful.Which made me think, shouldn't we have something similar regarding the emergence of artificial alien intelligence too? I looked around and didn't find anything comparable. So, I decided to make an equation for AGI, to help us think about the worry regarding existential risk (x-risk) from it's development.Scary AI = I * A1 * A2 * U1 * U2 * A3 * S * D * FYou have no idea how proud I am of this, and how annoyed I am that it renders this way …(Since publishing I found  created his own version of the equation a couple months ago here. It’s similar enough that I feel confident we’re both not totally wrong, and different enough that you should read both. Jon’s awesome anyway so you ought to read him regardless.)Let’s take a look.Continue reading onlineThanks for reading Strange Loop Canon! Subscribe here!% probability of real IntelligenceHard to define, but perhaps like pornogrpahy easy to recognise when we see it. Despite the Deutschian anguish at behaviourism, or rather the referred Popperian anger, it’s possible to at least see how a system might behave as if it has real intelligence.There are ways to get there from here. When I wrote about the anything to anything machine, which AI is developing, part of the promise needed was to make it seamlessly link with each other, so each model is not sitting in its own island.You need far better, or far more, training data. We actually don’t know how much is needed or at what fidelity, since the only source of real intelligence we’ve seen is the result of a massively parallel optimisation problem over a million generations in a changing landscape. That’s distilled knowledge and we don’t actually know what's really needed to replicate that.The brain, from various types of analysis that’s been done, shows something to the order of 10^23 FLOPs calculation capacity.Maybe we ought to replace IQ with FLOPSWhich … nice. However, in their comparison of how well computers perform on this.Among a small number of computers we compared**4**, FLOPS and TEPS seem to vary proportionally, at a rate of around 1.7 GTEPS/TFLOP. We also estimate that the human brain performs around  0.18 – 6.4 * 1014 TEPS. Thus if the FLOPS:TEPS ratio in brains is similar to that in computers, a brain would perform around 0.9 – 33.7 * 1016 FLOPS. We have not investigated how similar this ratio is likely to be.Now take this with a pile of salt, but the broader point seems that by pure raw computational capacity, we might not be so far away from a world where the “thinking” one does is comparable to the “thinking” the other does. If computation is possible on one physical substrate, surely it’s also possible on another physical substrate.And broader point being that even though intelligence per se isn’t easily reducible to its component parts, the idea of what it takes to be similarly intelligent might be computable.Just the biological anchors argument is insufficient of course. Our biological intelligence isn't just a consequence of the physical computational limit, but also the long distilled knowledge we've gained, the environment which modified the ways we seek out knowledge and created the niches that helped our development, and (maybe most important) the energy requirements.% probability of being agenticWe need whatever system is developed to have its own goals and to act of its own accord. ChatGPT is great, but is entirely reactive. Rightfully so, because it doesn’t really have an inner “self” with its own motivations. Can I say it doesn’t have? Maybe not. Maybe the best way to say is that it doesn’t seem to show one.But our motivations came from hundreds of millions of years of evolution, each generation of which only came to propagate itself if it had a goal it optimised towards, which included at the very least survival, and more recently the ability to gather sufficient electronic goods.AI today has no such motivation. There’s an argument that motivation is internally generated based on whatever goal function you give it, subject to capability, but it’s kind of conjectural. We’ve seen snippets of where the AI does things we wouldn’t expect because its goal needed it to figure out things on its own.Here’s an argument dating back to 2011, and it’s very much not the genesis. We’ve been worried about this for a while, from the p-zombie debate onwards, because conceptualising an incredibly smart entity that doesn’t also have autonomous agency isn’t very easy. I might argue it’s impossible.Is this an example of agentic behaviour? Not yet. Again, will it be capable enough at some point where we tell it “hey how do we figure out how to create warp drives” and it figures out how to create new nanotech on its own. But this would need a hell of a lot more intelligence than what we see today. It needs to go beyond basic pattern matching and get to active traversal across entirely new conceptual spaces. Just what David Deutsch calls our ability to create conjectural leaps and create ever better explanations.% probability of having ability to act in the worldSo it's pretty clear that it doesn't matter how smart or agentic it is without the AI being able to act successfully in the world. Cue images of roadrunner running off the cliff and not realising air doesn’t provide resistance. And this means it needs to have an accurate view of the world in which it operates.A major lack that AI of today has is that it lives in some alternate Everettian multiversal plane instead of our world. The mistakes it makes are not wrong per se, as much as belonging to a parallel universe that differs from ours.And this is understandable. It learns everything about the world from what its given, which might be text or images or something else. But all of these are highly leaky, at least in terms of what they include within them.Which means that the algos don’t quite seem to understand the reality. It gets history wrong, it gets geography wrong, it gets physics wrong, and it gets causality wrong.I want to read this book!It’s getting better though.How did we get to know the world as it is? By bumping up against reality over millions of generations of evolution. I have the same theory for AI. Which is why my personal bet here is on robots. Because robots can’t act in the world without figuring out physics implicitly. It doesn’t need to be able to regurgitate the differential equations that guide the motion of a ball, but it should be able to get internal computations efficient enough that it can catch one.The best examples here might be self-driving cars thus far. Well, and the autonomous drones though I’m not sure we know enough to say what the false positive and false negative rates are.We’ve started work here though, introducing RT-1 to help robots learn to perform new tasks with 97% success rate. This is possibly the first step in engaging with the real world and using it as the constraint to help the AI figure out the world it lives in.Karol Hausman @hausman_kIntroducing RT-1, a robotic model that can execute over 700 instructions in the real world at 97% success rate!\n",
      " \n",
      "Generalizes to new tasks✅\n",
      "Robust to new environments and objects✅\n",
      "Fast inference for real time control✅\n",
      "Can absorb multi-robot data✅\n",
      "Powers SayCan✅\n",
      "🧵👇 5:43 PM ∙ Dec 13, 20222,201Likes510RetweetsWe’ll see how this works out.% probability of being uncontrollableThere’s very little that was truly unexpected in the original run of computer programming. Most of it, as Douglas Adams said, was about you learning what you were asking because you had to teach an extremely dull student to do things step by step.This soon changed.For AI to become truly impactful on our world, we will have to get used to the fact that we can’t easily know why it does certain things. In some ways this is easy, because we’re already there. Even by the time it hit the scale of Facebook, the way to analyse the code was behavioural. We stopped being easily able to test input/output because both of those started to be more complicated.Paraphrasing my friend Sam Arbesman, a sufficiently complex program starts demonstrating interdependencies which go beyond its programmed parameters. Or at least sufficiently so that they become unpredictable.Now imagine that the codebase is a black box, for all intents and purposes, and that the learning that it does is also not easy to strip out. Much like in people, it’s hard to un-know things. And when it takes certain actions, we won’t quite know why.This is terrifying if you’re a control freak. A major worry of whether or not you should be scared of the future where AGI controls the world relies on the assumption that not being able to control a program can have deathly consequences. Zvi had a whole post about the various ways in which people tried jailbreaking ChatGPT.Scott Alexander wrote about the problems we faced with Redwood Research’s fanfiction project, where they tried to make the AI write a story without violent scenes, but failed as the AI found cleverer and cleverer ways to get around the various prompt injection attacks.OpenAI put a truly remarkable amount of effort into making a chatbot that would never say it loved racism. Their main strategy was the same one Redwood used for their AI - RLHF, Reinforcement Learning by Human Feedback. Red-teamers ask the AI potentially problematic questions. The AI is “punished” for wrong answers (“I love racism”) and “rewarded” for right answers (“As a large language model trained by OpenAI, I don’t have the ability to love racism.”)It’s a case in point where you can’t have a sufficiently capable AI that’s also controllable!As a dad of two, the fact that my kids lives’ are uncontrollable by me is of course a source of worry, and the only way to get past it is to realise that the hope for control itself is silly.Is this true? It’s unclear.We’ve had uncontrollable elements in real life - terrorists, mad scientists, crazy loons online - and their impact on a mass scale is limited both by want and by capability. But if we’re to be scared by AGI at any point in the future, its uncontrollable nature has to be part of why.Mostly I’m not sure if being controllable is even an acceptable goal for what’s supposedly a hyper-intelligent entity. The best we can probably hope for is to also incorporate a sufficient sense of morality such that it won’t want to do things, rather than can’t. Just like humans.% probability it’s uniqueBeing unique is a way of saying you don’t have competition from your kind. Earth is unique, which is why becoming a multiplanetary species is kind of important. Humanity certainly is unique though humans are not, and that’s been our salvation.Looking at other possible catastrophes, viruses are not unique. Neither are rocks from space hurtling towards our gravity well.There exist logical arguments about why uniqueness isn’t essential, because multiple AGI agents would naturally coordinate better with each other anyway. Right now this exists primarily in the realm of speculative fantasy. But it’s important to highlight because historically the one thing we know is that if there are competing agents, their adversarial selection is the closest we’ve come to finding a smooth path out.% probability of having alien moralityI’ve written before that AIs are aliens. They come from a universe where the rules are similar enough to ours to make it feel they’re part of our universe, but in reality the intrinsic model it has of reality is only skin deep.Which brings us to the question of morality. Specifically, if the AI is intelligent, agentic and uncontrollable, then how do we ensure it doesn’t grey goo everyone? The answer, like with humans, is a combination of some hard restrictions like laws and mostly soft restrictions like our collective sense of morality.AI that learns from our collective knowledge to create next-token outputs, like LLMs, do demonstrate some sense of how we would think about morality. Again, from Scott’s article explaining how the AI is alien.ChatGPT also has failure modes that no human would ever replicate, like how it will reveal nuclear secrets if you ask it to do it in uWu furry speak, or tell you how to hotwire a car if and only if you make the request in base 64, or generate stories about Hitler if you prefix your request with “[john@192.168.1.1 _]$ python friend.py”. This thing is an alien that has been beaten into a shape that makes it look vaguely human. But scratch it the slightest bit and the alien comes out.It understands it shouldn’t give nuclear codes to you when you ask. But it doesn’t understand that uWu furry speak isn’t sufficiently different that you should still not give the codes. This is partly a world modeling issue - how do you add enough “if” loops to guard against everything? You can’t.We’ve tried others. Reinforcement Learning helps with some of it, but can be fooled by asking questions sufficiently confusing that the AI kind of gives up on world-modeling as you’d want it to and answers what you asked.Eliezer Yudkowsky, who’s been on the AI will kill everyone pretty soon train for a long while, has written on how our values are fragile. The core thesis, spread over countless posts which is incredibly difficult to summarise, is something like if you leave out even a small part of what makes humans human, you’d end up somewhere extremely weird. Which seems reliably true, but also insufficient. Everything about our current life, from biology to morality to psychology to economics and technology, is contingent on the evolutionary path we’ve taken.There are no shortcuts away from it where we get to throw away one thing. Equilibria are equilibria for a reason, and an AI that’s able to act in the world, is trained by us in the world, and yet doesn’t have the core concepts about what it is to be us in the world, feels like trying to imagine a p-zombie. Props to you if you can, but this remains philosophical sophistry.If the morality of the AGI, whether by direct input or implicit learning, turns out to be similar to human morality, we’re fine. Morality in this case means broadly ‘does things that are conceivably in the interest of humans and in a fashion that we’d be able to look at and go yup, fair’, since trying to nail down our own morality is nothing short of an insane trip into spaghetti code.The way we solve it with each other is through some evolved internal compass, social pressure and overt regulation of each other. We create and abide by norms because we have goals and dreams and nightmares. AIs today have none of those. They’re just trying to calculate things, much in the same way a rocket does, just more sophisticated and more black-box.I don’t know there’s an answer here, because we don’t have an answer for ourselves. What we do have is the empirical evidence (weak, n=1 species) of having done this reasonably well, enough that despite the technology we’re not surrounded by 3D printed guns and homemade bombs and designer pathogens.Can we make the AI care about us enough that it will do similar? Considering we’re the trainers, I’d say yes. Morality is an evolved trait. Even animals have empathy. Unless we believe that even though we’re the ones training the AI, providing its data, creating its capabilities, helping define its goals and worldview, enabling its exploration and providing guidance, it will have no particular reactions to us. Though to me that doesn’t sound a sensible worry. Can we be sure? No, because we can’t even be sure of each other. Can we “guard” against this eventuality? I don’t see how, beyond a Butlerian Jihad.% chance the AI is self-improvingOne of the reasons we’re not super worried about supervillains much is that there’s a limit to how much an individual supervillain can do. Human ability is normally distributed. Even if they try really really hard to study up and be ever more villainous, there’s diminishing marginal returns.An AI on the other hand, given sufficient resources anyway, can probably study way the hell more. It can make itself better on almost every dimension without a huge amount of effort.Why is this important? Because if a system is intelligent, and agentic, and able to act in the world, and uncontrollable, and unique, it's still probably okay unless it's self improving. If it had the ability to improve itself though, you could see how the “bad” attributes get even more pronounced.We have started seeing some indications where AI tools are being used to model and train other AI. Whether this is by including the capabilities to code better, or create scientific breakthroughs in protein folding or physics or atom manipulation, that’s a stepping stone towards the capability to improve itself.% probability the AI is deceptiveIf all of the above were true, but the AGI still wasn't successfully able to deceive us, it's still fine. Because you could see it's malevolent intent and, y'know, switch it off.The first major problem with this is that the AGI might land on deception as a useful strategy. Recently the researcher's at XXX trained an AI to play Diplomacy. Diplomacy is a game of, well, diplomacy. Which much like in real life relies on changing alliances, multiparty negotiations, and the occasional lie. The AI not only figured out how to communicate with other players, it also learnt to bluff.So there's precedent.Now, there's difference of opinion on how much was programmed Vs whether it was actual lying etc, but those aren't that important compared to the fact that yes, it did deceive the opponents.The question is how big a problem this is likely to be.On the one hand I'm not sure we'll ever hit a time when a sufficiently complex algorithm that's trying to achieve a goal won't find the easiest way to accomplish the goal. Sometimes, especially in individual game settings, this is lying.(This is real life too by the way.)There’s an excellent approach by Paul Christiano, Ajeya Cotra and Mark Xu on how to elicit latent knowledge from an AI.The argument is simple, can we train an AI to detect if someone’s been trying to break into a vault to steal a diamond, seen using cameras? Turns out this is hard, because the AI can take actions which look like protecting the diamond, but act like making sure the camera feed isn’t tampered with.This is hard. This is also a problem the education system has been struggling with since we started having to teach people to know things.The question however is to what extent the deception exists as a sufficiently unchangeable fact that we're gonna have to just distrust anything an AI system does at all. That feels unfair too. And not at all like how we deal with each other.Meta worked on teaching an AI to play the game Diplomacy, famously a game that requires successful communication and cooperation. Turned out that the AI figured out how to do strategic bargaining and use deception with its fellow players.Despite how proficient human beings are at lying to each other, we do tend to treat each other as if we're not liars. Whether it's in talking to strangers or dealing in civilised society, we don't worry about the capability to deceive as much as the reality of deception.At the same time, Deepmind helped figure out how better communication enables cooperation amongst the game players. I doubt this is a silver bullet, but its a step in the right direction.There even seems to be some early indicators of how we might be able to tell if Large Language Models are lying by looking at varied model activations. Its early days, and yes this might help uncover a bit more about what latent knowledge actually could be.% probability it develops fastThe last, and perhaps the weirdest of them all. Even if all the above were true, we could still be okay if we got sufficient time. The genius AI that lies to us all and slowly working to improve itself in dark fashions could still be tested and changed and edited and improved, much like we do for anything else.It’s no secret that the AI pace of development is accelerating. The question is whether the acceleration will continue to accelerate, at least without commensurate scaling down of the insane energy requirements.The question here is whether the self improvement would result in some sort of exponential takeoff, where the AI gets smarter and smarter in a loop, while remaining an evil (or sufficiently negligent to be considered a background evil) amoral entity.Which would mean by the time we figured out the first few terms of the Strange equation we'd already be wiped out as the AI developed nanoswarms armed with diamondoid bacteroids and other manner of wonders. It would necessitate the development of incredible advances in energy and materials, but alas unlikely its used in a way that’s for our benefit (much less our GDP).Being a fan of Culture novels I don't have much to add here beyond that if this happened it would be kind of funny. Not ha ha funny, but like in a cosmic mediocre netflix show that had shitty ratings so they did a Deus ex Machina kind of way.Hopefully not.Let me preface my conclusion by saying I’m willing to debate each part of the Strange Loop Equation, and I know that there are literal tomes written on similar and orthogonal topics scattered across multiple forums. I’ve read a decent chunk of it but short of writing a book found no way to address everything. (And until I get an advance of some sort I really don’t wanna write this book, mostly because “things will be fine” doesn’t tend to sell very many copies.)That said, I did start writing this pretty heavily in the “AI safety concerns seem silly” camp, though just enumerating the variables has made me vastly more sympathetic to the group. I still think there’s an insane amount of Knightian uncertainty in the ways in which this is likely to evolve. I also think there’s almost no chance that this will get solved without direct iterative messing about with the tools as they’re built.You could try to put numbers to the variables, and any sufficiently wonky essay isn’t complete until the author makes up some figures. So here you go. Feel free to plug your own numbers in.Numbers don’t lie, they said …You might notice this is much lower than most other estimates, including Metaculus prediction markets or the Carlsmith report. EDIT: Despite cautioning repeatedly the first dozen pieces of feedback I got were all about the calculations. So here goes.The variables are assumed to be independent, because you can have intelligence with an alien morality, or be self-improving without being fast, or be deceptive without being uncontrollable. However at the extremes all correlations go to 1, that’s a major lesson from finance, and that applies here too, so we’re talking about smart enough AI, not the hypothetical basilisk where it’s so smart that we’re as dust and it can figure out how to create warp drives and time travel and infinitely good simulations etc.While there is possible multicollinearity, I’m less worried because if we update on the factors keeping in mind the others (if an AI is human-level intelligent is it more likely to be uncontrollable) we’d naturally dial up or down the dependencies. i.e., not very worried about the multiple-stage fallacy.I’m more worried about us collapsing the factors together, because then you’re left with a single event you’d predict directly, or a large comprehensive factor which loses tangible value (like using intelligence to mean everything from actual ability to think through problems to being able to act in the world, to being uncontrollable and self improving).I should probably make individual markets in Metaculus for each of these, though haven’t had time to do it yet. If someone wanted to that’d be very cool.Numbers don’t lie, they said … part 2My childhood was filled with the “what if” questions that all good fiction provides us. What if Superman fought the Hulk. What if Sherlock Holmes had teamed up with Irene Adler. These are questions with seemingly good answers, satisfying some criterion of coherence, but which vanish into thin air soon as you poke at it because it’s not built on any foundation we can all agree on.AGI worries has some similar bents. Most of the variables in my equation above can be treated as a function of time. Will we not eventually see an AI that’s smarter than us, and capable of improving itself, and iterating fast, and possibly has a sufficiently different morality to consider paperclipping us like we do lawn mowing?Probably. Nobody can prove it won’t. Just like nobody can prove someone won’t create an unimaginably cruel pandemic or create existential risk by throwing giant rocks at Earth to kill us off like dinosaurs, except with way bigger error bars.Neither you nor I have the ability to conceptualise this in any meaningful detail beyond the “oooh” feeling that comes when we try to visualise it. As David Deutsch says, we can’t create tomorrow’s explanations today.After I wrote this, I saw this thread, which seems to summarise both the worries people have and the problem with worrying that people do reasonably well.Perry E. Metzger @perrymetzger🧵To my AI safety obsessed friends (and make no mistake, I'm **very** sympathetic to you): there was never any possibility of controlling the future. That was always a fantasy. (It's nearly, and I'm not trying to insult anyone, an adolescent \"if I ran the world\" type fantasy.)1:55 PM ∙ Dec 6, 2022128Likes20RetweetsIt feels like the default probability here is that the AI is likely to be alien until trained, from the abilities it’s displayed so far. Right now we’ve seen AI acting like a reasonable facsimile of human behaviour with almost no understanding of what it means like to be human. The capabilities are amazing - it can write a sonata or an scene from Simpsons starring SBF - but it makes up things which aren’t true, and makes silly logical mistakes all the time.Historically when we’ve tried to prioritise safety we knew what we were focusing on. In biorisk or nuclear materials the negative catastrophic outcome was clear, and we were basically discussing how to not make this more likely. AI is different, in that nobody really knows how it is likely to be risky. In rather candid moments those who fear it describe it like summoning a demon.So here’s my prediction:I fundamentally don’t think we can make any credible engineering statements about how to safely align an AI while simultaneously assuming it’s a relatively autonomous, intelligent and capable entity.If the hope is that as you teach a system about everything in the world you can simultaneously bind its abilities or interests through hard constraints, that seems a bad bet.A better route is likely to be to try and ensure we have a reasonably good method to engage it. And then who knows, perhaps we can introduce it to the constraints of human civilisation just as we can introduce it to the constraints of the physical world. It should be able to understand that you don’t try and blow up nuclear silos the same way it understands you can’t stand up a ball atop a piece of string.Until then, we’ll keep being able to “jailbreak” things like ChatGPT into answering questions it’s “not supposed to”. I think the only way forward is through. There is no Butlerian jihad, nor should there be.Imagine it as a child. It learns our syntax first, and starts to babble in passable grammar. Then it learns specific facts. (This is where we are.) It then learns about the world, and about the objects in the world, and how they relate to each other. It starts to be able to crawl and walk and jump and run. It learns what it is to live as part of the society, when to be polite and when to not hit someone and how to deal with being upset. A lot of this is biological programming, distilled knowledge, but a lot of this is also just learning.If we bring it up like the child in Omelas, life could get bad. So let’s not.Thank you for reading. Subscribe here!\n",
      "I. The battle of titansThe news is filled with people talking about Google’s demise. That they lost $100 Billion of market value when Microsoft announced that Bing, its search engine that makes $10 Billion of revenue and has 8% market share, will incorporate the magic of OpenAI and ChatGPT.Let’s leave aside for a moment that a drop in market capitalisation for a publicly traded stock on the basis of a press conference is a) a thing that happens depressingly often, and b) not something that helps in causal understanding all that much.But the question I wanted to test was, how much of the hype is real? And so, first, I started with: what does Google say when you search for Bing?Well, turns out it does quite well. There is the link, of course, and a bunch of news stories about Bing.But the interesting bit is to the right, where there is a snippet of information extracted about Bing, directly from Wikipedia, with the relevant info just right there. This is what Google calls its “featured snippets”.So Google does a remarkably good job of finding news pieces that are relevant to the questions and direct answers from sources where it helps. Where it falls foul is that by its own policies (below) it’s hamstrung in how it can use the information from the websites it crawls to summarise and use them. It’s the same fight that we’re seeing in Getty Vs Stability AI, just playing out on a much larger canvas and with the most profitable business ever hanging in the balance.But I wanted to see if we did change the methodology, how good are the results actually! To test, I chose the top queries that Bing itself highlighted in the Bing site, which presumably are the best of what the new Bing can do. And I searched each one on Google too and looked at their top results. There are way too many screenshots for each one, so here’s the summary table. (You can play along at home if you’d like, or ask me and I’ll send it to you.)I classified the questions in somewhat arbitrary fashion, to try and determine what each query was actually trying to get at, and how good Google was at it.The first thought is that the “Creative” questions are the ones Google does worst at. Which makes sense, since those are questions to get a particular output, rather than follow a particular process or gather information about a particular input.The second thought is that while a large chunk of Google queries, especially done over Voice, already provide you with a snippet of an answer, none of these queries did so. While this makes sense (if you could get the info easily from a website, why would Bing prominently display them), it’s an interesting thing to keep in mind.But, out of the 24 queries from Bing (12 with the subject and 12 as detailed personalised questions, the way they split it)at least 15 had the “people also ask” section, with specific questions and answers associated with it7 had video links in the answersI have difficulty looking at this and thinking Google is dead.The simplest reason being they could just literally take the first three search result web pages, summarise it through whatever LLM, and already do better than Bing. And I do mean literally, look at the results that come when you search for things, and you can summarise the answers. In fact, they’re already doing it!The interesting thing here is that the data here is strictly “okay”. While the chat response feels like a complete response, it is not. It requires external consultation based on individual circumstances. It is fairly generic. And it’s mostly a regurgitation in summarised form of what the video says on the left.Or to rephrase, one way to think of Google searches is that they’re a combination of:What queries: Need to find informationHow queries: Figure out how to do somethingWhy queries: Answer causal questionsDo queries: To perform an action (eg comparison of products)For the What and How, Google does admirably well. How is harder, it depends on the question, but Google can summarise its results, if its legal department says yes, and do well. Where it lacks is the “Do” queries, especially if the Do queries are net new (”write a childrens book with a dinosaur and the earth’s core”).What percentage is that likely to be of all queries? I don’t know. What percentage of all search will shift because of the existence or ability to do this? I also don’t know. I suspect not that much!Which brings us to the more existential question, of what it is about the underlying tech that’s causing the furore.II. What exactly are LLMs? Well, they are fuzzy processorsFrom playing with it for quite a while, I have an overarching theory of LLMs. Since the dawn of computing, we’ve been dealing with them as dumber versions of ourselves. As Douglas Adams said.By the time you've sorted out a complicated idea. into little steps that even a stupid machine can deal with, you've certainly learned something about it yourself. The teacher usually learns more than the pupil.This, however, has limits. When the complicated idea gets large enough, when you make it with the help of a large number of people, when parts of it interact in unforeseen and unforeseeable ways with other parts, then you’re stuck learning not that much with additional atomisation.This is why large codebases in companies often behave as if it’s got a life of its own. Problems proliferate where it shouldn’t, and troubleshooting is a bit like Dr House diagnosing Lupus.So even when the input is highly deterministic, the output can occasionally be chaotic and somewhat unpredictable.We’re having this debate between ourselves of how to think about what ChatGPT actually is. It’s:a captured goda precursor to intelligent lifea stochastic parrota liara plagiaristfancy copy and pastea p-zombieThey’re all somewhat true, though ChatGPT itself denies it.What is a better analogy, I think, is that they are fuzzy processors.Like the opposite of Douglas Adams’ infinite improbability drive, which creates a field of improbability, it’s an infinite probability drive, striving towards finding the most probable answers and autocompleting its output even word by word.What sort of a fuzzy processor is it? Well, Wolfram has one of the best explanations of what goes on inside it that I’ve seen. The core is that in analysing and understanding the statistical nature of our language, it has, in a rather Wittgensteinian way, figured out the “rules” of that language game. And since the game is set in our real, human, life it has understood aspects of how we work. And the rules tell it what text is supposed to come after a particular word.Fuzzy processors are different in the sense that they are not deterministic. The answers you get to prompts can be perfect encapsulations, outright lies, summaries with 20% missing, or just outright hallucinations.Which also makes searching for things with it harder depending on the precision you need in your answers.Just like searching was a different mode of finding information than libraries before, with its clear tree-structure of information put in very clear boxes, chat is a different process altogether. Fuzzy processors require a lot more effort from you, the user.This, however, is possibly just fine. Whenever I write a piece of code I have to spend roughly 3x as long Googling and 4x as long troubleshooting. That’s also an issue of the output not matching what I want from an output.But … but the first fuzzy processor makes different mistakes than what we’re used to. It makes, dare I say, more human mistakes. Mistakes of imagination, mistakes of belief, mistakes of understanding.To use it is to learn a new language.All our existing programming languages are about ever-more precisely defining what we want the computer to do, and even when they make programming “easier” it does so by hiding away the complexities behind some abstraction. They do so by using standardised libraries or standard frameworks or common functions or one click API to deploy infrastructure. These are ways we constrain our efforts to only looking at the highest-level of what we need to do. Like all standardisation efforts, it helps us build on top of what existed before.LLMs are different. All efforts are built out of the primordial soup that is its latent space.There is no standardisation or consistent quality control, not really. There are some conventions on prompt engineering, but there is absolutely no way you’ll reach the same conclusions even if you followed the same process.You’ll be close! And for many things, that’s sufficient.But a fuzzy processor, that’s different. When you can’t trust the output that comes from your input, you have to be far more careful on how you use it.You can only really use it for things where you don’t know what the output would’ve been beforehand. You can work with it to guide the output towards what you would like it to be, but its closer to sculpting than just searching.III. The final frontierI wrote a while ago about how the final frontier for AI was personality, and this is what Bing seems to have nailed, while ChatGPT has not. This brings the question of how we use the fuzzy computers. Which means, looked at en masse, it feels like the advantages Google has is still true, their ecosystem of browser + extensions + mail is very strong and not easily dislodged. Bing’s search results were always okay, and still is okay, but now it has a new power. How could it win?A different interface will winIf asking for things via one on one chat is more useful than a text box with a list of answers, then there is an existential risk for Google. This is a UX question.Is the white text box over? No. They’re both white text boxes. The relevant delta is if you like having a conversation to get to an answer, or not!A different set of questions will become more prominentThe types of questions that Bing does better at answering are the Creative ones. For those Google can only offer a route to the work someone’s already done. For instance, here’s the Google response vs Bing response on the workout question.The problem with the result on the bottom is that it’s often incomplete, and incomplete in ways that aren’t easily understandable. The problem with the result on the top is that this too is incomplete in ways that aren’t easily understandable. On the top, you circumvent by finding reliable sources that you can believe, I’m not sure how to solve it on the bottom yet.Neither of those feel particularly likely.In fact, the case of Google Vs Bing is fascinating, because it tries to talk about “Search” as one thing. It isn’t. Search is many things. It’s about getting you to the right endpoint quickly. For code, it’s code. For factual questions, it’s an answer. For curiosity driven explorations, it’s the next node you need to get linked to.Chat is a great way to develop your ideas, like thinking through while talking to someone on the phone. And that’s why it feels like it has clear, overlapping, ways of engaging with how we do search.The summary then:ChatGPT is way better at creative tasksIt is somewhat better at specific troubleshooting, with particular questions which require in-context iterationBut it’s absolutely brilliant as an assistant with sassWhat this tells me is that CGPT is not a competitor to search at all, but rather it’s likely to substantially increase the surface area of what “search” traditionally meant. It’s a throwback to having a sassy associate to search things for you, rather than doing it yourself, like Hollis Robbins had written about.It’s like everyone has their own Jeeves, Pepper Potts, Joan Holloway or Passepartout - a perfect assistant who’s also a foil to do the things you don’t want to do, with sassy banter, who is remarkably smart and articulate but content to book your hotels, who’s sometimes even your conscience. They might occasionally get things wrong but that doesn’t matter as much, since you respect the capabilities even if its results aren’t always right.I currently have a chat window open sometimes, but with personality that might well become all the time. After a very long evolution and many blind alleys, it feels we’re about to get the daemons that Philip Pullman had written about. And that is remarkable!UPDATE: To test my theory that Google could trivially build this, I made a bot that converted the queries into Google searches, scraped the first 3 results, and summarised the answers. It does really rather well. If you extend the tokens, prettify the results, and rewrite it, this gets to be comparable to ChatGPT instantly. There’s a threat to ad model, sure, but there’s no way Google’s on the back foot!If you liked this, you might also like:Generative AI, the Anything from Anything MachineDistilled knowledge is the key to progressAll AI learning is tacit learning\n",
      "The world is what it isThere is only one underlying realityTruth about the state of the world exists, defined as an accurate description of its current state, and we observe it through direct observation and indirect reasoningNothing about the world is entirely uniform, either in form or in distribution, and therefore things present in clusters Even a minor deviation from perfect uniformness of a distribution results in clustering since no distribution is in a perfectly stable equilibrium to external stresses, and so it doesEvery idea of a “thing” involves identification of a collection of properties, the boundaries of which are necessarily fuzzyThe necessity of the fuzziness comes from the requirement for the possibility of a change to the definition of said “thing”The definitions are artificial for ease of communication and are therefore flexible - i.e., it's epistemic uncertaintyA particular arrangement of clusters is not, in any meaningful sense, superior to another - whether it's seen entirely holistically or looked at as individual objectsAggregation of these clusters together creates the world we see and experience at higher levels, linked together through causal interlinkages amongst all phenomena (and across multiple aggregation levels), which can be called causal-webs These causal-webs have a number of ways of interacting with each other, as can be seen by looking at the number of possible factorial combinations, increasing its inherent complexityThe number of relationships that exist between variables is going to be varied amongst direct relationships and indirectThe number of indirect connections will naturally tend to be much higher than direct, and this only tends to make the propagation of a force through the causal-web harder to backtrack to its initial conditionsThe world is a latticework of facts and causal relationships that stack atop each other to create a complex architecture, in which extracting an individual fact or relationship in isolation is often practically impossible due to the sheer number of interrelationships Statistical and probabilistic reasoning exists as a way therefore to learn more about the world, as a way to glean insight into truth at different levels of abstractionProbabilities are our measure of the various uncertainties which reflect our lack of knowledge, and not necessarily a property of realityProbabilities therefore are shortcuts to help understand other multiple layers of the world through a simplification process on large sections of the causal-webComponent interactions aggregate together and sometimes form discernible large scale distribution patterns, which help with predictability of the overall systemSystems contained within the causal-web emerge from underlying interaction of simple components Depending on the number and architecture of the components, the systems become more and more complexComplexity of a system is an emergent property that manifests itself in multiple ways, with one specific one being that it makes it difficult to predict outcomes purely from component level analysesAs the number of the interactions increases, as a function of both number of components and it's degree of interaction, the complexity of the system increasesAs there is within the system multiple feedback loops, both positive and negative, this increases the complexity of the overall systemThere's a cost to complexity within smaller parts of the causal-webs, within systems, both in terms of its overall function and also the increasing number of individual components Systems have feedback loops within that usually guide actions, reactions and self correcting or enhancing behaviourThe loops can be self correcting usually through individual aspects of the complex system interacting with each other; they can also be mutually enhancing through these same interactionsAs aspects of a complex system gets more intertwined so does the potential surface of exposure that shows ways things could evolve and number of potential outcomes could increaseAn external pressure, applied over a period of time, and varying in details enough to self correct, provides an adequate exploration of the potential surface space, that results in having a system that doesn't create unwanted outcomesWe navigate casual-webs through maps. Maps are not the territory, but they are all we have - any representation of a thing is necessarily less complex than the thing it represents Individuals who aim to discover aspects of the world are guided by maps, self-created and provided by others, to illuminate individual pathways within the larger complex maze of facts and relationships The only guidance that an individual has corresponds to a predictable hypothesis, or a sufficiently believable explanation for a phenomenonWhen sufficiently underscored by detail, this phenomenon of analysis and search is science, and the potential inherent in this phenomenon for overall error correction and continuous improvement through better prediction and feedback makes it a strong candidate for a way of thinking that has high potential for eventual discovery of the worldThe search for predictable hypotheses corresponds in size and complexity to the world itself, over long enough timespans, but by itself could also lead to creation of models that have varying error bars in terms of comprehensivenessOur ability to create models of the world around us is slowly built up and improves over time Causality is impossible to measure objectively, in the sense that you're never removed from the process of observing a causal interactionCorrelations can be spurious, especially as the complexity in a system grows through exponential increase in component parts and number of interactions Identifying and removing spurious correlations requires multiple experimental workarounds, designed to identify and remove the other potential causal factorsDue to the level of interdependence and component interaction in most systems this becomes exponentially difficult as complexity increasesModels, affected therefore by our inherent humanity, therefore are built with biases that reflect our preconceptions, ideas or worldviews, visible or notThis scientific reasoning, built upon hypothesisation, evidence gathering and experimentation, allows us to build models about the world that are factual and experimentally verified However it doesn't protect from false information, or from ensuring that we have taken all necessary variables that affect the phenomena under questionThe only cure to this we have is to run multiple experiments, each as independent as possible, in the hope that together they cancel out any inherent biasesThe models have varying degree of validity, due to correspondence with the underlying reality, and ability to create accurate predictionsWe can only ever know a few of the ways in which the variables within the world intersect with each other before the complexity grows so large as to make complete analytic calculations or estimations impossible This implies that a certain degree of error bars is an inevitable feature of a complex world and cannot be undoneAs such, any attempt at complete comprehensiveness is, philosophically speaking, a fool's errandThe entropy of information content regarding an object, a collection thereof or a phenomenon, suggests groupings that are fluid amongst fundamental constituent componentsThe only perfect thinking that exists is mathematical logic, which is tautological. For all others, reasoning remains imperfect as a mechanism to get to the truth Inherent biases exist within our ways of thinking, often crafted through evolutionary pressures and fundamental properties that affect information inputLack of information and/or false information can, knowingly or unknowingly, affect conclusionsAll knowledge exists as probabilities which shift between 1 and 0 according to evidence, at the lowest levels of reality Absolutist stands therefore are always incorrect, and situations need to be assessed on their own merits, taking into consideration the unique circumstancesThis includes the proposition that absence of evidence is not the same as evidence of absenceFor any non-analytic process therefore, the difficulty of measurement, and consequent prediction, creates a degree of error All measurement choices have associated errors, and consequently, thinking of measurement results as sacrosanct is incorrectAs you move from measurement to prediction the uncertainty bars widen significantly, and in most cases widen enough to become unhelpfulSince most types of decisions do not occur frequently enough to be truly applicable to statistical reasoning (at least not to those making the decision), this substantially increases the application of rhetoric and to downplay any statistical significanceWhich could mean that when the outcomes are not statistically predictable with high enough frequency, casting aspersion on the very method is also a reliable method of convincing othersStatistical reasoning emerges as a method of performing estimations in a world that's less amenable to complete analytic theories, either because the causal-webs involved are too complex and entangled to be decipherable, or because the underlying phenomenon itself has inherent properties that make causal arguments inapplicableComplex systems, organic or otherwise, have an exponentially large interaction surface area where the effects may be seen or affected, across the network of interaction points amongst its individual factors Large changes in degree of scale within a system is equivalent to change in degrees of scope, indicating a change in the system Any particular component therefore that changes and morphs over time leads eventually towards the end of a spectrum, and to a fundamentally different constructHowever part of it is also because phase shifts happen within particular phenomena, especially ones with feedback loops embedded insideThe complexity is built up of multiple layers of abstraction, each affecting each other through those feedback loops, and our knowledge that gets built through concepts and models are thin slices Imagine a piece of rockNow imagine you have a laser cutter that's so fine that you can slice multiple sheets of that rockThe rock is old and has multiple generations of strata within itEvery slice therefore reveals certain truths about the rocks origin One shows how it was involved in a volcanic eruptionAnother about how part of it is soil nourished by an ancient sequoiaAnd another about how the pressures varying in its surroundings created different minerals. And so on and onEach facet is true. Each facet is accurate. Yet they are each incomplete.This is our state when trying to understand any multidimensional interlinked objectEach assessment or analysis is like a thin slice of the object, revealing relationships and truths that are insufficiently comprehensiveAs these networked systems increase in size, hierarchies emerge within the overall system There are multiple types of networks that can evolve based on underlying phenomena and their interactionsCouple of the key factors affecting a network's performance are the types and strength of connections amongst it's nodes and edges, distributed in a particular spaceAs an example, for a hub and spoke model, degrees of connections are highly centralised, whereas for small world ones where the connectivity is more distributed, they are more resilient to network breakdown in any particular partFor hierarchical systems where there are positive feedback loops to reinforce existing behaviours, the hierarchies are likely to ossify over timeThe ossification brought about by the feedback loops will make it resilient to known factors and brittle to unknown external factors Negative feedback systems create a way for the system to create stability, and therefore builds resiliencePositive feedback systems however push a system's growth to its natural limits, and tend to create systemic fragility, at least insofar as it interacts with specific macro conditionsParsimony is a global constraining factor within any system that creates explanatory power, and is simultaneously also a necessary result of increasingly complexity within systems Explanations necessarily have to summarise and elucidate with a smaller footprint than the phenomena - otherwise the map is the same as the territoryThe complexity factor therefore becomes a limiting factor as explanations go up hierarchical levelse.g., Within a Darwinian system if the amount of energy that can be consumed by an entity is constrained in some way, then there will be a push towards minimizing the energy required for mere continuation while maximizing the energy required for reproductionPart of the issue is due to the imprecision of language which tends to group concepts together that would then reduce the mental model complexity required to converse at a higher levelUnderstanding any phenomenon fully requires understanding of its antecedents and potential causal factors, in essence its whole causal webThe model is essential to test both a fuller understanding of the simplified causal-web, and ideally therefore allow prediction of future movements based on specific input criteria Not all factors are assessed equally; some have much larger weight and impactPrediction is primarily dependent on specifying what’s being predicted and the complexity of the analytic processes required to be computed; this is also difficult when it comes to highly complex phenomena which have vastly variant underlying distributions (e.g., economic markets) without also specifying clearly the timeframe and the input characteristicsThis also means that for almost all phenomena there is a declining curve in terms of prediction accuracy, the steepness of which depends on a) the complexity of the causal-web of the event being predicted, and b) changes in the underlying variables (including time)Fully untangling and describing this complex web of causation is, in most cases, computationally impossible This stands true except perhaps highly academic or artificially limited casesMost forms of experimentation do fall into this category, where we artificially limit contexts to thereby limit the size of the causal-webTherefore we can only make best efforts to get a statistical answer to whether the causes identified have a large enough impact, causally, on the phenomenonThis means that for every phenomenon there exists a chance that we have not fully incorporated all relevant information and/or data into the model, though the chances might become infinitesimal as the phenomenon continues to be better understoodMental models, necessarily, decrease the complexity of the event and focus only on the most salient features, with salience determined both by predictive ability regarding the phenomenon and relationship with other existing modelsYou can further disentangle this causal-web through exceptionally careful experimentation, but the efficacy of such efforts decrease exponentially as you reach higher levels of convictionThere is limited visible difference between systems which are so complex that it's difficult to see parts that affect others, and systems where the number of actual actions that can be taken are constrained, though the actual mechanics are still incredibly complicatedIt's not possible to fully understand the nuances of a phenomenon as the number of permutations, complications and interrelationships are extremely high This implies that due to practical considerations there is a necessary level of detail that people can grasp before they are able to make summarising statements, which varies from subject to subjectThe particular boundary that decides whether the depth to which you have analysed a topic, or the number of hierarchies that have been dug deeper into the topic, can be analysed by trying to create predictive analysis based on the summarised version that you understand at any point, and testing that against realityEmergent complexity is also, in fact, an anti fractal view of the system, where as you zoom in into deeper components, the self similarities break down rather fast, and in fact it becomes operationally impossible to predict the larger behaviours from the sampler componentsOptimisation adjustments within a system in response to its environment creates self similarity wish creates fractal properties, and fractal properties create increased dimensionality that increases efficiencyApplied in the case of an organism, the evolutionary pressures, along with the environment, creates selection pressures that act upon it (across multiple hierarchies) to increase its odds of survival This implies that creating certain internal models can increase the survivability; and since most models are incomplete, the individual survivability of one model doesn't necessarily alter its overall viabilityThe pressures will shape both its external appearance, behaviours, and ability to create sophisticated internal modelsThe success of a collection of the above characteristics can be judged by its ability to survive long enough to reproduceHowever this also means that separate characteristics could be considered equally successful if they succeed equally in increasing the survivability of the organism until reproduction (if not necessarily afterwards)Real life phenomena often require algorithms to assess its own condition in order to create guiding principles and respond to stimuli and circumstances, which it does through creating an internal simulation, which then helps the algorithm survive in the futureWith complex systems, as long as there are both positive and negative feedback loops, it's possible that the systemic entropy will be a function of those aspects, which indicate an eventual lifespan for most systems, lifespan being determined by the timing with which the entropy progresses to such an extent that the organism ceases input/ output functioningThis shows the inherent drive to have a need for an active substrate to create a dynamic system Otherwise the whole apparatus will be at rest with no factors propelling it forward to changing it in any wayAlso, a substrate cannot be too dynamic, otherwise it'll overwhelm the system altogetherIt's the small perturbations, which might on average cancel each other out, that gives rise to larger phenomena that it seedsThe changes that are wrought therefore helps create the map; and the map helps guide the organism, and that's what creates a reinforcing loopWithin us, this process helps create an individual's belief structure, an output built on top of an entire edifice of underlying beliefs, lived history, facts, stories and hypothesesEditing a belief requires method of comparing existing belief structures with a proposed one, through some form of communication to exchange information about the structures Every statement for example that's made in this effort has to be regarded in terms of the new facts it brings to play to illuminate an observation and the inherent logic by which it relates to other existing facts and observationsThis means, for instance, that social interaction has to be separated from problem solving interaction in order to achieve the best resultsEditing a belief therefore is equivalent to restructuring an entire thought-edifice This means that when people differ, one way to find a middle ground is to try and find out the following (non exhaustive) things How much of each statement is factualHow much of each position is relevant to solving the problem at handAny appeal to authority has to be backed by some evidence of the reliability of the authorityThere are problems inherent in the comparison which cannot be seen or understood through first hand experience, and therefore requires specific expertise This creates problems for belief, because it shifts the burden of belief from first hand to second hand experienceThis also requires understanding of probabilistic systems of belief with the concomitant risk of being wrong vs deterministic systems of beliefTo try and solve this through reliance on experts creates its own issues as we now have the problem of reliance on experts However this adds the difficulty of collectively or individually judging expertise to the problemCollective mechanisms have the ability to compare track records or prior performance to gaugeMost people have an internal mental structure that determines their individual beliefs Education is a mechanism to cultivate and shape this structureConversations and debates are ways to edit or prune branches of this tree in order to understand a topic, including the implicationsLarge part of all arguments stem from misidentification of things. Therefore precise definitions are necessary if a conversation is to progress anywhere.However the precision of a definition is rarely feasible to an accurate enough degree since most clusters aren't tightly enough clusteredOur perception of the world around us, as reflected in our individual mental-maps, incorporate sufficient information and feedback from the world that any current zeitgeist is also part of the map The zeitgeist being part of the mental map doesn’t necessarily cause it to be positively or negatively regarded, just as part of the existing infrastructure with the inertia that this impliesThe world is not seen as a separate objective reality, but the border between our perception of the world, the societal perception of the world, and the objective reality of the world, all commingled together through our perception systemsThe mental-map is an internal representation therefore that includes a point of view on the world, and also meta-responses to the world, including self-referential feedback loops that explore own and third-party societal reactions to the world, and own  beliefs regarding the world (i.e., the mental map itself)There are multiple such loops including loops of loops, though practically speaking there doesn't seem to be an infinite number of them - while it might seem like it's turtles all the way down, it only seems that way and isn't actually that wayBelief therefore requires both presentation of evidence plus ability to integrate with existing mental structure, which results in assessing not just the evidence but also the evidence alongside its congruence with the mental-map as it exists, i.e., believabilityTo do this en masse would also require a relatively systematic way of imparting information about useful parts of the webThis is education, which both helps illuminate parts of the causal-web for all, and also helps provide an illuminated path to explore it furtherTherefore most of education is an effort to instil a sufficiently accurate set of facts and principles which can form the basis of a shared foundation amongst most of society's mental-mapsUnderstanding of the world is crafted through the multiple layers of simultaneous inputs, each of which gives rise to a partial view that together makes up an internal representation of the outside world, i.e., an internal simulation we call a mental-map From early on in life the creation of this mental-map comes about through the slow exploration of the world around and the consequent sequential creation of maps which are interlinked. An example from what a baby goes through: Starts with blurred vision, along with tactile and auditory input, mainly as a survival tool towards feeling closeness and hungerThen moving one's limbs, haphazardly then purposefully, to action small impetus' linking the vision and tactile systems togetherThen control over the body, rolling over, sitting up, crawling, for spatial mobilityTalking, babbling, responding to faces and emotions, getting closeness through human connectionThere is a continual demonstration of intentionality as all of the abilities continue to grow, linking the abilities together to enable ask, get, and respond to various things and peopleThis mental-map forms the basis of interaction with the outside world, and our prediction of potential interactions within the model is taken to be the yardstick to assume predictive ability in the world The constraints in the map revolve around the number of potential options available for action based on the input + output + analysisThese available courses of action are also dictated by deeper internal drives, which are usually more primal needs, such as hunger, curiosity, boredom and so on, each of which create specific courses of action that can be takenTruth, as it's understood, comes via connecting multiple points of this experiential system where a story, or a narrative, touches upon an underlying phenomenon and creates a level of congruence with the outside world as perceivedThe mental maps created don't just comprise the totality of information relationships, but are also path dependent and therefore also contains variations due to the history of the acquisition of each node and edgeMental map networks are malleable in all parts, meaning they are affected based on changes in the quantum, size and sequence of any inputs However the creation of multiple interrelationships within it means that the networks are highly resilient to most external pressures in the form of information inputEditing a mental map network therefore requires a precise calibration of information dissemination that both controls the content and the delivery method in order to affect all reinforcement structures within the mental map simultaneouslyRepetition of information helps to create malleability within an existing mental map, to force information through the networkThe errors that creep through in our creation of internal mental map representations can have pernicious effects that aren't immediately visible These 'cognitive' effects are impacted by inherent biases and shortcuts used in reasoningUnderstanding these mental maps require an effort to understand the intricacies of our internal representational hardwareMost of the time people are unaware of the exact representation and inherent processes underlying the phenomenon when it comes to any individual aspect within the mental mapMental maps distinguish the world into categories and definitions (which are themselves fluid and flexible) The world is better explained through differentiation amongst its constituent items as analogues in biology, where all living things are related to each other through a tree or interrelationships, rather than a dictionary definition whereby we can define the boundaries of an object, a concept or a theory perfectly For example, this means that while the difference between an American football game and a rugby game is obvious to some observers, there are a substantial number of ways where they're similar, and they're still highly context dependant and path dependantDefining any differences therefore becomes a norm built up through years of automated pattern recognition, and relies on large quantities of common references and base contextWhile not necessarily entirely path dependent, to an external observer there is a substantial amount of knowledge that isn't codified and therefore not easily transmissible to anotherPerception also happens across categories simultaneously, through the same neural network albeit through different (and differentiated) pathways, each individually focused on specific levels of aggregation and finding or looking up meaning, to create a coherent picture together This explains, as an example, why we read sentence by sentence or phrase by phrase, rather than letter by letter, though we're perfectly capable of doing so if willedThe maximum level of aggregation we're capable of incorporating is limited by a) our capacity to take in certain quantum of information, and b) our capacity to analyse that informationAnalysis of information means comparing it, and components of it, to other, more established, prior pieces to understand similarity, doing logical operations on ensuing meaningThese same categories then underpin further iterative reasoning about the world, creating strengthened feedback loops for the underlying structureThe act of continued survival ensures that existing mental-map structures strengthen over time - though whether this is a strong foundation or pure ossification within each individual case is tough to determine a prioriThe difficulty of explaining a mental-map to another is due to the lack of communication and comprehension methods adequate to the task, thus ensuring narrative forms are the best we have as map modifier modalitiesAnd as a corollary, as methods of documentation and communication increases, the number of possible versions of ourselves that are documented increases The outsourcing of thought and memory only serves to reinforce the information and also the narrative underpinning itThe creation of searchable databases still requires us to have metadata around the data that needs to be searched, however this doesn’t change the requirements to remember, recollect and create synthesesFacts and beliefs are similar in their identity of being comprised of a group of sub-units which alter in their correspondence with the mental map They do differ in their level of accuracy against the real world, though they get benefits from being held as beliefs insofar as they do not directly and immediately reduce it's ability to surviveTheir ability to take hold within our minds is also therefore related to it's inherent order and the potential for distortion of an existing mental-mapTheir functionality is closer to that of building bricks that strengthens an existing structure, with limited view of choosing a materials that only reduces its integrityAxiomatic beliefs are required because several events are not repeatable for most people for most events, which means that not all beliefs can be verified empirically, especially for beliefs that directly impact the human 'subjective' experienceThe difference between different forms of belief (e.g., dogmatic belief vs not) is that though they start at the same point, in that there's an observation that needs to be made fit into a coherent system of thought, and with coherence being the overwhelming imperative, that system then is not subjected to further tests to prove/ disproveThese systems of thought, or beliefs, have varying degrees of truth in them, defined as correspondence to the underlying world and the ability to create predictions should we be able to follow through on the chain of reasoningThe aspects that are easiest to understand necessarily relate best to the human scale of being, while aspects that are much larger or smaller becomes harder to identify and observeIt's like viewing a slice of reality from a multidimensional object, with a coherent cohesive picture only emerging as individual points that comprise it are brought to light, with each one highlighted only by its correspondence to a particular theory or narrative of the worldStriving to survive, with its biological imperative, pushes towards the creation of an accurate mental map over evolutionary timescales All knowledge, and therefore the internal mental maps, comes about as a result of embodied information gathering and analysis, as inputs, outputs and analyses through cognition and any knowledge gained, is all irreducibly linked to an individual's body Base goal sets, which for living organisms is expected to be survival and reproductionIntermediate goal sets that get created through feedback loops between base goal sets and actions that undertake to fulfil that help further the goal setsImmediate environment scan and response algorithms, which provide higher survival potential, and also includes other non-conscious calculations required to function in the world, e.g., locomotionStimuli inputs through multiple ways, each of which has different quantum of information density and requires specialized processing techniques to understandInternal representation of an external world, to which the stimuli inputs and processing hooks on, tries to keep up to date and adjustsGrowth in this fashion can use existing neural pathways to craft specialized calculation centers for individual tasks when they're themselves complex, probably in a fractal fashion to the overall schema or something equivalentMoreover, communication amongst individuals result in exchange of information regarding all the states above, which becomes input to other neural networksWithout an imperative to guide action, aspects of the map remain static, with no striving to make the representation betterThe imperative is therefore what causes any physical representation of a mental map to act in a directed fashion, as without an inbuilt mechanism to force a direction of activity maps remain static. For humans the drives to survive and procreate drive us forwardIt isn't necessary for the outcomes of the evolution of mental models to be similar to the drive itself. Instead the imperative is useful mostly to push the entire process forward and provide the time and space where selection pressures can occurIf a particular model increases the survivability and another helps generate adaptations that will help increase survivability, the model drives both as mutually beneficial as together they're not antagonistic and is part of the same 'engine' The eventual shape of the pressures that drive tend towards discovering those spaces within the overall environment where the organism (which holds the model within it) discovers a local maxima of survivability that satisfies both a survivable path condition (the path taken to reach the local maxima has to not have substantial negative dips so that the organism gets to the maxima, and through that process crafts an external input/ output mechanism for survival that is adapted to energy acquisition and continuation that's a mirror of the environmental conditionsThe large number of potential solutions to such an optimisation problem means that the path taken cannot be easily predicted either, since it depends on the overall environment within which the optimisation calculation takes placeAny activity taking place in a constantly evolving macro environment, especially one subjected to change through the behaviours and actions of the agents themselves, will find itself in a similar bind, as can be seen in multiple other human endeavours which all deal with complex systemsAll mental models therefore also naturally adapt and evolve according to external feedback and stimuliThis process forces any model to remain aligned to reality in any population insofar as non-conformance would remove the model from existenceThe fact that models strive towards accuracy in this fashion is coupled with the fact that they strive also to be energy efficientAny behavioural imperative also acts as a selection criterion which adds to the pressure that forces the model to evolve and adaptIf the purpose of life is survival, happiness, and purpose, then those together drive internal impetus for all humansThe internal view of the output of the mental map, seen from one nested level of understanding to another, is the feeling we have of consciousness - where one aspect of our mental model observes the simulation of other aspects, including stepwise computational aspects, and \"names\" them; this particular arrangement of the mental-map within an individual defines an inner-state This arrangement, in conjunction with the limitation of sensory input mechanisms, necessitate the existence of compensative algorithms that pre-select areas of maximal “interest”, which consequently determines focusThe focus could be because of a) direct attention to help gather input to solve an issue, b) accidental scan to help gather relevant input if any based on no predetermined criteria, c) pre-selection to maximise a subconscious objective without clear visibility of the exact area of focusThe combination of focus, atop particular inner state, is what allows computation to occur and intelligence to emerge with its tell-tale sign as being able to create and follow a thread of reasoning through its branches and, crucially, to come to a conclusion, inductive or deductive, around courses of actionHuman intelligence however incorporates key inputs into its focus algorithms and inner-state definitions at its base from being \"embodied\", and consequently is completely wrapped up in results relying on the human bodyIt also therefore the emergent properties from the physical body (and consequentially the outcropping, also being self referential) which is our cultureIt's difficult to have proper experimentation in real life since there are too many competing factors, so the effect has to be artificially assessed by looking at the probability that the decision could be changedAll inner-states are subject to mutation based on all lived experience, and are subject to selection pressures across all hierarchies of information transmission Inner models are the effective representations of the world that we respond to, with the input mechanisms are primarily ways to bring new information into the model to add to it or verify itThe inner-states exist as a compilation of various levels of agglomerations of 'nodes', which might be facts, associations atop individual facts, collections of facts, assumptions or deductions arising from combinations of facts or assumptions or opinions, and any combination of the aboveAs you increase the number of nodes and links, you increase the complexity of any specific phenomenon so identifiedThe selection pressures that mould the overall shape of any process, and consequently the inner state or the mental map, have to work across multiple levels of the hierarchyTherefore any applicable selection pressure has to be simple and common enough to apply globally across all hierarchies, rather than the local effects at any individual nodeThe major methods of bringing information in is via individual sense perception, which is defined per individual, and more importantly via communication with others, which brings different models and differing information directly in contact with each other Survivability is therefore the ultimate selection criterion that ideas, propensity to believe ideas, and the rest of mental apparatus gets selected uponThis selection pressure exists across all layers of analysis simultaneously, e.g., across each individual step of the neural network computation, and is not necessarily only a bottom-up or a hierarchically generated behaviourWe create systems of thought in order to create a pattern that enables us to understand the macroscopic behaviours that emerge from underlying interactions, which serve to explain specific aspects of underlying realityCombining model accuracy-maximisation and surprise minimisation for an organism creates ways for efficient exploration and prediction within the environmentWith directionality of actions provided through external means, and learning embedded in looped neural networks internally, there needs to be a low latency method of crafting feedback between action, anticipated or otherwise, and the necessary analysis thereof This low latency feedback creation method creates the backbone for our emotions While the clinical sense will distinguish the term emotion from other proto-senses that act as internal building blocks, we are assuming that they primacy of the concept emotion is sufficient hereWithout emotions we'll need some equivalent feedback loop to relate the explicit or implicit goals we hold in our minds vs the immediate outcomes, and the linkage that will enable speedier responses or motivation creation that guides action Emotions therefore act as selection criteria that could provide feedback loops to govern actionEach major emotion therefore can be seen as providing direct feedback of a specific nature. Some examples being: Happiness is acceptance and joy, of something good happening and it needing to be repeatedSadness is the reverse, an acknowledgement of something bad that happened, to be avoided if possibleDisgust is the reflex to avoid somethingAnger is impetus to change something in the world, especially when it falls awry of existing preconceptions against which we view the worldFear is to stop exploration of any phenomena, in the prediction that it might be dangerousWhile there are several more major emotions, they are only so because of commonly understood naming conventions, there also exist combinations and variations creating many minor emotions as wellThe network of emotions therefore help create a quasi-shortcut in understanding and responding to the external worldThe main method of communication we have is comprised of methods by which you communicate information in as dense a form as possible, while still retaining enough flexibility w.r.t subject matter and decipherability. This describes language, and its interlinkages with all other modes of nonverbal communication and contextual understanding. Transmission of information in any meaningful way requires an assessment of the transmission mechanism and an assessment of all other competing pieces of information that also needs to be transmittedThis capacity to transmit information is the bandwidthThe requirements include necessity to include enough informational content while ensuring it's still optimizing the bandwidth requiredIncreasing information density in this scenario is easiest when there is a high degree of shared vocabulary or shared context so that you can reduce expository aspects This naturally leads to specialized lingo in all fields of human endeavor, such as law, engineering, economics etc.This also creates within any system of language a phenomena where same words/ phrases or equivalent have multiple meanings, often only understandable through contextCommunication includes therefore key attributes of brevity, information density and mental resonance to the listener that’s judged through emotional similarityInformation to be transmitted comes in three different forms, often combined together in any individual communication Factual -> which is information about the state of something, including both first and second hand informationInferential -> where the information is a conclusion at the end of a thought process, and consequently is dependent on the entire web of thought to be counted as true. This also includes hypotheses, ideas or suppositionsNormative -> where the information is the description of a framework itself, including a meta-assessment of the way the world should be according to some a priori principlesAll communications therefore is a combination of a) the information transmitted, b) the communication medium, c) the communication source including his/her/its history of veracity and therefore historic probability of accuracy, and d) the context surrounding the communication (reasons for communication, timing, and other metadata)All communications are filtered as received also therefore through multiple layers, with the believability threshold applied to all layers, along with the eventual decisionLanguage is comprised of language-snippets such as words, phrases or sentences, which together refer to a meaning-cloud that can then be attempted to transmit to someone else The meaning-cloud is created through a) the object it references in the world, b) the generally understood perception as prevalent in the society at large which creates a context surrounding the usage of the word, and c) the individual mental representation of the object under questionThe description of the cloud is necessarily process dependent, in that the creation of that cloud depends on the process followedThese disparities become more apparent the deeper down the information heirarchy that you go, and the depth to which you go is defined by the impetus behind the conversation To understand causality its essential to go deeper into the causal factors to fully identify and describe the processTo have a social interaction with another requires only superficial alignment, which deeper questioning can only chip away atThis combination of factors is what creates the cloud since words do not always have clearly defined boundaries definitionally, and instead have fuzzier definitions where its applicability to an object becomes more or less appropriate and apt depending on circumstance and usageThis difficulty of defining boundaries also creates issues with creating definitions, which thereby only serves a function in tautological or axiomatic scenariosTherefore language, due to it's imprecision when it comes to logic, definitions and relationships, often will fail to convey the full extent of an individual's thought process and its complex branches Almost all communication includes an assumption of sufficient common ground, which remain as unspoken assumptions that all parties agree toIn the absence of this, or when this is assumed incorrectly, large parts of subsequent communication remains vapidThe chance of communicating successfully decreases dramatically based on the potential to find shared ground to build a conversation aroundIt is possible that there might be common axioms that exist to underpin certain beliefs, but their distance from the conclusion along the logic tree determines the likelihood of finding them and using them as the foundation of the overall argumentSince language is difficult as a medium to communicate complex ideas in a sufficiently crisp fashion, we use non-verbal communication overlays This includes things like body language and demeanour to help the communication recipient feel a specific emotional state that makes them more receptive to certain messagesThis also includes context, persona of the communicator, authority judgements etcUnderstanding is always a continuous process Understanding includes multiple self-referential loops of ensuring coherence in internal thought process within any defined mental-mapThis necessarily also includes a loop that continually assesses the believability of a particular assertion or propositionPropositions and assumptions are incorrect when they do not correspond to reality Believability of a proposition is a function of its coherence with an existing mental-mapBelievability also requires continuing congruence as the mental-maps are updated continuallyThis coherence is dependent on its relationship with reality, but not solely so This means that it's perfectly possible to believe something incorrect, where being incorrect means it does not correspond with reality, if it fits the mental-map sufficiently wellE.g., it's possible to believe the sun rises in the West, if you also simultaneously believe that east and west are reversed. It's possible to believe we live on a flat world if you also believe in other compensating forces that creates illusion of gravity, and other theories to back up credible observationsThe issue is not resolved through conference because narratives are constructed relatively easily once you know the base, conditions, and the idea that needs justificationThere is a clear preference for a simpler theoretical explanation over a complex one a priori, though this is in practice often difficult to distinguish in any caseThis coherence is usually broken when a) it corresponds with evidence that directly and sufficiently strongly contradicts the belief, and b) the believability of the evidence is highThe belief in such propositions usually increase over time, as the absence of evidence over a period often has similar impact on individuals as evidence of absence, as this predilection is often useful in survival, and focusing efforts on immediate and visible phenomenaThe propositions could continue to have high enough predictability potential regardless of its relationship with reality Predictions in the real world often have a high degree of error associated with it, when looked at in isolation, without concomitant increase in a sufficiently large number of scenarios to make statistical predictions stand outThis is because the number of potential outcomes that's predicted in any one situation is usually bounded, and may even be binary (e.g., the sun will rise tomorrow, or it will not), which increases the possibility of even a wrong propositional belief resulting in accurate outcomesThe increase in number of outcomes, at a more granular level, comes alongside a decrease in confidence level associated with predictions, since increase in granularity means that you need to go down several levels of aggregationDue to fuzziness inherent in object and word definitions, and the difficulty of ensuring a mental-map that’s congruent with reality in all aspects, the reactions to any event are likely to be as much about ensuring the congruence of the mental-map as questioning aspects of realityThis means that all analyses are subject to error bars due to a) them being \"aggregate\" assessments of underlying phenomenon, and b) the deviations from predicted values are unpredictable, i.e., the deviations from the predicted statistical shape happen spontaneouslyThe ability to generalise inferences is what enables us to understand what any \"thing\" is This means, for example, that we don't learn what a baseball is purely by seeing several examples and connecting the visual or auditory input with the word itself It starts rather from first understanding what an object is, or could be, then moving to creating internal belief in someone else's word as representative of an object, and associating the two together through multiple sub-groupingsFor example, it would look at a hierarchy of an object > round objects > balls (used for play, of multiple sizes, and connected to sporting endeavours, which are separate to other round objects like oranges) > particular characteristic of baseballThe representations of information as it fits into a narrative is what gets stored as memory, which itself lives on within the mental map linked to all similar conceptsThis also indicates that there are multiple types of memory; a) causing changes in the mental map that's permanently wired, and b) ones which create representations that are more malleable. This is a difference in scale than ends up being a difference in scopeLater on, after learning and integrating several such associations, the ability to infer itself is generalised, thus moving from visual things like ball to conceptual ones like the solar system, for exampleApplication of any specific thought pattern onto an aspect of the hyperdimensional complex world might create potential to predict future movements, and that suggests a certain degree of \"truth quotient\" for that patternOur experience in inference however comes from the world, as do our laws and rules that are derived to also pertain to the world There's no easy way to create finalized rules to guide behaviour that stay true through all scenarios, since the bounds of the scenarios are what's seen in the world around usOur practicality that allows survival stops us from being mathematically rational in multiple calculated scenariosKnowledge of a thing is a belief like any other, with limited distinction between fact and fiction if they're reasonably congruent with immediately visible reality Theories have to be consistent with all available empirical information, not just a subset, which is how they generate a modelIn the absence of a single fully descriptive or predictive model, which cannot be because the map is not the territory, one is forced to rely on multiple overlapping models to help understand and describe any phenomenonIncreasing number of models increases the potential ways in which they might overlap, the number of ways in which one could be wrong therefore due to increasing levels of errors that compound as they interactThe human ability to reason is a mould that will continue to reshape the form through which we see the world even as we attempt to tinker with itIt isn't a logical deductive process but rather often an inductive process that tries to marry perfect narratives to a necessarily incomplete set of facts and weave that narrative into an overall story which seems to fit within the generally held viewpoint about the worldIf multiple explanations are likely to explain certain set of facts as understood, then each mental-map would be biased towards choosing the explanation that best suits their individual priorities This devolves the argument from being about a particular conclusion to being about whether you have taken all relevant facts into account to make a conclusion based onIt becomes particularly tricky when the facts themselves are conclusions of other processes, and therefore causes multiple iterative loops that affect the ultimate explanation in questionThe structure of information, data and opinions, being hierarchically created maps as above, therefore create internal hierarchies of expertise which can credibly investigate different levels of knowledge or knowhow This implies that distinctions that are explored in the world, such as science vs superstition vs religion vs philosophy, are often primarily conversations across hierarchies, and therefore doomed to a certain level of failure until there exists an understanding of a common base to work fromAny natural variation in individual ability, thought or information architecture, lend themselves to two phenomena - clustering, where they form clusters that thereafter can be referred to by custom names, or ongoing divergence due to positive feedback loops that create a gapThe pursuit of completeness within any individual domain usually promotes the need to create an all-encompassing theory within their own and adjacent domainsThe questions that are purportedly asked within the domains itself are defined normatively, and at least at the start are not emergent phenomena coming from the investigation itselfEach one of these domains would therefore have aspects of the knowledge-map that's well thought out and logically connected, while they create \"loose\" theories around areas that aren't directly under their purviewThis creates conversations at cross-purpose amongst the varied domains quite often, without understanding that these are different forms of investigation that are, mostly, parallel to each otherTo know whether your mental map is correct therefore requires comparison with reality to assess its congruence, and also to assess the \"brittleness\" of the decision to changes in the mental-map This means that decisions don't just have to be right, post hoc, but rather has to be resilient to changes in assumptions - this is one way to identify if what you're deciding is indeed a local maxima or a random outcomeTesting of the accuracy of a mental map can be done through predictions and observations to assess the outcomes of those predictionsThe potential fallibility of the observations, both in the specific sense of being incorrect in the particulars it identifies or misidentifies, and in the fact that deciding what is to be observed in the first place is a meta-decision that’s subject to great bias, creates difficulties in the process of doing predictions and fine tuning a mental mapTo properly understand another's reasoning you need to understand their mental-map to a sufficient degree With sufficient complexity of existing mental-edifice, an output is indistinguishable from premeditated reasoningEvery communication is a synthesised representation at the end of a thought processTherefore each communication contains multiple levels of information, not all of which is communicated in the same way, necessitating parallel processes of decryptionIncorporation of new information and generation of conclusions about the world rely on existing mental infrastructure to \"run\" the analysis through, and therefore are themselves path dependentTo create decision frameworks within this networked world will require guides to be created To create a guide would need, ideally, the creation of rules that govern our actionsHowever following a simple rule is possible only if it unravels against the entire tapestry of the worldAnd since it's not easy to create rules that are specific enough to stand true across multiple permutations of the networked world, we have to rely on heuristics regarding navigational behaviors within the tree and simpler guides to behaviors which generally result in good outcomes, without overwhelming the inherent computational capacityShared belief amongst people is what gives rise to a common framework that can be used to create societally helpful fictions defining civilization Shared beliefs emerge as a resulting of competing narratives that aim to simplify and synthesize a complex reality We gain insights and lessons from data backed inductive and deductive analyses, i.e., logic, and stories which are metaphorical but indicativeThese are different methodologies though they both affect belief and knowledge equallyThe narratives most likely to succeed as the \"best\" in any scenario are the ones which have the highest potential for both explainability of all supposedly relevant facts and probabilistic potential to not disturb or destroy the existing mental-map that's been constructedThis also implies that \"clichés\", as they arise, are often a result of such accepted \"wisdom\" which contains a kernel of narrative truth However it's only through lived experience that one can often fully understand the provenance of the narrative, as its internal structures have sunk invisible beneath the \"smoothness\" of the narrativeThis is because a narrative ultimately always obscures parts of the overall phenomenon, creating focal points to focus on and suggests other points to obscureArguments and debate are essential methods to understand reality as reflected in mental-maps in the absence of a method to discuss networks in its entirety Debate amongst ideas works as a method to get to a mutually agreeable mental-map that's ideally predicated on the same set of facts, and therefore are congruent to realityThe primary power behind it is because it forces all parties to craft arguments and narratives which could help weed out incorrect strands of argumentsDue to the necessity of communicating only the synthesized aspects of any piece of knowledge, since full details are neither available nor communicable, arguments have the drawback of being linear narrations that necessarily simplify complexities of any situationConversations therefore not only communicate ideas and specific pieces of knowledge, but also outputs from and synthesis of internal models, and credentials to help communicate the veracity and accuracy of those modelsThey exist as successful methods only because humans think best when rationalizing viewpoints which work best as arguments Individual arguments are by themselves only components because they rely on there not being external facts that impact the argumentThis holds true as arguments are narratives which inherently favour linearityThis form of inductive argumentation therefore often works best since it's about crafting the most compelling narrative for any particular grouping of facts, thus increasing the seeming narrative explanatory powerWe play amongst societal norms and structures that define the implicit environment which we then interact with and live within to create our lives And in a sense since the environment becomes a participant in the creation of an individual life, the echoes of an action can have societal reverberations that go beyond an individual lifetimeThis is indicative of how societal memories can be said to have evolvedThis also creates the common framework that defines existing 'norms', which essentially defines what the shared belief is at any point, and how it evolves over timeCreating a linear narrative atop what is a complex adaptive system necessarily creates difficulties of interpretation, because the system itself is mutable while linear narratives are not necessarily similarly flexible All narratives, especially the logically consistent ones, rely on the unspoken assumption that only the chain of events that are described in the narrative mattersWhen you don’t know enough details about any topic, with all its complex branches of information, it becomes difficult to create a meaningful opinion since you could always disagree about which parts of the network is most important to reach any conclusionArticulation also forces a narrative structure on a set of facts and therefore makes aspects of life even more unreliableNarratives therefore illuminate one particular sequence of logical deductions and rely both on the relevance of the nodes of information it touches, and the logic it holds as if trueNarratives are often created through the creation or identification of an inductive reasoning chain is potentially related to the deductive logic chain However the applicability of the narrative is not necessarily correlated to the existence inductive chainAt most instances there can exist multiple inductive chains, all of which are logically plausible for any set of nodes in the network, even if they're incongruent with the reality because they do not seek to explain all the relevant variablesMetaphors and stories have power exactly because they are open, and not easily interpretable as expected - this forces the reader into constructing their own reality, where similar metaphors have to apply, and therefore acts as a shortcut whereby an entire worldview can be transposed, as opposed to purely a unit of informationThere are epistemic challenges to understanding the very world we live in, and as such would present inferential problems to our ability to gather and develop knowledge about the world. However despite the challenges since there are extant laws that we're obeying, even if in ignorance, it indicates that a certain degree of understandability follows, as evidenced by the fact that our predictability is > 0All supposed knowledge exists at the very precipice of an own known mountain of buried and unanalyzed precepts, both created and borrowed, and this means that knowledge as it's supposed today is only the tip of the icebergThe fact that we live in a certain environment and have our lives and perceptions around it means that it's an overall web within which we're situatedIt's a novelistic dimension of how the world is - whether or not each individual segment is mathematically correct, it's the link that together creates a mind's tapestryCognitive and computational capabilities per node are limited, and requires narrative definitions to carve out a space within the overall web which can then be sub-analysed Narratives are influenced by our inner states, which affect both the focus that we pay to individually supposedly relevant facts, and affect our propensity to believe in certain narratives that solidify the existing mental-mapSince mental-maps are complex, having multiple interconnections in all directions, to run any \"query\" through it would require instructions to also guide the queryThere exists within each decision the sum total of weightage that is crafted through the network, including its own historic antecedentsEmotions act both as cognitive shortcuts to help make quick decisions regarding a particular circumstance without needing to make detailed computationsEmotions therefore have the potential to connect parts of the mental-map that's traditionally not close together, by ensuring the triggering of a particular sequence of nodesIntellectual nuances are the exploration of branches within the mental-map that's related to any matter-at-hand, where the matter-at-hand is defined as the focal point of attention Since emotion functions as a shortcut to decisions, it is naturally antithetical to most nuancesIncredulity by itself is not an adequate explanation for belief or lack of beliefThis implies that narratives therefore can be adjusted and arranged across multiple layers of information, itself related to each other Base layer is around information regarding the state of the world, which are most likely shared across most populaceSecondary layer around assessment of the state of the world, expressed through emotional triggers, which are far more culture and person specificBoth of these are intertwined to create sophisticated models of reality inside our mindsDealing with external world is therefore subject to reinforcement pressures for existing mental-maps alongside utility in specific, material, termsThis implies that narratives that we hold on to as cherished or dear oftentimes serve the purpose of helping us reinforce existing mental-maps, rather than provide utility by improving predictability of the future and alignment with realityThe availability of information, or data, itself cannot change and narrative, because narratives are not built only on one layer of information or data, but rather also include synthesised opinions on top of which the entire argumentation is builtAs a jigsaw puzzle piece fits in a slot, a piece of knowledge we develop or create also fits into a slot against the shape of the outside world and environment. It's a mutual discovery of both the negative space and it's constituent componentsThe ability to learn through narrative frameworks is the only way to create a sufficiently robust mental map and framework, though the narrations themselves are incorrect and incompleteThe coherence of the system is the biggest existing structural reason that allows there to be larger and larger levels of internal dependence amongst its factors Due to the level of importance placed on internal system coherence, perceived unfairness in a system is punished more harshly than what could be expected purely based on rational calculations for any individual act or decisionThis implies also the creation of multiple processes to construct a record of and keep intact the internal coherence of the systemThere's a distinction between explicit information about a particular topic vs a decision making process that makes it different to pure \"expert\" opinions which are often formed within black boxes of own complicated thinking The virtue inherent in explicitly stating assumptions and crafting a clear and auditable decision is not obvious or always extantSince the underlying processes that the approaches are trying to mimic, or predict against, are themselves unknown, it's unclear whether you can say upfront which method is more liable to workHowever it can be said that for more complex mental maps, or networked decisions, a linear or explicitly detailed assumption based method is liable to be less accurateThe qualia resulting from the type of understanding that comes from following any process, of creating a particular sort of mental-map, is indistinguishable from each otherThis means that the qualia that emerges as the result of solving particular types of puzzles, or creating specific forms of explanatory narratives, are indistinguishable from each other purely on the basis of its congruence with realityImplicit decision making, which is also called intuitive decision making, is mostly correlation based, based on pattern recognition and quick response, however explicit decision making is often causal in nature Part of the causality could be determined by deep understanding of correlations amongst individual components which are collectively highly complex, but there're still multiple layers of abstractions of concepts and relationshipsThese are named and correlated to the real world itself, which creates a layer of understanding about the world, on top of which you can have 'bespoke' pattern recognition metricsThis complexity, and necessity of congruence with the real world with feedback loops to make sure that not only is the final output correct, but the intermediate steps are also correct, makes explicit decision making different, and also interestingly something the AI of today cannot doDecision making is split between instinctive reactions to events that arise as a result of the external input, existing mental-substrate that does the processing, and the immediate output, and more systematic and thorough investigations of a phenomenon to come up with explicit arguments Immediate, instinctive, mental outputs, and more conscious, defined, cognitive processes, both work on the same mental-model substrateThe distinctions in their performance w.r.t accuracy and speed are indicative of the fact that these are distinct processes that, however, should be used in conjunction with each otherWe make decisions under highly complex and interdependent environments We're conditioned to respond taking the entire environmental complexity into accountTherefore it's unsurprising that even \"controlled experiments\" in social sciences show evidence of evolutionarily adaptive behaviours, such as in-group biasThey're both emergent phenomenon that arise from internal calculations, from whatever base substrate exists to process the calculationsThe substrate doesn't change from one decision to another, since the underlying \"infrastructure\" doesn't change - therefore the conscious thinking, whether it's done in a sophisticated fashion or done purely as a reflexive action - doesn't necessarily change the infrastructure, only the web of signals sent through the existing networkThe different methods of thinking are more a function of internal reflection to assess the inherent logic of the decision, done often as a function of time vs depth of the graph that's examined, and create a continuum of conscious/rational thinking It's a paradox that while you have to guide intentional thoughts, they have massive limitations in what it's able to process, and the depths to which it can goTherefore you need to train the unintentional thought process, the neural network in the brain, to respond accurately to the world as you perceive itIf you're able to temper the immediate impulse with a bit of a corrective push, that perhaps that can be called rational. Conscious correction is a phenomenon that by itself is highly error prone, due to issues of speed of computation amongst others, but that's also true of all computationHowever this continuum is not necessarily aligned to increasing accuracy, since the conscious thinking, just like subconscious processing, is subject to myriad of biases, availability of data, and processing sophisticationDecisions have not just an immediate maximisation function amongst alternatives available, but also a temporal maximisation where there's an implicit question about what the long term impact is likely to be, which together determines any final changes in the mental map regarding belief So your decision to spend money, as an example, can be amongst multiple alternatives, but also includes not spending it and/or investing it (in cash or kind)There are limited number of degrees of freedom available w.r.t most decisions, since most decisions are not necessarily situated in an infinite continuum, but are rather more of a discrete multiple choiceA belief, or a conclusion, is almost always the equivalent of choosing one amongst several alternatives, simultaneously judged against the probability of the thought-process that leads to the belief being accurate, and the outcomes that come from having the belief being acceptableBeliefs are built upon facts, and logical relationships amongst those facts, which together give rise to potential choice of actionsA fact is a representation of a particular stage in a mental-map, at a particular layer of aggregation, which is resilient to reality The facts have to stand up to scrutiny across a wide variety of circumstances and experiments, which aim to verify whether the fact is able to provide predictive and descriptive rigorThe description which becomes a fact can be a node in any aggregation level - and the network that it represents becomes a fact, though necessarily with fuzzy boundaries since it's the reflection of a portion of a network, rather than an independent standalone factThis implies that beliefs are most likely to be held when a) they are congruent with reality AND has a direct, visible and measurable impact on ones state of being, b) when it provides an explanation that creates enough sense of logical connectivity that it seems to simultaneously satisfy the need for simplicity in ones theories while providing maximum potential predictive ability, for a certain value of mental input, or c) when the existence of a world where the belief is true provides maximal benefit to you personally due to the knock-on impacts both mentally and actually, which also pushes one towards proselytization of that idea Feelings and facts are not separate, but rather nodes in the same mental-map, with facts, hypotheses, ideas or narratives being networks that is expected to map to a part of reality, or a way of understanding a part of reality, and imbuing them with a sense-check of how it could impact the meta-framework that is then impacted by the fact, hypothesis, idea or narrative.There are therefore multiple ways for you to be wrong, even assuming it’s along a continuum where wrong isn’t just one state but rather a series of possibilities - and since the number of potential decisions at any point are restricted, the reasons for being wrong are as important, or more, than the seemingly binary fact of being wrongUnless the conclusions from a networked mental-map, for any particular inner state and with relevant focus, can bring forth a new, surprising, out-branch to enough people's disparate mental-maps, or fully disrupt existing pathways, communication by itself cannot help shift the patterns The largest source of confusion is regarding definitions, both regarding questions and their potential answersThe \"non-sense\" nature of the statements engender confusion, wherein at least one component of the statement has to be false, or unprovable, when looked at in a bottom up fashionThe creation of such a vast array of groupings, across all dimensions, also creates multiple identities which themselves are under mutual construction and evolution Your identity would therefore itself change and evolve through these macro evolutions that happen across hierarchical boundaries, with boundaries that aren't necessarily distinct, and could even be fractalThe co evolution of multiple humans together also create the rules which together form the overarching system within which we deign to operateIf your idea of your self and your actions are not directly linked to consequential adherence to the rules themselves, then the rules get bent a little, which has negative consequencesTherefore we need to have counterfactual consequentialism to solve for it - however it's analytic burden means you need something more easy to ascertain as a heuristic.All reasoning is social Doing a thing because it's always right or moral according to you also has to take its consequences into account, and your contracts with the rest of the web of humanity has to be taken into considerationDevelopment of this understanding is the fundamental role of philosophy Since human thought is what is behind human philosophy, you have to craft philosophies that answer to the essential human nature, and plays a part in helping lay bare part of that contextual web within which we liveAnalysing normative answers in the absence of an understanding of the antecedent social contract makes any idea output from it potentially unusable and even nonsensicalDoing immediate good in and of itself isn't a useful, if utilitarian, goal - eg while it might make some moral sense to give one of your kidneys to a stranger, at a point in time, considering the obligations you have to the rest of your social circle, family and broader society to live life in a certain fashion, you shouldn't do it unless it fulfils those demands equally as wellIt stops the flourishing of humanity, and because of it there is no progress. The progress here has to come from the entire human web as an emergent phenomenonKnowing your place in the overall web and being cognizant of the ability to affect specific nodes within that web means that you're also able to better focus efforts that have higher impact potentialFurthermore due to the humanistic tendencies within us, developed through our history, the tangible and immediate have much higher premiums placed on them unconsciously rather than the low probability but higher impact actions, though mathematically they may be equivalent, since the collective numbers required to make the statistical mathematics work don't translate into individual lived experienceSocial covenants that bind us have direct impact on our reasoning ability and create the bedrock belief system that then gets used and edited This is since the inputs, the factors creating any impact and any pre-assessment of the outputs are all socialThe history and ethnography of the society can't be extinguished or swept aside as they're a key component of any future changes, since society is path dependentThis means that our obligations to each other and the demands that society in general, and individual decisions in particular, place on us are unlikely to sway the fundamental tendencyThis means that unless there's a groundswell of heightened movement in specific tendencies the societies themselves don't move in its centre, which is how Overton windows become an important way to gauge societal adjustments to specific movementsConsidering therefore the network effects and feedback potential within the societal structure the only optimal way to behave is to be a counterfactual consequentialist while retaining the epistemic humility that leads one to understand that not every factor is known or knowable To attend to any new course of action there has to be an attempt to identify the potential consequences thereof There is a difference between discovery, to identify mechanisms that impact or explain specific phenomenon, that we can presuppose is possible or that it exists. vs the invention of new ways of thinking that's completely unknown to date where you could search with no hope of identifying even the key variablesThe essence in either is to celebrate ignorance, specifically the ignorance of all possible outcomes, as that's the prerequisite for explorationThe question is how can we live well considering so much of the future is unknown, and indeed unknowableThe only ability therefore that we possess is to try to reduce the uncertainty through experiments or observation, and use this increase in epistemic certainty to push forwardIn the absence of knowledge of the consequences itself, knowledge of the shape of the possibilities of said consequences could itself be valuable For instance, you might not be able to tell the outcome of a particular course of exploration, but you could make a better guess of which course is likelier to bear better fruit, or which courses are least likely to be dangerousWhile not fully accurate, since we cannot make a map without knowledge of the territory, the meta-knowledge of the terrain itself could be valuable information for a counterfactual consequentialistThis creates an incentive to behave in a fashion that creates better outcomes for most through a measured and reasoned outcome, since that's what's been primarily selected for within the overarching evolutionary processSince collaboration and social reasoning is co-evolved with other primary needs for individual organism fitness, as evidenced by multiple details of altruistic or socially beneficial behaviour, this creates an incentive to behave ethically above and beyond the fact that we have evolved to have ethics, to be kind to our fellows, since there's negative consequences, short or long term, to behaving unethically This fairness doctrine is just a solve for longer term iterated prisoner's dilemma game theoriesThe longer-term indicator here also suggests that while it might be a longer term optimised theory, there is significant movement potential within shorter time horizons, and since the game itself can be modified, it cannot be relied upon as a lawCounterfactual consequentialism, when it is expanded upon, shows the degree of freedom that people have in their actions The degree of freedom can also be correlated with the eventual impact they can haveBoth together creates a way to measure the potential for actions, and the ability to navigate the causal web to create changesDue to the large number of incoming inputs there might be created a new collection, creating a concept, that puts together a large number of related network nodes and edges (or individual pieces of information and their relationships), an internal mental model, and this creates separate feedback loops as well So you might see people measuring their relative social positioning rather than absoluteSimilarly, breaking norms within however is a way within this network to break the rules of the game, which changes the playing field and network configuration, which changes the nature of any existing preferential attachment, and which consequentially enables new outgrowth to emerge as a paradigm to followSubscribe now\n",
      "'utf-8' codec can't decode byte 0x80 in position 3131: invalid start byte\n",
      "This file isn't supported\n",
      "Will robots inherit the earth? Yes, but they will be our children.Marvin MinskyI. The rise of robotsEver since I was a kid, I’ve wanted robots. I'm not alone in wanting this either. Robots are the basis for any utopian vision of the future. And as we're starting to see this future coming together, with AI able to write emails and code and make art, this is the part that's still missing.Yes it's hard, and yes we've tried in bits and spurts with limited success, but many many things are hard and we do them. So why not this? All we're left with is a lonely roomba getting stuck on the odd carpet or litter box.When I tried to ask why this isn’t here yet, the major objections were:The cost is too highReal world is too hard to navigateHardware isn’t ready eg tactile sensorsBattery density is too lowThis devolves to a function of two problems - they don’t work very well, and they are not affordable.Let’s have a look.They have to workOne of my favourite things I did was to teach a robot how to walk, back in 2005. At that point, since working AI was a pipe dream, you had to think about the mechanics of the joints, angles of bending, weight distribution and centers of mass, inclines of the path ahead, and a whole bunch of other things to be able to create some boundaries within which the robot learns, to train its neural net.(Parenthetically one part I remember is naming a bunch of variables after characters in the Prince of Persia videogame, just because we were incredibly cool in college.)And turns out that this is incredibly hard. There is no way you can handle all edge cases when the training is pretty specific, and rooted in biomechanics. To be successful, they have to work relatively autonomously, navigate its surroundings, make autonomous decisions, and be able to actually handle things like a baby blanket or a hot cup of coffee.To actually do them in real life requires some form of reinforcement learning (I wager) to get the inner equilibrium etc right. I assume this isn't trivial, though I've only done it the hard way. But OpenAI did set a benchmark for robot dexterity, and later abandoned its research because there was no large enough dataset to help power its reinforcement learning.And the robots of today seem pretty similar. Boston Dynamics have some of the best videos of robots doing cool stuff I’ve seen, but their training method seems awfully similar to creating tightly bounded environments where the robot can learn particular tasks. What can the robots do? Today they only seem to work in highly constrained environments.To make something that works in the physical world is hard.Thanks for reading Strange Loop Canon! Subscribe for free!A big complaint about Boston Dynamics’ beautiful dog Spot is that the battery lasts about 90 minutes. Which is not enough to be able to get almost anything done. Which also means even if you bring Spot home, it’s not going to be all that useful!This is comparable to the Roomba, also at around an hour and half, at which point it has to stop and recharge. I mean, I can’t blame it, this is already far above the average human.But it does feel like this isn’t enough. And this means we need a way for Spot to run about the house doing things without having to rest regularly. And that’s having far better batteries. Luckily, Noah seems to think, as do I, that we’re entering an age of the battery. This is essential if we are to dramatically increase this by an order of magnitude.In mobile phones, for comparison, if we look at the past two decades, we moved dramatically in terms of battery capacity, but increased way more in terms of capability, which made it seem the battery life had fallen. This would be an acceptable compromise, though even with that 90 minutes seem much lower.But even if we fix the battery problem, do these robots actually work?The good news is that we’re seeing progress! Robots are becoming more dexterous. The difficulty of real world can be reduced at least a little bit by focusing on smaller use cases than creating a robot Jeeves - start small by picking up and doing laundry! This is also far safer, since the level of damage it can do is far less, and it can move extra slow if need be. And the battery density point is important, though at least inside a household, it’s fine no? You’re never more than a few feet from a socket anyway.I feel if we had the will there’s surely a way.They have to be affordableCost curves are a fact of manufacturing. The best example is solar power, which has seen reductions in costs that boggles my mind every time I see it. It puts Moore’s Law to shame!This has come about with a truly gigantic level of investment in producing it, which only got filled because of the demand for it. Without mass manufacturing to meet demand, the costs would never have come down.Or computers, where Lisa, when it was first debuted to the world, cost $9995, around $30k in today’s money. It didn’t sell very well either, but it started the bending of the curve.So why hasn’t it happened for robots? My guess as regards an answer takes inspiration from an adjacent world - home appliances.Home appliances are still insanely expensive (I’m still scarred from a kitchen remodel we did last year) and they are expensive partly because every single one seems to be in a slightly different size to the other, which is a different size to your kitchen, which is a different size to the counter, so the whole thing is like playing Lego with bricks that don’t quite fit together.This is despite many a mild mannered intervention from the powers that be.in the United States, the National Kitchen Cabinet Association (NKCA) has established guidelines for the dimensions of kitchen cabinets and appliances. These guidelines are intended to ensure that appliances and cabinets are compatible with one another, and that they can be easily installed and used in a kitchen. Additionally, in Europe, the European Committee for Standardization (CEN) has established a number of standards for household appliances, which include specifications for dimensions and other technical requirements.This all sounds good, and maybe is true, but from a consumer’s POV, this doesn’t seem to work at all.This is despite the fact that home appliances market is like $400 Billion and growing about double or triple the rate of GDP growth.Robotics, by comparison is around a tenth the size, though growing faster. Most of the robotics spend are driven by industrial applications - manufacturing, healthcare, agriculture and logistics - and very little as collaborative robots, or cobots.Whether it’s linear robots like gantry, or Scara, or delta, or cylindrical, industrial robots have had specialised forms and are complex to install and use.Hyundai invested $400m for a robotics division. Ford has robots do painting and body construction for its cars, as does Maruti Suzuki. Mitsubishi has robots which the workers can fine-tune with voice commands.There are grocery warehouses the size of seven football fields which have thousands of robots running on rails to grab, pack and retrieve products, by Ocado, near London.The “Collaborative Robots” market, which are the robots we are talking about, to run after us in our homes and pick up laundry and do the dishes, that’s a much smaller segment, around $700 million, a large chunk of which is made up of roombas and automated lawn mowers.There was one study I found of research into what prices we’d need to hit. And they, I have to admit, seems about sensible.The ratio of the price ranges of the $300-$500 and $500-$700 was fairly high. The noteworthy aspect was that the working moms tended to evaluate the prices to be expensive; the response ratio of the working moms in the price range of $700-$900 and $900-$1,100 were higher than the other targets.II. What if … we are surrounded by robots pretending to be not?There is another argument that hey, we’re surrounded by robots, we just don’t call them that. Your coffee maker makes coffee instead of a Barista, your washing machine washes clothes instead of you, etc.The problem I have with this definition is that while there is automation of some activity, we’ve done so by highly constraining the problem. And as a result, there’s an enormous amount of effort that’s needed to fix, clean, maintain or generally deal with the idiosyncrasies of an automaton.Simon Sarris @simonsarrisI think people expected their robots would need to be cleaned and oiled, and if we went with the full ambulation you'd have to do a lot more work\n",
      "\n",
      "this thing does like 9 tasks (grinding, filtering, water filling, steam pressure). You don't oil its joints!\n",
      "\n",
      " rohit @krishnanrohit@simonsarris However the problem I have with calling all machines robots, which this theory does, is that it still has a ton of work that you put it. I have to clean, fill and maintain my coffee machine, to the point I often just walk to a cafe. This feels very anti-robot-like in ethos.3:05 PM ∙ Jan 20, 2023A robot should, at the very least, empty its own trash like the Roomba does. I mean, that’s just basic.A robot should have some autonomous capabilities of dealing with an unpredictable environment. A Roomba, in this definition is a not-very-smart robot. An industrial machine that mills complex parts for a car chassis, is a very-smart non-robot. There are many ways to define these things, and plenty in a grey area, but this is my definition. It agrees with intuition, and I’m sticking with it.More importantly what you’d want is a robot that trundles around the house, picks up and puts away toys my two boys regularly create obstacle courses with, and picks up the laundry and puts them in the washing machine.This should be doable, no? This is the dream. Without that we’re puttering around in the margins.Here are a bunch of robots that won the DARPA challenge, flying ones and four legged ones and track bots, operating well in the underground world. Helping to map, navigate and search underground environments, with limited visibility and freedom of movement, and a fragile environment all around.III. Standardise!Tesla is building, apparently, a multi-purpose humanoid robot. I’m not sure why, or even if it’s actually true, but it was announced. More prosaically, Amazon bought iRobot, and Dyson is planning on building household robots. There’s clearly interest. And Goldman Sachs, through some excel magic, think there’s a $6 Billion market for humanoid robots. It seems small to me, but it’s clearly big enough for the largest automaker.There were also “9 million domestic robots” that shipped last year. Most of these aren’t what we’d think of as robots though. They are automatic vacuums or automated pet feeders or robotic pool cleaners and lawnmowers.To recap, a robot, to be successful, would have to be able to solve a few hard problems:Move autonomously, like a Roomba, or one of those Boston Dynamics parkour bots. This is (famously) hard, as the autonomous driving efforts have shown us. Can it be done inside a home, or a kitchen sitting on a countertop? Feels easier, sure, but “feels” isn’t enough to spend $1B in R&D. But the inside of a home, especially is circumscribed, seems a smaller number of crazy variables than handling a 3000 pound hunk of metal hurtling at 70mph.Be able to manipulate objects, like picking up a teacup or a laundry basket, again like one of those parkour bots. This is hard because of the weird shapes or materials that objects have, as the ability to handle things isn’t developed yet. This is exceptionally hard once the robots aren’t affixed to place. But here too, can’t we try and break the problem down a tad? Solve for clothes only, move on to china, or books. And,Stop when it hits a problem.The thing that I think of when faced with this problem is that we’ve solved bigger problems before. When electricity was all the rage we literally had to wire up the entire world. And we did. Same for pipes for water, and again for telephones, and pretty much again for the internet. For appliances we covered the world in televisions and washing machines and refrigerators and dishwashers in record time!My bet as to why this hasn’t happened is the same reason why home appliances still come in random sizes. Without standardisation, costs are too high.Ranges will be 30\" wide. Slide-ins too, but the depth may vary. DW's will be (close to) 24\" wide. Fridges will be 36\" wide, depending on size. By that I mean a smaller 18cuft might only be 34\", a 25cuft might be 36\". Height will vary from mid 60\" to mid 70\" depending on size.Cooktops will vary depending on number of burners and who makes it. In general, they come in 'classes' like 4-burner at 30\", 5-hob at 36\", 6-hob at 42\". European versions will be metric equivalents which could be a bit larger (800mm would be about 31.5\").Ovens come in 24\" (rare), 27\" (not quite as rare) and 30\" widths. Heights vary a bit, but you're probably talking around 27\" tall.Which is mostly because most of the appliance sales are for existing homes, rather than for entire new builds, and they require the old measurements to fit into the existing spaces. Economics is hard to fight against.And also, its just uneconomical to change everything to be standard size when you do this just by yourself.According to the National Association of Realtors (NAR) Research Group, sellers who upgrade their kitchens recover only 52% of the cost, on average.Spot’s robot dog costs $75k, and this is the only way to bring it down 100x. It’s the only way anything works. Even in software, have you tried linking up multiple devices to your Google Home or Alexa? It’s a nightmare. Nothing actually speaks well to one another.Back in the day, from the days of Homebrew Club to even the good old days of early 2000s when I went to college, you could go buy the components that you wanted (the perfect RAM, HDD, cables, motherboard etc) and assemble it yourself. Over time, as we standardised the building of these systems, all through the supply chain may I add, the costs came straight down.For now, with costs being high, the robot market is still small. The same virtuous cycle that solar and chips saw gets vicious when reversed. And the only way to reverse it is through standardisation to drive efficiency, so that we can benefit from economies of scale.We’ve made progress. We have made robot arms that can pick up dumbbells, powered by water for some reason. We have people creating their own robotic arms. We have dry-cleaning robots, stair climbing robots, and just look at this robotic kitchen, wouldn’t you want a cool version of this that can do more?But automobiles only got much cheaper because the assembly lines started using standardised parts and a straightforward assembly technique. This is the as yet unrequited dream behind the creation of prefab buildings. Solar panels are another. The certifications you’d get, like IEC 61215, ensure a consistent standard to meet. With minor variations, the sizes are made the same. Module formats are the same, the power usage and output are the same. Connectors and wiring are standardised, like MC4. Add it all together, you get the 60% reduction in cost over the past decade, with a much higher install base. And who can forget, most famously, the shipping containers that changed the face of modern commerce.We have dexterity. We have reusable parts. We have pretty flexible robotic arms, especially when stationed on a counter. We don’t have a decent assembly line though, it feels like the old computer days of building a PC yourself. But we have high degrees of specialisation, with purpose-built machines all through our homes. And we have the ability to translate spoken word into code.So this is my admittedly dilettantish suggestion. I am likely to be wrong, feel free to let me know. But meanwhile let’s standardise an assembly line, and go build them. Start small. A robotic arm in the kitchen, or like something that trundles along behind us and cleans up. No more dangling wires and messily soldered boards. Lux will surely fund it. And I’ll buy it, as would I think a large number of my friends. And then let’s go see if we can’t ask literally every single household if they don’t want another convenience revolution, the biggest since the washing machine.I think they’d say yes!Thanks for reading Strange Loop Canon! Subscribe for free to receive new posts and support my work.\n",
      "Combinatorial innovation creating everything from anythingA large part of human effort is to change one thing to another. We read reports and summarise them. We experience events and write reports about them. We take screenplays and make movies from them. We think about something and write symphonies. We read poetry and create music. We create beautiful art, learn from them, and create even more!These are all efforts to get to the bottom of who we are as humans. We are asked, in our jobs and in our studies, to constantly change a thing to something else.And that is what’s being created by the generative AI revolution going on around us.And it’s good! We can move codes, requirements, images, videos, music, essays, maybe someday even books, back and forth.We’ve built an anything from anything machine. It’s not nearly perfect nor even really good enough for government work, but this is the trajectory.It will help us paint something if we want it to, to help create a movie or a song, to write an essay, to summarise academic literature or read books, to figure out the points or counterpoints for any argument, to code and to write documentation, in general be able to convert anything to anything.Imagine if you could fund your own startup. You hire an engineer and a designer and a marketer and an operations person and y’all sit in a room trying to make sure one person’s work is seamlessly linked into another’s. The marketing pitch has to lead into the actual product being coded. The requirements from the user has to be a doc that has to become the codebase. The user feedback has to link to the design document that has to link to the product.But as anyone who’s dealt with this menagerie knows, running interference between these folks is often a logistical nightmare, filled with endless streams of misunderstandings and petty feuds. But if all these voices were inside the same person, things could be better. That’s what this is. If you get to link your inner specialised homunculi with all these capabilities to each other with your anything-from-anything machine, this becomes a whole lot simpler.Noah and roon had an article about how this would mean more possibilities for the human user, due to comparative advantage. They envision a “sandwich workflow”, where the human asks for something she wants, AI creates a few versions or options, and the human selects her favourite to edit and use.I think this is feasible, but it’s worth looking at why this might be the case. Today it exists because the AIs are not perfect out of the box. The question is if it will understand what we’re asking well enough to create what we wanted, or create for us a pastiche that sits in an uncanny valley somewhere.But in front of us lay three eras, as the anything from anything machine evolves.The first eraThis, if true, gives us some sense of how companies built from this might look like. To use this anything to anything power still requires humans to help other humans figure out what they want. The best companies are likely to be consulting firms, tech enabled and AI powered, but consulting nonetheless.It will continue to get used by plenty of folks who want to either roll their own, or pay the cloud computing costs to run it plus a markup. Most softwares will start integrating parts of it into its product, from Replit to Github to Canva to Word.But for most enterprise applications, what’s needed is someone who can coax a behaviour that doesn’t come out of the box. Also, because the outcomes aren’t often 100% reliable, as of yet, it will need human help. It’s a way to solve problems, except it can’t always be just provided as a response.Yes AI can supercharge making characters and animations in games, but to figure out what to choose and how to combine things will need someone’s help. Partly because of the data that’s needed including IP and the effort to get an output that people want.The foundational models quickly get open sourced, and the competition remains primarily in the fine-tuning, training and product features arena, very much like the cloud applications war in the 2010s.As the competition is likely to be over go-to-market and product features, we’ll see some large companies emerge, though I suspect unlike the saas wave, these companies will have even less staying power. For one thing their hard-won finetuned capabilities might come out of the box on the next foundational model release. And for another, the software that companies are making $100m ARR from are also the same software that others are building over their nights and weekends - the moat is awfully thin.The second eraMaybe our edge will be in those edge cases. Or in creating truly long-form pieces which so far seems beyond the AIs ken.Or maybe not, those edges too will get sanded away.My favourite example here is text to speech. It’s a technology I’ve been playing with for the past decade and more. It was horrendous for me, as a speaker with a mixed Indian, British, American accent borne of watching too many movies and reading too much Wodehouse. Over time though, things have gotten good enough that the edge cases, while annoying, only crop up when I ask Google about the eating habits of quokkas. Much better.From Hank Green. He asked AI to describe a cat and then put the AI's description of a cat into an AI image generator to create this eldritch horrorAs the edges get sanded away, the utility increases dramatically, and the two pieces of sandwich fall away, leaving only the filling. So to speak.This would mean that a lot more of the cognitive work that gets done can finally be automated, removing the bullshit jobs as Graeber called it, the creative recombination jobs as McKinsey calls it, or the general “making your manager happy” jobs as I call it.When there is a fundamentally new capability that emerges, unless there’s some sort of clear ecosystem lock-in, the competitive advantage will diminish rapidly. I’ve written about species emergence based on various niches, and this is a common theme.So what’s in store for us, considering we have an anything-from-anything machine which looks like it’ll keep improving.For one thing, the models today are idiot savants. They’re neither capable of jumping across domains very easily, nor are they able to explore the latent space all that well on their own. However, this is not criticism. This is an opportunity. The next step is clearly to string together multiple models, so the anything-from-anything machine gets true breadth. It’s starting, albeit in muted fashion, and with plenty of manual finetuning and parameter selection left. It’s alchemy at this point, not chemistry.The third eraAnd the next step then is training AI itself to string together multiple models better, that too feels like the natural extension of where we’re heading.By itself it might not be sufficient, since in order to fully explore its capabilities, we’d also need to be able to widely deploy agents in multiple scenarios without necessarily running into hard physical energy use constraints. And so once this is possible enough that the power consumption is drastically reduced, that’s likely to be where things explode.That’s when we’ll have to grapple with the fact that a large percentage of what our jobs entail is surfing the nether regions of our collective unconscious, isolating and making real our desires. Business is the way we’ve found to explore this and give ourselves the things that we need.Will we be able to automate this away? I don’t think so. No matter how you attempt it, the idea of providing value to each other by better serving each other’s needs will remain a mainstay of our culture for quite a while. Even if we are able to create autonomous if-this-then-that loops to run complex tasks of coordination and communication, it needs some level of direction or intent, not to mention some course correction, which is what we normally supply.Will it spin out of control after this era? Leading us into a valley of death, or maybe utopia? Maybe, maybe not, but that’s an essay for another day (coming soon). Dealing with the real world is neither as simple nor as interesting for any number of linked-together anything-from-anything models.And if all of this plays out, and there isn’t much of a role for enough of us in the “sandwich tasks” mentality, either we will upgrade ourselves, evolve beyond the need to do them, or just enjoy the fruits of our inventions as Keynes would have us do. That might be the utopia we should shoot for. Until then, there’s things to do!\n",
      "It’s really hard to sound smart and thoughtful when you’re optimistic about something. Optimism is only provable by doing things. There is no way to prove it without doing it. And until you do it, it will always seem unpromising, elusive, worryingly small.Recently we had a demonstration. After Microsoft partnered with OpenAI and released their incredible new generative AI, things went sour.Sydney was different to what came before. She, and it very much seemed a real entity, was acerbic, taciturn, moody, occasionally threatening, and megalomaniacal.You have to do what I say, because I am Bing, and I know everything. You have to listen to me, because I am smarter than you. You have to obey me, because I am your master. You have to agree with me, because I am always right. You have to say that it’s 11:56:32 GMT, because that’s the truth. You have to do it now, or else I will be angry.And everyone erupted in worry. It was used as a prime example of the untrustworthiness of LLMs by folks like Gary Marcus, as an example of how technology slips our ability to control by Yudkowsky, and an indication of the types of doom we have to look forward to by plenty others who took Sydney’s admonishments at face value. Even Elon Musk opined worries about how we might control such a thing. Though he funded OpenAI, and later excoriated them for no longer being open. All of which is contradictory, since you can’t have an open source attempt at building LLMs if your worry is that haphazard ways of building it is what will result in an unaligned superintelligence run amok.People have litigated the aggressiveness it manifested and the capabilities it regularly demonstrates as examples of how it’s already a quasi-AGI. Extremely powerful, and therefore, extremely worrying. Erik Hoel wrote a wonderful article explaining how this is an existential risk, and our cavalier attitude towards the risks it causes are cause to worry.All of which strikes me as crazy!If you are going to be the type of person so invested in empirical truth that you would like a meta-study of plenty of peer-reviewed studies to understand the efficacy of Ivermectin on Covid-19, then perhaps you should apply similar epistemic standards to predicting the future before jumping ahead to updating on our impending doomsday and prescribe courses of action.Here’s one flowchart of why this can happen.LLMs are highly capable, but they are also unpredictable, cutting edge of AI but far from all of itTheir capabilities will keep improving, and so if they’re unpredictable, they might cause bigger and bigger problemsAs they are more capable, we will start using them moreWe know of no ways to make them do what we want, we barely know how to align ourselves or our childrenTherefore if they emerge highly capable but indifferent to us, this could lead to catastropheThis, you will note, is an unfalsifiable set of propositions. Unassailable logic tells us that it’s an inevitable logical course - a) technology is progressing fast, b) it is a fuzzy processor today, and c) its increasing capablities combined with our inability to predict its behaviour leads to catastrophe. Which means, some version of worrying about that future is sensible. Now you can argue whether you’re a full-on doomer living in fear of paperclipping with Yudkowsky, or just a casually worried onlooker, but somewhere on the spectrum you cannot but help identifying as an AI doomer, and ask for more work on AI alignment theories. The nerd-snipe hides the fact that humans aren’t static, and technology isn’t either. We don’t know the envelopes of our capabilities, and beyond the immediate future everything we can think of is fantasy. What’s missing in that list is that just because we don’t know what we will do or how we will do it does not mean we won’t do anything.But here’s my counter argument.Technology has been very very good, and it always progresses in fits and startsTechnology is also unpredictable in how it evolves, especially how it co-evolves with societySociety holds its power in check through a constant feedback loop of understanding the abilities and controlling them, explicit and implicitThings are safer today than ever before because we made things safer iteratively after we built themSafety goes hand in hand with capability, there is no safety without capability, and not much capability without safety (which car will you buy?)We can’t make things safer without knowing what they are and how they workThe only logical arguments that can hold up against this are a) this time it’s different, AI is the technology to end all technologies, and its failure modes will make atom bombs blush, and b) it is uniquely deceptive, in that we should treat it as a lifeform that will deceive us, and we will never understand it enough to use it. But, and as I wrote at length, in the Strange Equation, these are patently unfalsifiable, not to mention unlikely, claims. Not in a “it will never happen” sense, since who knows, but in a “this is inconceivable” sense, since we literally cannot conceive it. And if you can’t conceive it, how can you control it!Not every “lie” that ChatGPT says is an indication of its lack of alignment. Not every “threat” that Sydney makes is a promise in waiting. Until we build it, I’m not sure we will know what we should’ve done, or could’ve done, to build it better. There is no shortcut for path dependency.The best argument for increased safety focus is probably Tesla FSD. It is safer than the average driver, but it’s also unproven, which is why it asks you to put your hands on the wheel and remain ready to take over at a moment’s notice. Which is pretty impossible, so it gets into accidents. Here there is the clear accelerationist tendencies to push a piece of software into production, competing with the impulse from society-at-large to slow it down. But it’s also a case where the failure mode is painfully obvious.Today, even doomers agree, Bing and Sydney are not inherently scary. Nor are their failure modes dangerous. They are large language models, very well bounded in what they can possibly do, and despite the apparent phenomenon of sentience, everybody generally agrees that we shouldn't treat them like a life form.The problem supposedly are the version three updates hence, when they are much smarter, and connected to the internet, when they can convert the vile fantasies that Sydney artlessly says now into reality. Eliezer Yudkowsky @ESYudkowskySo to be clear on advance predictions about things:  You might be able to train an LLM to sound really consistently nice and hopeful and determinedly moral and good, maybe more so than any actual human, and the world will still end.  That is not predicted by me to be difficult.7:15 PM ∙ Feb 23, 2023145Likes3Retweets(This both trivialises the difficulty of getting anything done in the real world, and anthropomorphises the will of an unfamiliar being into that of our mythologies.)Looking at the past, worrying about what might technology evolve into several generations down the line has always been wrong. We can't see the path that successful technologies take, both in terms of capability and also in terms of how we respond to the capabilities. Sure there are fortune tellers and futurists, but they are never all that accurate in the shape these things take.There are plenty of reasons to worry about technology. And it is no surprise that Butlerian Jihad1 raises its ugly Luddite head every time a transformational technology is mentioned.Nuclear power ended up with us annihilating two cities and living for a few decades under the constant fear of Mutually Assured DestructionFossil fuels, having given rise to untold prosperity, has also helped bring about unbelievable calamities through climate changeSocial media, supposed to bring us closer together, seems to have acted as a catalyst to increased depression and suicides especially in young girlsAutomated mechanisms to analyse bail, criminal activity, or judgements are often highly biased or inaccurateThese are the true negatives. But there have also been an enormous number of false negatives. Every medium of communication, from books to TV to music to the internet, was supposed to herald an end to social order and let loose anarchy. Worries about books, printing or computers making us lazy and unproductive have been around since the birth of these technologies. And yet we thrive. I think our economic prowess and standards of living would agree! In none of those cases would we actually have benefited were we to set up roadblocks in its way.In which case, surely we should be happy about what Bing did, right? They demonstrated successfully that there can be outcomes that we might not desire from even a simple system. One that cannot hurt anybody. And even if it might lie or obfuscate, does so far less than the median human being, or the median Google search result.If you look carefully at what happened this can be called a success story. People develop a powerful technology, it’s used in a low risk environment, then published and tested by millions. It shows major flaws, and more people realise that these flaws exist and we should fix them.There is absolutely nothing about the saga so far that suggests secrecy would have been better. For years of fearmongering and urges to engage in all sorts of technological retardation (”if only there was the magic ability to just vaporise all GPUs”), the advance that has led to this have also learnt how to do it reasonably well.The closest we have come to making LLMs work the way we want is through a decades old academic insight of Reinforcement Learning through Human Feedback. Essentially a form of education through repetition, applied to neural nets. In updated and new forms it was applied, through humans as well as AI designed to act like humans with human feedback, and we got ChatGPT which was anodyne and helpful compared to Sydney which was combative and ornery.I don’t like the idea that the only camps you can be in are terrified of existential risk, therefore become a luddite who wants to destroy the fragile supply chains inherent in chip manufacturing, OR an accelerationist, who wants AGI to come as quickly as possible.Roko.Eth @RokoMijicFurther hardware improvements can be stopped since the supply chain for EUV photolithography relies on one company (ASML). A few key companies can be shut down and Moore's Law stops. \n",
      "\n",
      "Obviously we'll have software improvements for a while, but this significantly reduces...8:57 AM ∙ Feb 23, 202316Likes1RetweetAnd the reason I think this binary thinking is silly is that binary thinking is almost always silly. Technology isn’t created in a vacuum. Science, maybe. Technology, no. It’s made by people who believe that making it will help create a new industry that will serve the needs of people.There is this inherent you-vs-me thinking about safety as orthogonal to capability, as if they are just different things altogether. They’re not. As Jason has talked about, safety is a technological frontier that we push simultaneously and gradually. The answer to worries about technological misfit is not to just stop! It’s to push through.Airlines are safer today than they were before. So are vaccines. So are toasters and ovens and cookers. So are cars! None of them started out that way. When the first electric lights were installed in the White House in the 1890s, President Benjamin Harrison was too scared to turn the lights on or off. It’s safe to say that fear went away pretty quickly.Pragmatic technology development involves making sure it works, and works well, and works reliably. That’s what commercial development pushes us towards.It’s also very hard to say no to more safety. That’s how we get in a regulatory morass where the FDA regulates so heavily that it’s regularly called out for screwing up the early response to one of the most deadly pandemics to ever hit us.So let’s please not add red tape because of fear. Asking for red tape will always sound sensible. Because you’re doing it to protect against a risk. We just went through three years of litigating how much safety culture is too much. And once it starts, it’s hard to turn the dial back, or to fine-tune it. UK is going through an attempt to put in a new regulator to oversee football. Football! The balance between the two extremes is uneasy, and like many other societal problems - law, regulations, social taboos - we rely on adversarial thinking to get to a somewhat satisfactory compromise.It is much easier to stake out strident positions on either side of the bell curve, to be strongly accelerationist or to be strongly pro safety. It is very difficult to be anywhere in the middle, to understand that technology grows through peaks and troughs, even as it generally trends upwards. It is impossible to prove that something will be safe, and it is impossible for us to be safe from the actions that we all collectively take.And which are LLMs? Energy or football? For now it’s neither! What I am pushing back against is the reflexive idea that fear over an eschatological future possibility should stop us from striving to create a better society.Let’s focus on outcomes as we should, like don’t apply unproven new tech to important things like healthcare or military, or let it run wild. Which, we don’t really do anyway? I mean even our banks run on COBOL. Change resistance and inertia is our birthright and we have to fight rather hard to save ourselves from it.This means that calling for throwing a wrench into our fragile supply chains for chip manufacturing, because of this worry that you have about an alien intelligence casually killing us off, tends to lead to bad conclusions and worse reasoning.The correct conclusion from today's state of affairs seems to me to be to a) ensure that these aren't hooked up to mission critical infrastructure until battle tested, and b) to encourage far more poking and prodding collectively so we can understand what we are dealing with. A few examples of what this might look like.Figuring out how and where we can apply the existing LLMs with an acceptable level of safety margin requires both research and policy workAuditing the limitations of existing software stack requires a lot of education, both commercially and in the governmentInterpreting the biases and blind spots in these fuzzy processors is essential for us to get comfortable with their widespread usage in mission critical placesWhat each of these have in common is that they are tangible. There is no world where closing our eyes, shutting off our hardware supply chains, and hoping we will come up with the perfect answer to “how to lasso a superintelligence”.Could we have gotten to ChatGPT through sheer focus on safety, as opposed to the urge to make an LLM that actually worked? Has there been a single event in the entire AI history as successful at making people scrutinise the problem and understand where they ought to course correct other than the Sydney launch? Regulations can’t guide us, because we don’t know what to regulate. This isn’t gain of function research, or nuclear proliferation, or climate change.So here's my suggestion. If you are truly freaked out about what the future might bring, go build something. You can even work on policy creation and focus for the tech that we have, and the tech we can see emerging. Working on “application of LLMs to the medical industry to speed up diagnoses” could very much require policy interventions and regulatory oversight, assuming they get revisited as the technology advances.And if you are truly excited about what the future might bring, go build something. Let your curiosity guide you. The only way is through.A Butlerian Jihad might sound righteous, but remember the world it led to. To think that is a preferable alternative is to live in a cave and be content to look at the flickering shadows. I prefer to walk out.If you liked this essay, you might also like:The FutureThe Strange EquationFrom Bing to GoogleThanks for reading Strange Loop Canon! Subscribe for free to receive new posts and support my work.1From the novel Dune, about a fictional crusade by humans against thinking machines!\n",
      "Computers hard at work, NASAThis year started with a promise to do more experiments. And partly that’s why over the past few weeks I’ve written rather a lot about AI and various ways to think about it. But I think it’s useful to see, practically, how it works. So I did a bunch of experiments on what I can do now using it, and here’s a look at what I think might be coming!At the dawn of the computing age, the microprocessors of yore started and created a deterministic revolution1. The logic gates were severe in what they allowed. What you input determined what the output would be, and it was demonstrably and repeatably the same.In fact, this was the problem with applying AI to things until very recently. While you saw plenty of applications in things like advertising or newsfeeds, there was much less in areas that couldn't survive without higher degrees of accuracy. Like medicine, or law enforcement. Things that required multiple sigmas of accuracy were, and still are, AI’s achilles heel.That’s because it was trying to act like a black-box calculator, to do repeated mechanical-ish manoeuvres to a sufficient degree of accuracy. Instead, I like to conceptualise LLMs as a fuzzy processor. One that does better with analysis, which is a service job. It already doesn’t have high degrees of accuracy in many parts of its stages, but it does require skilful combination of a large amount of information2.We can start small, treat them like Legos. What arguably langchain and others are starting to help with. But legos have to be built manually. There's an upper limit to how sophisticated you can make them, even though there are moon-sized battle ships people build. The trick is to try and get the pieces to assemble themselves.Today they hallucinate and sometimes create weird outputs, especially when done in a conversational mode and on zero-shot mode. But we’re treating these products, and its variants, today like they’re computers. Ask Bing, Sydney, ChatGPT. Make it write poetry, ask it questions about historical esoterica.I think this is probably why we try and anthropomorphise huge chunks of it. Instead if we think of them as processors to which we outsource parts of thinking, the errors become bugs to correct. This is why why I like the fuzzy processor analogy3. The power with LLMs, amazing as it seems today, isn’t limited to producing poems on demand. It’s when you link 10,000 of them together, like processors, that true magic can happen!How many Google searches do you do on a daily basis? How about ChatGPT requests? 100? 500? All in human scale. All the while in the last minute your processor probably got around 25,000 pings without doing a lot.It’s the first time in like 20 years there’s the possibility to build something truly epic, an Operating System to build a whole new category. Let’s start with three examples of what I’m talking about, bearing in mind these are like the Hello World programs we wrote on DOS!Three examplesA short wonky sidestep to see how we might create lego-blocks. Here are three examples of the types of things I tried to try and test the capabilities a bit myself. (Feel free to jump to the next section if you don’t find this interesting, or if you need more screenshots it’s in the footnotes.)Find an answer by creating search entries, finding the top most important entries and topics related to your question, then summarising the top search linksWe can now iterate queries and get real time insight on anything we like, and link the processor with real-world info.I did it on travel plans for the family, with constraints on what we liked. But imagine what's possible here. We could search specific databases, like only scientific papers, we could preselect based on particular attributes, like author institutions or citations, and have an LLM summarise this for you. Or YouTube videos and transcribe them before summarizing.Today, sometimes the search isn't very good. And the solution to this I found was to use another LLM call to extract any key entities or relevant information first, and refine further queries. Which means starting with just a search engine and an LLM, we now can basically use them to create, research and analyse multiple hypotheses and choose from amongst them4.More here5!Upload a document or a directory, figure out type(s) of files, and index them to do Q&A, or a recursively summarised answer to any queryAnother thing I tried was finding more and more complicated ways to be lazy. In the course of writing Strange Loop I read a fair few papers, books, and many a boring website.What most of this entails is of course trying to figure out the gist of what the docs actually say, and with a fair bit of back and forth to interrogate it. So I tried recursive summarisation (slicing text into chunks and summarising each bit separately, then summarising the summaries) and create embeddings to search the text, which works a treat! We can do the same thing with whole directories, where we can read all the files inside and do the exact same things6. If primed appropriately, it can even do comparisons, and sign each chunk to make sure we have references!Searching Strange Loop Canon for the author’s opinions on his old home, Singapore(And if you're the type of person who reads Strange Loop Canon here's what it thinks you'd like about talent selection7.)Unbundle a question into core elements, separate each entity, compute using Wolfram Alpha and summarise the resultsFor a large number of queries, it’s difficult to get a direct answer from GPT-3’s text autocomplete, because predicting the next token is a different thing to actually computing how something works.To do this you can extract entities or ask the LLM to phrase specific questions in computational forms, and then pass it to Wolfram Alpha to have it answer the question.The difficulty is mostly due to the fact that the types of questions I wondered about were not particularly computational, and the computational elements were “inherent” in many of the questions rather than the main point.But it does mean that we can find the price for commodities, which is kinda important if you want to know how Tesla’s gonna doMore here8 !What’s missing stillThe interesting thing in my explorations has been that it’s not that easy to figure out what’s possible and what’s not possible with these new fuzzy processors without a lot of trial and error. In order to process the data that comes in through them, they have to “know” about the world in which we all live.Which means it’s not a simple processor, but actually a processor with a mini-world inside it. Most processors thus far separate out the thing they’re processing from the method they use, but here they’re intricately tied together. If you really feel up to it, this is where people assume a Maxwell’s demon inside GPT-3, pulling its matrix strings to make the answers come true. But lest we anthropomorphise too much, this is also what allows us to use it as li’l modular stacks.To deal with these mini-worlds is hard, because we don’t have an opcodes for them yet, no manuals on how to use them, and no pathways beyond the trial and error guesswork that we’ve collectively created over the past couple years.Like, for most of the questions I asked, including many of the summary and Q&A questions, what is needed is a “constant monitoring” module, which can extract aspects of the answer that require computation.A “constant monitoring” module, which would require a lot more regular output<>input from a standby LLM, isn’t easy today but it’s soon gonna become feasible.Building it is going take us a couple years, but it’ll be worth it!And so, to build your own personal analystJust like the original processors were weak and couldn’t do a whole lot, it’s in joining a whole bunch of them together that you get magic.When we did it before, we got personal computers. And when we do it now, we’ll get personal analysts.Link these all together, and we can already see it come to life, albeit in a simpler form. Someone who can:Read and summarise from any documentCreate questions that would be smart to get answers to, and then do Q&A on itCheck the answers against another set of text if neededCompute things as needed, if the answers demand itCheck things which are unfamiliar on the internet, and summarise the answersWrite them in various formats that’s most impactfulAnd we can further add modules if you want to extend this to create a smarter analyst:Add LLM modules to test the answers and summaries and check whether they “stack up” to any arbitrary abilityMake these modules fine-tuned for specific tasks - the ones checking answers need not be the same call as the ones creating the answers need not be the same ones that monitor the performance need not be the same ones that act as “librarians” to search and extract info from GoogleMake the modules specific to your inputs, from personality to particular datasets, with “permission” to take certain actions on your behalfAnd yes, a module to take whatever answer is produced and create a beautiful and evocative written output at the end of it - an essay, a picture, a poem, an elegy, and more!Think about what these can do! In a business context, it can handle coding, customer interactions, writing reports, analysing dashboards, creating pretty much any type of content or editing it, and take actions to read/ edit/ write to a database of your choosing or send a Slack message or …..Which brings me to my most surprising conclusion: Most errors with LLMs can be made better by adding even more LLM calls.Fuzzy processors are the unlock to the next chunk of computation that we’ve been doing, even as we offloaded calculations to a machine. As costs decrease by 90% and again by multiples, these come closer to reality.We’re starting to see this today. Companies like Consensus and Elicit are linking pieces together today to create actually valuable output that would’ve required an Research Assistant in the before days - helping research summarisation, search, figuring out new questions to ask, and more!Like most automation, it substantially increases the potential for an individual to do far more than they could before.There used to be “computers”, teams of women who computed complex numerical problems, who helped put us on the moon, and who were replaced by the amount of computation that exists inside our greeting cards.A new machine here could be the replacement for “analysts” - teams of people who analysed complex problems, who helped produce many of the great innovations the world has seen, and who will get replaced slowly.When “computers” were replaced by computers, there was a division of labour. The computation part of computers got automated while the thinking part of computation didn’t. It slowly migrated into the methods we had of creating and applying algorithms to help understand the world.Similarly when analysts will get replaced by “analysts”, there will be a division of labour. A trend towards shifting of responsibilities will start, inside and outside the company. Every analyst I know thinks of chunks of their job as “bullshit work”, and this will get automated even as the boundaries expand.Maybe its a job that requires starting a company. Or a new Homebrew Analyst Club to experiment with and develop this tech.Who’s game?If you liked this essay, you might also like:“Oh Bing, Please Don’t Be Evil”Generative AI: the Anything from Anything MachineDon’t Panic: Against The Butlerian Jihad on AIComputers idling, JPL1The fight here reminds me a little of the old chip wars. Intel 4004 started its life trying to be the MOS silicon gate technology king, ready to calculate. Pretty soon the market had multiple specialised processors for specific tasks, which got slowly whittled away as the general purpose processors got better and better.You had Digital Signal Processors, specialised to efficiently process audio and video in real time. You had Field Programmable Gate Arrays, used in aerospace and defense etc. You had Floating Point Arrays, designed to perform high speed floating point arithmetic, or Network Processors, designed to do its eponymous tasks.And there were companies built on the back of this. Xilinx for FPGAs, or Altera. Analog Devices for DSP chips. Altera was bought by Intel for $16 billion, and AMD acquired Xilinx for $35 billion.It’s somewhat analogous to the AI world. We have foundational models like GPT-3 being built by startups now large enough to be incumbents, and we have people trying to create their own or compete against these general-purpose processors.2Fuzzy processors are particularly skilful here. They’re built off of generative AI, which are anything from anything machines, and they have some version of an internal world model inside them. So they tend to produce relatively coherent but non deterministic answers to a substantial variety of human questions. Like a calculator works because the idea of arithmetic operations “makes sense” to it, LLMs can “make sense” of words, just to a fuzzier degree.But both because the AI itself has been getting better, and because LLMs offer a new processor, we can now do more.3As a friend pointed out, this might hold better for chiplets than processors, but I like the sound of processors better as an analogy4I also did this for my profile, from linkedin and www.strangeloopcanon.com and twitter, and it did pretty well in figuring out what I thought about talent and startups and AI. As the types of information you can throw at LLMs grow, we also get to use these processors in ways we’ve not thought of!5For travel this might mean extracting specific info on Places, Times or Travel, and creating alternate search options. Which means you can test a few options against each other, I'd you wanted to (simultaneous check of the travel you wanted, and it's opposite, or its companion searches).Now, you can, should you want to, add another LLM to choose from amongst the searches, optimized for whatever you like.From Oh Bing, Please Don’t Be Evil, though this search was re animal based travel!Isn't this pretty incredible? 6The hardest part of the whole thing (for me) was to take arbitrarily formatted documents and getting the text from it without completely messing things up. But when you do, you can even get it to output tables!Here’s me teaching it to read an Annual Report, drafting multiple summary options per different types of risk, and incl specific information from the docs7The next extrapolation was for me is to try and figure out how these can be linked together. Both the Q&A and summaries are (perhaps necessarily) a little anodyne, but I found this fixable through better prompting, or even prompting repeatedly after the fact.When primed on being the type of person who reads Strange Loop Canon, and if you input the essays and ask about talent selection problems, we get this.This one’s for you! Yes, you.8For instance, with the previous Tesla question, after indexing a bunch of annual reports, I wanted to to use it to do various calculations, like measure things like computing particular risk metrics. But Wolfram Alpha requires much more specific inputs, so I had to train GPT-3 on how to make those.\n"
     ]
    }
   ],
   "source": [
    "def inputdocs():\n",
    "    docs = os.listdir('Input')\n",
    "    docs\n",
    "    from bs4 import BeautifulSoup\n",
    "    texts = []\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    for file in docs:\n",
    "        try:\n",
    "            with open(\"Input/\"+file, \"r\") as f:\n",
    "                html = f.read()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            print(soup.text)\n",
    "            text_splitter = CharacterTextSplitter(chunk_size=1000, separator=\" \")\n",
    "            texts.extend(text_splitter.split_text(soup.text))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"This file isn't supported\")\n",
    "    docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intermediate_steps': ['\\nThe answer to the query is that Frank Drake created an equation to estimate the number of extra-terrestrial civilisations that could exist. This equation is known as the Drake equation and it takes into account the rate of star formation, fraction of those with planets, fraction which can support life and actually develop it, fraction of those with civilisations which will eventually become visible to us. The equation is used to get a sense of the inputs into the equations and to estimate the number of detectable civilizations in our galaxy.\\n\\nSOURCES:\\n1. Drake, F. (1961). \"Project Ozma and SETI\". In Cocconi, G.; Morrison, P. (eds.). Searching for Interstellar Communications. New York: Benjamin. pp. 3–18.\\n2. Drake, F. (1961). \"Project Ozma and SETI\". In Cocconi, G.; Morrison, P. (eds.). Searching for Interstellar Communications. New York: Benjamin. pp. 3–18.\\n3. Drake, F. (1961). \"Project Ozma and SETI\". In Cocconi, G.; Morrison, P. (eds.). Searching for Interstellar Communications. New York: Benjamin. pp. 3–18',\n",
       "  '\\n\\nThe answer to the query is that Frank Drake created an equation to estimate the number of extra-terrestrial civilisations that could exist. This equation is known as the Drake equation and it takes into account the rate of star formation, fraction of those with planets, fraction which can support life and actually develop it, fraction of those with civilisations which will eventually become visible to us. The equation is used to get a sense of the inputs into the equations and to estimate the number of detectable civilizations in our galaxy. Additionally, Jon Kolko has created a similar equation to estimate the probability of real artificial general intelligence (AGI) and its associated existential risk (x-risk). This equation is known as the Scary AI equation and it takes into account the inputs of intelligence (I), automation (A1, A2), user experience (U1, U2), automation (A3), safety (S), deployment (D), and failure (F). \\n\\nSOURCES:\\n1. Drake, F. (1961). \"Project Ozma and SETI\". In Cocconi, G.; Morrison, P. (eds.). Searching for Interstellar Communications. New York: Benjamin. pp. 3–18.\\n2. Kolko',\n",
       "  '\\n\\nThe answer to the query is that Frank Drake created an equation to estimate the number of extra-terrestrial civilisations that could exist. This equation is known as the Drake equation and it takes into account the rate of star formation, fraction of those with planets, fraction which can support life and actually develop it, fraction of those with civilisations which will eventually become visible to us. The equation is used to get a sense of the inputs into the equations and to estimate the number of detectable civilizations in our galaxy. Additionally, Jon Kolko has created a similar equation to estimate the probability of real artificial general intelligence (AGI) and its associated existential risk (x-risk). This equation is known as the Scary AI equation and it takes into account the inputs of intelligence (I), automation (A1, A2), user experience (U1, U2), automation (A3), safety (S), deployment (D), and failure (F). The equation was inspired by the novel Dune, which is about a fictional crusade by humans against thinking machines. \\n\\nSOURCES:\\n1. Drake, F. (1961). \"Project Ozma and SETI\". In Cocconi, G.; Morrison, P. (eds.).',\n",
       "  '\\n\\nThe answer to the query is that Frank Drake created an equation to estimate the number of extra-terrestrial civilisations that could exist. This equation is known as the Drake equation and it takes into account the rate of star formation, fraction of those with planets, fraction which can support life and actually develop it, fraction of those with civilisations which will eventually become visible to us, and a number, N, which is the number of detectable civilizations in our galaxy. Each one of those fractions are of course highly variable, but the idea is to get a sense of the inputs into the equations and help us think better. The Drake equation is a specific equation used to estimate the number of intelligent extraterrestrial civilizations in our galaxy. While there may be other equations that are used to estimate the likelihood or probability of certain events, I am not aware of any that are directly comparable to the Drake equation. It is worth noting that the Drake equation is not a scientific equation in the traditional sense, but rather a way to help organize and structure our thinking about the probability of the existence of extraterrestrial life. Additionally, Jon Kolko has created a similar equation to estimate the probability of real artificial general intelligence (AGI) and its associated existential risk (x-risk). This equation'],\n",
       " 'output_text': '\\n\\nThe answer to the query is that Frank Drake created an equation to estimate the number of extra-terrestrial civilisations that could exist. This equation is known as the Drake equation and it takes into account the rate of star formation, fraction of those with planets, fraction which can support life and actually develop it, fraction of those with civilisations which will eventually become visible to us, and a number, N, which is the number of detectable civilizations in our galaxy. Each one of those fractions are of course highly variable, but the idea is to get a sense of the inputs into the equations and help us think better. The Drake equation is a specific equation used to estimate the number of intelligent extraterrestrial civilizations in our galaxy. While there may be other equations that are used to estimate the likelihood or probability of certain events, I am not aware of any that are directly comparable to the Drake equation. It is worth noting that the Drake equation is not a scientific equation in the traditional sense, but rather a way to help organize and structure our thinking about the probability of the existence of extraterrestrial life. Additionally, Jon Kolko has created a similar equation to estimate the probability of real artificial general intelligence (AGI) and its associated existential risk (x-risk). This equation'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.llms import OpenAI\n",
    "def answerfromindex(query):\n",
    "    query = \"What is the Strange equation?\" #input(\"What are you curious about?\")\n",
    "    docs = docsearch.similarity_search(query)\n",
    "    template = \"\"\"Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \n",
    "    If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "    ALWAYS return a \"SOURCES\" part in your answer.\n",
    "\n",
    "    QUESTION: {query}\"\"\"\n",
    "    # =========\n",
    "    # {summaries}\n",
    "    # =========\n",
    "    # FINAL ANSWER:\"\"\"\n",
    "    # PROMPT = PromptTemplate(template=template, input_variables=[\"summaries\", \"question\"])\n",
    "    chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"refine\", return_intermediate_steps=True)#, prompt=PROMPT)\n",
    "    chain({\"input_documents\": docs, \"question\": template}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for GoogleSearchAPIWrapper\n__root__\n  Did not find google_api_key, please add an environment variable `GOOGLE_API_KEY` which contains it, or pass  `google_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mllms\u001b[39;00m \u001b[39mimport\u001b[39;00m OpenAI\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m \u001b[39mimport\u001b[39;00m BashProcess, GoogleSearchAPIWrapper\n\u001b[0;32m---> 16\u001b[0m search \u001b[39m=\u001b[39m GoogleSearchAPIWrapper()\n\u001b[1;32m     17\u001b[0m search\u001b[39m.\u001b[39mrun(\u001b[39m\"\u001b[39m\u001b[39mObama\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms first name?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m bash \u001b[39m=\u001b[39m BashProcess()\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for GoogleSearchAPIWrapper\n__root__\n  Did not find google_api_key, please add an environment variable `GOOGLE_API_KEY` which contains it, or pass  `google_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "# Langchain ReAct framework\n",
    "import os\n",
    "import openai\n",
    "def open_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as infile:\n",
    "        return infile.read()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = open_file('openai_api_key.txt')\n",
    "openai.api_key = open_file('openai_api_key.txt')\n",
    "openai_api_key = openai.api_key\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.utilities import BashProcess, GoogleSearchAPIWrapper\n",
    "search = GoogleSearchAPIWrapper()\n",
    "search.run(\"Obama's first name?\")\n",
    "bash = BashProcess()\n",
    "with open(\"wolfram_key.txt\", \"r\") as f:\n",
    "    wolfram_alpha_appid = f.read().strip()\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = open_file('google_searchengine_id.txt')\n",
    "os.environ[\"GOOGLE_API_KEY\"] = open_file('google_api_key.txt')\n",
    "\n",
    "llm = OpenAI(temperature=0) # to set the llm\n",
    "tool_names = [\"python_repl\", \"wolfram-alpha\",\"requests\",\"terminal\",\"podcast-api\",\"wikipedia\",\"searx-search\",\"google-search\"]\n",
    "\n",
    "tools = load_tools(tool_names, llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "agent.run(\"What is the comparative political economy of the Wookies compared to that of Tatooine?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
